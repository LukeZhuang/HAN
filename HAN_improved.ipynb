{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.base import Layer\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HAN import HAN\n",
    "from Dataset_util import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improved points**:\n",
    "1. retrain word2vec with &lt;UNKNOWN&gt; token\n",
    "2. add regularization/distilling to attention term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-08 11:46:08,649 : INFO : loading projection weights from data/yelp2013/yelp_2013_50.vector\n",
      "2018-06-08 11:46:11,285 : INFO : loaded (43531, 50) matrix from data/yelp2013/yelp_2013_50.vector\n"
     ]
    }
   ],
   "source": [
    "year='2013'\n",
    "word2vec_size=50\n",
    "cut_long_sentence=200\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-08 11:46:11,293 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "_=word2vec_model.most_similar('ridiculous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 43531\n"
     ]
    }
   ],
   "source": [
    "# f_vocab=open('data/yelp'+year+'/yelp_'+year+'_500.vocab')\n",
    "# index2chr={}\n",
    "# chr2index={}\n",
    "# index2chr[0]='<UNKNOWN>'\n",
    "# chr2index['<UNKNOWN>']=0\n",
    "# cnt=1\n",
    "# for line in f_vocab:\n",
    "#     w=line.strip().split(' ')[0]\n",
    "#     index2chr[cnt]=w\n",
    "#     chr2index[w]=cnt\n",
    "#     cnt+=1\n",
    "# f_vocab.close()\n",
    "# vocab_size=cnt\n",
    "# print('vocabulary size =',vocab_size)\n",
    "\n",
    "f_vocab=open('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vocab')\n",
    "vocab_set=[]\n",
    "for line in f_vocab:\n",
    "    vocab_set.append(line.strip().split(' ')[0])\n",
    "vocab_set=set(vocab_set)\n",
    "f_vocab.close()\n",
    "print('vocabulary size =',len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data(f):\n",
    "    longest_doc=-1\n",
    "    longest_sen=-1\n",
    "    documents=[]\n",
    "    for line in f:\n",
    "        this_document=[]\n",
    "        cols=line.strip().split('\\t\\t')\n",
    "        sents=cols[1].split('\\t')\n",
    "        for s in sents:\n",
    "            this_sentence=[]\n",
    "            words=s.split(' ')\n",
    "            for w in words:\n",
    "                if w in vocab_set:\n",
    "                    this_sentence.append(w)\n",
    "                else:\n",
    "                    this_sentence.append('<UNKNOWN>')\n",
    "            longest_sen=max(longest_sen,len(this_sentence))\n",
    "            this_document.append(this_sentence)\n",
    "        longest_doc=max(longest_doc,len(this_document))\n",
    "        documents.append((this_document,int(cols[0])))\n",
    "    return documents,longest_doc,longest_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train=open('data/yelp'+year+'/yelp-'+year+'-train-nonvocab.txt')\n",
    "documents_train,longest_doc_train,longest_sen_train=raw_data(f_train)\n",
    "f_train.close()\n",
    "f_train=None\n",
    "\n",
    "f_val=open('data/yelp'+year+'/yelp-'+year+'-dev-nonvocab.txt')\n",
    "documents_val,longest_doc_val,longest_sen_val=raw_data(f_val)\n",
    "f_val.close()\n",
    "f_val=None\n",
    "\n",
    "f_test=open('data/yelp'+year+'/yelp-'+year+'-test-nonvocab.txt')\n",
    "documents_test,longest_doc_test,longest_sen_test=raw_data(f_test)\n",
    "f_test.close()\n",
    "f_test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum document length = 100\n",
      "maximum sentence length = 653\n",
      "size of train data = 268006\n",
      "size of validation data = 33499\n",
      "size of test data = 33503\n"
     ]
    }
   ],
   "source": [
    "max_doc_length=max(longest_doc_train,longest_doc_val,longest_doc_test)\n",
    "max_sent_length=max(longest_sen_train,longest_sen_val,longest_sen_test)\n",
    "print('maximum document length =',max_doc_length)\n",
    "print('maximum sentence length =',max_sent_length)\n",
    "\n",
    "# documents_train=documents_train[:128]\n",
    "\n",
    "num_train=len(documents_train)\n",
    "num_val=len(documents_val)\n",
    "num_test=len(documents_test)\n",
    "\n",
    "print('size of train data =',num_train)\n",
    "print('size of validation data =',num_val)\n",
    "print('size of test data =',num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(documents_train,word2vec_model,word2vec_size,cut_long_sentence)\n",
    "train_eval=Dataset(documents_train,word2vec_model,word2vec_size,cut_long_sentence)\n",
    "validation=Dataset(documents_val,word2vec_model,word2vec_size,cut_long_sentence)\n",
    "test=Dataset(documents_test,word2vec_model,word2vec_size,cut_long_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  6  3 11  3  7  9  7 11  3  7  9  3 10  5  4  7 11  7  5  4  4  1  4\n",
      " 10  7  5 42 18 21  7  8  1  5  6  8 69  3 13 11 26 15 33 10  5 10 12 17\n",
      " 10 25  5  9  8 12 29  7 19  3 20  6 18  5  3  8]\n",
      "[[19 0 0 ... 0 0 0]\n",
      " [5 17 20 ... 0 0 0]\n",
      " [5 7 6 ... 0 0 0]\n",
      " ...\n",
      " [8 4 8 ... 0 0 0]\n",
      " [2 10 3 ... 0 0 0]\n",
      " [22 12 2 ... 0 0 0]]\n",
      "(64,)\n",
      "(64, 69)\n",
      "(64, 69, 80, 50)\n",
      "[0 3 3 3 3 4 2 3 0 4 3 2 3 3 1 4 3 4 3 4 2 3 2 0 4 3 3 0 2 3 4 1 3 3 3 3 0\n",
      " 4 4 3 3 3 4 4 0 2 0 3 3 3 1 4 4 0 2 3 3 2 4 4 0 4 4 2]\n"
     ]
    }
   ],
   "source": [
    "train.start_epoch()\n",
    "corpora,l,a,labels=train.next_batch(64)\n",
    "print(l)\n",
    "print(a)\n",
    "print(l.shape)\n",
    "print(a.shape)\n",
    "print(corpora.shape)\n",
    "# print(corpora)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_sentence=tf.placeholder(dtype=tf.float32,shape=[None,None,None,word2vec_size]) # shape=(batch_size,doc_size,sen_size,vocab)\n",
    "y=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "sentence_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size*document_size,)\n",
    "document_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size,)\n",
    "is_training=tf.placeholder(dtype=tf.bool,shape=None)\n",
    "dropout=tf.placeholder(dtype=tf.float32,shape=None)\n",
    "\n",
    "\n",
    "model=HAN(num_classes,word2vec_size,lamb=0.0,disTill=2.0)\n",
    "loss,predict,accuracy,attention_low,attention_high=model(X_sentence,y,sentence_length,document_length,dropout,is_training)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4e-4\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,num_train//128, 0.9, staircase=True)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=starter_learning_rate)\n",
    "# train_step = optimizier.minimize(total_loss,global_step=global_step)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch=40\n",
    "print_every=50\n",
    "bs=32\n",
    "\n",
    "def eval(dataset,num_iteration):\n",
    "    tot_loss=0\n",
    "    tot_accuracy=0\n",
    "    dataset.start_epoch()\n",
    "    for it in range(num_iteration):\n",
    "        output,document_sizes,sentence_sizes,labels=dataset.next_batch(bs)\n",
    "        feed_dict={X_sentence:output,y:labels,sentence_length:sentence_sizes.reshape(-1,),\n",
    "                   document_length:document_sizes,is_training:False,dropout:0.0}\n",
    "        loss_num,acc_num=sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "        tot_loss+=loss_num\n",
    "        tot_accuracy+=acc_num\n",
    "    tot_loss/=num_iteration\n",
    "    tot_accuracy/=num_iteration\n",
    "    return tot_loss,tot_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 11:46:37 start epoch 1/40, with learning rate = 0.0004000000\n",
      "2018-06-08 11:46:38 iteration 1/1000: current training loss = 1.604703, accuracy = 28.12%\n",
      "2018-06-08 11:46:51 iteration 50/1000: current training loss = 1.241785, accuracy = 37.50%\n",
      "2018-06-08 11:47:03 iteration 100/1000: current training loss = 1.192611, accuracy = 53.12%\n",
      "2018-06-08 11:47:15 iteration 150/1000: current training loss = 1.171434, accuracy = 43.75%\n",
      "2018-06-08 11:47:28 iteration 200/1000: current training loss = 1.090396, accuracy = 43.75%\n",
      "2018-06-08 11:47:41 iteration 250/1000: current training loss = 1.034346, accuracy = 56.25%\n",
      "2018-06-08 11:47:53 iteration 300/1000: current training loss = 0.928427, accuracy = 56.25%\n",
      "2018-06-08 11:48:05 iteration 350/1000: current training loss = 0.916046, accuracy = 68.75%\n",
      "2018-06-08 11:48:18 iteration 400/1000: current training loss = 1.067405, accuracy = 40.62%\n",
      "2018-06-08 11:48:30 iteration 450/1000: current training loss = 1.119217, accuracy = 50.00%\n",
      "2018-06-08 11:48:43 iteration 500/1000: current training loss = 0.997244, accuracy = 46.88%\n",
      "2018-06-08 11:48:55 iteration 550/1000: current training loss = 0.855066, accuracy = 56.25%\n",
      "2018-06-08 11:49:08 iteration 600/1000: current training loss = 0.837242, accuracy = 71.88%\n",
      "2018-06-08 11:49:21 iteration 650/1000: current training loss = 1.042255, accuracy = 53.12%\n",
      "2018-06-08 11:49:32 iteration 700/1000: current training loss = 0.882747, accuracy = 65.62%\n",
      "2018-06-08 11:49:45 iteration 750/1000: current training loss = 1.405534, accuracy = 43.75%\n",
      "2018-06-08 11:49:57 iteration 800/1000: current training loss = 0.791531, accuracy = 65.62%\n",
      "2018-06-08 11:50:09 iteration 850/1000: current training loss = 0.907784, accuracy = 68.75%\n",
      "2018-06-08 11:50:22 iteration 900/1000: current training loss = 0.855863, accuracy = 62.50%\n",
      "2018-06-08 11:50:35 iteration 950/1000: current training loss = 0.925175, accuracy = 65.62%\n",
      "2018-06-08 11:50:47 iteration 1000/1000: current training loss = 0.900971, accuracy = 59.38%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 11:51:51 end epoch 1/40: acc_train=57.703% acc_val=57.500% acc_test=58.578%\n",
      "\n",
      "2018-06-08 11:51:51 start epoch 2/40, with learning rate = 0.0004000000\n",
      "2018-06-08 11:51:52 iteration 1/1000: current training loss = 0.883280, accuracy = 71.88%\n",
      "2018-06-08 11:52:03 iteration 50/1000: current training loss = 0.901387, accuracy = 50.00%\n",
      "2018-06-08 11:52:16 iteration 100/1000: current training loss = 0.987812, accuracy = 59.38%\n",
      "2018-06-08 11:52:29 iteration 150/1000: current training loss = 0.775093, accuracy = 53.12%\n",
      "2018-06-08 11:52:41 iteration 200/1000: current training loss = 0.924396, accuracy = 50.00%\n",
      "2018-06-08 11:52:53 iteration 250/1000: current training loss = 0.904945, accuracy = 56.25%\n",
      "2018-06-08 11:53:05 iteration 300/1000: current training loss = 0.762742, accuracy = 59.38%\n",
      "2018-06-08 11:53:17 iteration 350/1000: current training loss = 0.761267, accuracy = 75.00%\n",
      "2018-06-08 11:53:29 iteration 400/1000: current training loss = 1.017718, accuracy = 50.00%\n",
      "2018-06-08 11:53:42 iteration 450/1000: current training loss = 0.772821, accuracy = 65.62%\n",
      "2018-06-08 11:53:54 iteration 500/1000: current training loss = 0.989022, accuracy = 59.38%\n",
      "2018-06-08 11:54:06 iteration 550/1000: current training loss = 0.722270, accuracy = 65.62%\n",
      "2018-06-08 11:54:17 iteration 600/1000: current training loss = 0.792362, accuracy = 75.00%\n",
      "2018-06-08 11:54:28 iteration 650/1000: current training loss = 0.853540, accuracy = 56.25%\n",
      "2018-06-08 11:54:41 iteration 700/1000: current training loss = 1.117748, accuracy = 59.38%\n",
      "2018-06-08 11:54:53 iteration 750/1000: current training loss = 0.805323, accuracy = 68.75%\n",
      "2018-06-08 11:55:05 iteration 800/1000: current training loss = 0.977474, accuracy = 62.50%\n",
      "2018-06-08 11:55:17 iteration 850/1000: current training loss = 1.109689, accuracy = 53.12%\n",
      "2018-06-08 11:55:29 iteration 900/1000: current training loss = 1.107181, accuracy = 43.75%\n",
      "2018-06-08 11:55:41 iteration 950/1000: current training loss = 0.894618, accuracy = 56.25%\n",
      "2018-06-08 11:55:53 iteration 1000/1000: current training loss = 0.685005, accuracy = 75.00%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 11:56:56 end epoch 2/40: acc_train=59.969% acc_val=59.781% acc_test=60.953%\n",
      "\n",
      "2018-06-08 11:56:56 start epoch 3/40, with learning rate = 0.0004000000\n",
      "2018-06-08 11:56:57 iteration 1/1000: current training loss = 0.771088, accuracy = 68.75%\n",
      "2018-06-08 11:57:09 iteration 50/1000: current training loss = 0.813460, accuracy = 71.88%\n",
      "2018-06-08 11:57:21 iteration 100/1000: current training loss = 0.837501, accuracy = 59.38%\n",
      "2018-06-08 11:57:33 iteration 150/1000: current training loss = 0.767038, accuracy = 65.62%\n",
      "2018-06-08 11:57:45 iteration 200/1000: current training loss = 0.854833, accuracy = 71.88%\n",
      "2018-06-08 11:57:57 iteration 250/1000: current training loss = 0.992887, accuracy = 65.62%\n",
      "2018-06-08 11:58:08 iteration 300/1000: current training loss = 1.077567, accuracy = 56.25%\n",
      "2018-06-08 11:58:20 iteration 350/1000: current training loss = 0.781924, accuracy = 65.62%\n",
      "2018-06-08 11:58:32 iteration 400/1000: current training loss = 0.753103, accuracy = 68.75%\n",
      "2018-06-08 11:58:44 iteration 450/1000: current training loss = 0.964755, accuracy = 56.25%\n",
      "2018-06-08 11:58:56 iteration 500/1000: current training loss = 0.973976, accuracy = 53.12%\n",
      "2018-06-08 11:59:08 iteration 550/1000: current training loss = 0.955981, accuracy = 59.38%\n",
      "2018-06-08 11:59:21 iteration 600/1000: current training loss = 0.819095, accuracy = 78.12%\n",
      "2018-06-08 11:59:32 iteration 650/1000: current training loss = 0.800054, accuracy = 71.88%\n",
      "2018-06-08 11:59:44 iteration 700/1000: current training loss = 0.978836, accuracy = 53.12%\n",
      "2018-06-08 11:59:56 iteration 750/1000: current training loss = 0.637257, accuracy = 68.75%\n",
      "2018-06-08 12:00:08 iteration 800/1000: current training loss = 1.105159, accuracy = 46.88%\n",
      "2018-06-08 12:00:20 iteration 850/1000: current training loss = 0.788835, accuracy = 56.25%\n",
      "2018-06-08 12:00:32 iteration 900/1000: current training loss = 0.675642, accuracy = 62.50%\n",
      "2018-06-08 12:00:44 iteration 950/1000: current training loss = 0.951378, accuracy = 56.25%\n",
      "2018-06-08 12:00:56 iteration 1000/1000: current training loss = 0.874195, accuracy = 62.50%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:01:58 end epoch 3/40: acc_train=61.859% acc_val=60.828% acc_test=59.938%\n",
      "\n",
      "2018-06-08 12:01:58 start epoch 4/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:01:58 iteration 1/1000: current training loss = 0.692779, accuracy = 62.50%\n",
      "2018-06-08 12:02:10 iteration 50/1000: current training loss = 0.912120, accuracy = 62.50%\n",
      "2018-06-08 12:02:21 iteration 100/1000: current training loss = 0.996886, accuracy = 53.12%\n",
      "2018-06-08 12:02:33 iteration 150/1000: current training loss = 0.728780, accuracy = 62.50%\n",
      "2018-06-08 12:02:44 iteration 200/1000: current training loss = 0.799199, accuracy = 65.62%\n",
      "2018-06-08 12:02:55 iteration 250/1000: current training loss = 0.898544, accuracy = 59.38%\n",
      "2018-06-08 12:03:07 iteration 300/1000: current training loss = 0.945821, accuracy = 56.25%\n",
      "2018-06-08 12:03:19 iteration 350/1000: current training loss = 0.682439, accuracy = 62.50%\n",
      "2018-06-08 12:03:31 iteration 400/1000: current training loss = 0.699116, accuracy = 59.38%\n",
      "2018-06-08 12:03:42 iteration 450/1000: current training loss = 0.901720, accuracy = 59.38%\n",
      "2018-06-08 12:03:54 iteration 500/1000: current training loss = 0.821533, accuracy = 65.62%\n",
      "2018-06-08 12:04:05 iteration 550/1000: current training loss = 0.944514, accuracy = 62.50%\n",
      "2018-06-08 12:04:17 iteration 600/1000: current training loss = 1.003758, accuracy = 50.00%\n",
      "2018-06-08 12:04:28 iteration 650/1000: current training loss = 0.809861, accuracy = 65.62%\n",
      "2018-06-08 12:04:40 iteration 700/1000: current training loss = 0.883659, accuracy = 65.62%\n",
      "2018-06-08 12:04:52 iteration 750/1000: current training loss = 0.712485, accuracy = 68.75%\n",
      "2018-06-08 12:05:05 iteration 800/1000: current training loss = 0.724333, accuracy = 68.75%\n",
      "2018-06-08 12:05:16 iteration 850/1000: current training loss = 0.861800, accuracy = 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 12:05:29 iteration 900/1000: current training loss = 0.732295, accuracy = 71.88%\n",
      "2018-06-08 12:05:40 iteration 950/1000: current training loss = 0.767685, accuracy = 59.38%\n",
      "2018-06-08 12:05:52 iteration 1000/1000: current training loss = 1.048533, accuracy = 50.00%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:06:56 end epoch 4/40: acc_train=62.219% acc_val=61.062% acc_test=61.750%\n",
      "\n",
      "2018-06-08 12:06:56 start epoch 5/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:06:56 iteration 1/1000: current training loss = 1.136068, accuracy = 53.12%\n",
      "2018-06-08 12:07:09 iteration 50/1000: current training loss = 0.882807, accuracy = 68.75%\n",
      "2018-06-08 12:07:20 iteration 100/1000: current training loss = 1.022346, accuracy = 50.00%\n",
      "2018-06-08 12:07:32 iteration 150/1000: current training loss = 1.026100, accuracy = 59.38%\n",
      "2018-06-08 12:07:44 iteration 200/1000: current training loss = 1.018672, accuracy = 53.12%\n",
      "2018-06-08 12:07:56 iteration 250/1000: current training loss = 0.724340, accuracy = 56.25%\n",
      "2018-06-08 12:08:08 iteration 300/1000: current training loss = 0.689555, accuracy = 78.12%\n",
      "2018-06-08 12:08:20 iteration 350/1000: current training loss = 0.901812, accuracy = 43.75%\n",
      "2018-06-08 12:08:33 iteration 400/1000: current training loss = 0.754203, accuracy = 71.88%\n",
      "2018-06-08 12:08:45 iteration 450/1000: current training loss = 0.989385, accuracy = 56.25%\n",
      "2018-06-08 12:08:57 iteration 500/1000: current training loss = 0.805608, accuracy = 71.88%\n",
      "2018-06-08 12:09:10 iteration 550/1000: current training loss = 0.666038, accuracy = 71.88%\n",
      "2018-06-08 12:09:22 iteration 600/1000: current training loss = 0.688173, accuracy = 71.88%\n",
      "2018-06-08 12:09:34 iteration 650/1000: current training loss = 0.694824, accuracy = 59.38%\n",
      "2018-06-08 12:09:46 iteration 700/1000: current training loss = 0.851474, accuracy = 50.00%\n",
      "2018-06-08 12:09:58 iteration 750/1000: current training loss = 0.894341, accuracy = 59.38%\n",
      "2018-06-08 12:10:10 iteration 800/1000: current training loss = 0.768915, accuracy = 68.75%\n",
      "2018-06-08 12:10:23 iteration 850/1000: current training loss = 0.838281, accuracy = 56.25%\n",
      "2018-06-08 12:10:34 iteration 900/1000: current training loss = 0.680573, accuracy = 68.75%\n",
      "2018-06-08 12:10:46 iteration 950/1000: current training loss = 0.813547, accuracy = 71.88%\n",
      "2018-06-08 12:10:59 iteration 1000/1000: current training loss = 0.726148, accuracy = 53.12%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:12:02 end epoch 5/40: acc_train=64.406% acc_val=62.109% acc_test=63.188%\n",
      "\n",
      "2018-06-08 12:12:02 start epoch 6/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:12:03 iteration 1/1000: current training loss = 1.016631, accuracy = 62.50%\n",
      "2018-06-08 12:12:15 iteration 50/1000: current training loss = 0.757654, accuracy = 59.38%\n",
      "2018-06-08 12:12:27 iteration 100/1000: current training loss = 0.631809, accuracy = 68.75%\n",
      "2018-06-08 12:12:39 iteration 150/1000: current training loss = 0.844684, accuracy = 71.88%\n",
      "2018-06-08 12:12:51 iteration 200/1000: current training loss = 0.827284, accuracy = 62.50%\n",
      "2018-06-08 12:13:03 iteration 250/1000: current training loss = 0.661135, accuracy = 65.62%\n",
      "2018-06-08 12:13:16 iteration 300/1000: current training loss = 0.623690, accuracy = 71.88%\n",
      "2018-06-08 12:13:27 iteration 350/1000: current training loss = 0.943945, accuracy = 46.88%\n",
      "2018-06-08 12:13:38 iteration 400/1000: current training loss = 0.774530, accuracy = 68.75%\n",
      "2018-06-08 12:13:50 iteration 450/1000: current training loss = 0.620364, accuracy = 75.00%\n",
      "2018-06-08 12:14:02 iteration 500/1000: current training loss = 0.857046, accuracy = 62.50%\n",
      "2018-06-08 12:14:14 iteration 550/1000: current training loss = 0.773586, accuracy = 71.88%\n",
      "2018-06-08 12:14:26 iteration 600/1000: current training loss = 0.994987, accuracy = 40.62%\n",
      "2018-06-08 12:14:38 iteration 650/1000: current training loss = 0.595530, accuracy = 75.00%\n",
      "2018-06-08 12:14:50 iteration 700/1000: current training loss = 0.945624, accuracy = 53.12%\n",
      "2018-06-08 12:15:02 iteration 750/1000: current training loss = 0.726183, accuracy = 71.88%\n",
      "2018-06-08 12:15:14 iteration 800/1000: current training loss = 0.809632, accuracy = 56.25%\n",
      "2018-06-08 12:15:26 iteration 850/1000: current training loss = 0.815471, accuracy = 68.75%\n",
      "2018-06-08 12:15:39 iteration 900/1000: current training loss = 0.590200, accuracy = 78.12%\n",
      "2018-06-08 12:15:50 iteration 950/1000: current training loss = 0.959216, accuracy = 56.25%\n",
      "2018-06-08 12:16:03 iteration 1000/1000: current training loss = 0.684547, accuracy = 68.75%\n",
      "2018-06-08 12:17:04 end epoch 6/40: acc_train=63.078% acc_val=62.109% acc_test=61.922%\n",
      "\n",
      "2018-06-08 12:17:04 start epoch 7/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:17:05 iteration 1/1000: current training loss = 0.644744, accuracy = 81.25%\n",
      "2018-06-08 12:17:17 iteration 50/1000: current training loss = 0.875269, accuracy = 56.25%\n",
      "2018-06-08 12:17:29 iteration 100/1000: current training loss = 1.047706, accuracy = 46.88%\n",
      "2018-06-08 12:17:41 iteration 150/1000: current training loss = 0.885326, accuracy = 56.25%\n",
      "2018-06-08 12:17:54 iteration 200/1000: current training loss = 0.598875, accuracy = 65.62%\n",
      "2018-06-08 12:18:06 iteration 250/1000: current training loss = 0.880956, accuracy = 65.62%\n",
      "2018-06-08 12:18:18 iteration 300/1000: current training loss = 1.039050, accuracy = 46.88%\n",
      "2018-06-08 12:18:30 iteration 350/1000: current training loss = 0.755342, accuracy = 62.50%\n",
      "2018-06-08 12:18:42 iteration 400/1000: current training loss = 0.812967, accuracy = 46.88%\n",
      "2018-06-08 12:18:55 iteration 450/1000: current training loss = 0.669965, accuracy = 71.88%\n",
      "2018-06-08 12:19:08 iteration 500/1000: current training loss = 0.717446, accuracy = 62.50%\n",
      "2018-06-08 12:19:19 iteration 550/1000: current training loss = 0.751862, accuracy = 68.75%\n",
      "2018-06-08 12:19:31 iteration 600/1000: current training loss = 0.875945, accuracy = 53.12%\n",
      "2018-06-08 12:19:44 iteration 650/1000: current training loss = 0.615551, accuracy = 81.25%\n",
      "2018-06-08 12:19:55 iteration 700/1000: current training loss = 0.833670, accuracy = 62.50%\n",
      "2018-06-08 12:20:07 iteration 750/1000: current training loss = 0.935940, accuracy = 56.25%\n",
      "2018-06-08 12:20:18 iteration 800/1000: current training loss = 0.776915, accuracy = 65.62%\n",
      "2018-06-08 12:20:30 iteration 850/1000: current training loss = 0.864972, accuracy = 62.50%\n",
      "2018-06-08 12:20:42 iteration 900/1000: current training loss = 0.793325, accuracy = 65.62%\n",
      "2018-06-08 12:20:55 iteration 950/1000: current training loss = 0.887577, accuracy = 50.00%\n",
      "2018-06-08 12:21:07 iteration 1000/1000: current training loss = 0.883200, accuracy = 53.12%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:22:09 end epoch 7/40: acc_train=64.312% acc_val=62.750% acc_test=63.328%\n",
      "\n",
      "2018-06-08 12:22:09 start epoch 8/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:22:10 iteration 1/1000: current training loss = 0.829492, accuracy = 56.25%\n",
      "2018-06-08 12:22:21 iteration 50/1000: current training loss = 0.748820, accuracy = 68.75%\n",
      "2018-06-08 12:22:34 iteration 100/1000: current training loss = 0.665868, accuracy = 75.00%\n",
      "2018-06-08 12:22:46 iteration 150/1000: current training loss = 0.818709, accuracy = 53.12%\n",
      "2018-06-08 12:22:58 iteration 200/1000: current training loss = 0.921932, accuracy = 62.50%\n",
      "2018-06-08 12:23:11 iteration 250/1000: current training loss = 0.589403, accuracy = 71.88%\n",
      "2018-06-08 12:23:23 iteration 300/1000: current training loss = 0.879815, accuracy = 56.25%\n",
      "2018-06-08 12:23:35 iteration 350/1000: current training loss = 0.898277, accuracy = 68.75%\n",
      "2018-06-08 12:23:47 iteration 400/1000: current training loss = 0.901863, accuracy = 53.12%\n",
      "2018-06-08 12:23:59 iteration 450/1000: current training loss = 0.895452, accuracy = 65.62%\n",
      "2018-06-08 12:24:10 iteration 500/1000: current training loss = 0.947351, accuracy = 59.38%\n",
      "2018-06-08 12:24:23 iteration 550/1000: current training loss = 0.794222, accuracy = 62.50%\n",
      "2018-06-08 12:24:35 iteration 600/1000: current training loss = 1.004179, accuracy = 50.00%\n",
      "2018-06-08 12:24:47 iteration 650/1000: current training loss = 0.862201, accuracy = 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 12:24:59 iteration 700/1000: current training loss = 0.705665, accuracy = 59.38%\n",
      "2018-06-08 12:25:11 iteration 750/1000: current training loss = 0.793980, accuracy = 59.38%\n",
      "2018-06-08 12:25:24 iteration 800/1000: current training loss = 0.546389, accuracy = 81.25%\n",
      "2018-06-08 12:25:36 iteration 850/1000: current training loss = 0.778279, accuracy = 62.50%\n",
      "2018-06-08 12:25:47 iteration 900/1000: current training loss = 0.744601, accuracy = 75.00%\n",
      "2018-06-08 12:25:59 iteration 950/1000: current training loss = 0.630521, accuracy = 81.25%\n",
      "2018-06-08 12:26:11 iteration 1000/1000: current training loss = 0.602908, accuracy = 81.25%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:27:13 end epoch 8/40: acc_train=64.578% acc_val=63.625% acc_test=64.547%\n",
      "\n",
      "2018-06-08 12:27:13 start epoch 9/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:27:13 iteration 1/1000: current training loss = 0.864097, accuracy = 65.62%\n",
      "2018-06-08 12:27:25 iteration 50/1000: current training loss = 0.950759, accuracy = 56.25%\n",
      "2018-06-08 12:27:38 iteration 100/1000: current training loss = 0.888320, accuracy = 53.12%\n",
      "2018-06-08 12:27:50 iteration 150/1000: current training loss = 0.676305, accuracy = 71.88%\n",
      "2018-06-08 12:28:02 iteration 200/1000: current training loss = 0.796729, accuracy = 68.75%\n",
      "2018-06-08 12:28:14 iteration 250/1000: current training loss = 0.516095, accuracy = 87.50%\n",
      "2018-06-08 12:28:26 iteration 300/1000: current training loss = 0.692849, accuracy = 62.50%\n",
      "2018-06-08 12:28:38 iteration 350/1000: current training loss = 0.533092, accuracy = 71.88%\n",
      "2018-06-08 12:28:50 iteration 400/1000: current training loss = 0.874182, accuracy = 56.25%\n",
      "2018-06-08 12:29:02 iteration 450/1000: current training loss = 0.825062, accuracy = 56.25%\n",
      "2018-06-08 12:29:13 iteration 500/1000: current training loss = 1.047971, accuracy = 53.12%\n",
      "2018-06-08 12:29:24 iteration 550/1000: current training loss = 0.761240, accuracy = 68.75%\n",
      "2018-06-08 12:29:36 iteration 600/1000: current training loss = 1.058724, accuracy = 56.25%\n",
      "2018-06-08 12:29:48 iteration 650/1000: current training loss = 1.003990, accuracy = 56.25%\n",
      "2018-06-08 12:30:00 iteration 700/1000: current training loss = 0.670619, accuracy = 75.00%\n",
      "2018-06-08 12:30:12 iteration 750/1000: current training loss = 0.658213, accuracy = 75.00%\n",
      "2018-06-08 12:30:24 iteration 800/1000: current training loss = 0.835980, accuracy = 56.25%\n",
      "2018-06-08 12:30:36 iteration 850/1000: current training loss = 0.729490, accuracy = 59.38%\n",
      "2018-06-08 12:30:48 iteration 900/1000: current training loss = 0.644705, accuracy = 81.25%\n",
      "2018-06-08 12:31:00 iteration 950/1000: current training loss = 0.536766, accuracy = 81.25%\n",
      "2018-06-08 12:31:12 iteration 1000/1000: current training loss = 0.771704, accuracy = 65.62%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:32:16 end epoch 9/40: acc_train=65.266% acc_val=63.906% acc_test=62.719%\n",
      "\n",
      "2018-06-08 12:32:16 start epoch 10/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:32:16 iteration 1/1000: current training loss = 0.943648, accuracy = 50.00%\n",
      "2018-06-08 12:32:28 iteration 50/1000: current training loss = 0.516239, accuracy = 81.25%\n",
      "2018-06-08 12:32:40 iteration 100/1000: current training loss = 0.729796, accuracy = 68.75%\n",
      "2018-06-08 12:32:52 iteration 150/1000: current training loss = 0.799276, accuracy = 68.75%\n",
      "2018-06-08 12:33:04 iteration 200/1000: current training loss = 0.703694, accuracy = 71.88%\n",
      "2018-06-08 12:33:16 iteration 250/1000: current training loss = 0.751447, accuracy = 62.50%\n",
      "2018-06-08 12:33:28 iteration 300/1000: current training loss = 0.876048, accuracy = 62.50%\n",
      "2018-06-08 12:33:40 iteration 350/1000: current training loss = 0.932510, accuracy = 56.25%\n",
      "2018-06-08 12:33:52 iteration 400/1000: current training loss = 0.704384, accuracy = 71.88%\n",
      "2018-06-08 12:34:03 iteration 450/1000: current training loss = 0.709776, accuracy = 65.62%\n",
      "2018-06-08 12:34:15 iteration 500/1000: current training loss = 0.886544, accuracy = 56.25%\n",
      "2018-06-08 12:34:28 iteration 550/1000: current training loss = 0.881674, accuracy = 56.25%\n",
      "2018-06-08 12:34:40 iteration 600/1000: current training loss = 0.937810, accuracy = 40.62%\n",
      "2018-06-08 12:34:51 iteration 650/1000: current training loss = 0.943313, accuracy = 50.00%\n",
      "2018-06-08 12:35:03 iteration 700/1000: current training loss = 0.813180, accuracy = 62.50%\n",
      "2018-06-08 12:35:16 iteration 750/1000: current training loss = 0.720783, accuracy = 65.62%\n",
      "2018-06-08 12:35:28 iteration 800/1000: current training loss = 0.722492, accuracy = 62.50%\n",
      "2018-06-08 12:35:39 iteration 850/1000: current training loss = 0.874082, accuracy = 65.62%\n",
      "2018-06-08 12:35:50 iteration 900/1000: current training loss = 1.037546, accuracy = 53.12%\n",
      "2018-06-08 12:36:02 iteration 950/1000: current training loss = 0.623574, accuracy = 78.12%\n",
      "2018-06-08 12:36:14 iteration 1000/1000: current training loss = 0.722288, accuracy = 68.75%\n",
      "2018-06-08 12:37:17 end epoch 10/40: acc_train=66.156% acc_val=63.203% acc_test=63.141%\n",
      "\n",
      "2018-06-08 12:37:17 start epoch 11/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:37:18 iteration 1/1000: current training loss = 0.665253, accuracy = 65.62%\n",
      "2018-06-08 12:37:29 iteration 50/1000: current training loss = 0.921077, accuracy = 56.25%\n",
      "2018-06-08 12:37:40 iteration 100/1000: current training loss = 0.578248, accuracy = 78.12%\n",
      "2018-06-08 12:37:51 iteration 150/1000: current training loss = 1.079458, accuracy = 62.50%\n",
      "2018-06-08 12:38:02 iteration 200/1000: current training loss = 0.727049, accuracy = 59.38%\n",
      "2018-06-08 12:38:13 iteration 250/1000: current training loss = 0.767306, accuracy = 62.50%\n",
      "2018-06-08 12:38:24 iteration 300/1000: current training loss = 1.112238, accuracy = 46.88%\n",
      "2018-06-08 12:38:35 iteration 350/1000: current training loss = 0.681341, accuracy = 75.00%\n",
      "2018-06-08 12:38:47 iteration 400/1000: current training loss = 0.718765, accuracy = 59.38%\n",
      "2018-06-08 12:38:58 iteration 450/1000: current training loss = 0.976947, accuracy = 56.25%\n",
      "2018-06-08 12:39:10 iteration 500/1000: current training loss = 0.895338, accuracy = 59.38%\n",
      "2018-06-08 12:39:20 iteration 550/1000: current training loss = 0.561947, accuracy = 84.38%\n",
      "2018-06-08 12:39:31 iteration 600/1000: current training loss = 0.666228, accuracy = 65.62%\n",
      "2018-06-08 12:39:42 iteration 650/1000: current training loss = 0.882686, accuracy = 50.00%\n",
      "2018-06-08 12:39:53 iteration 700/1000: current training loss = 1.027135, accuracy = 56.25%\n",
      "2018-06-08 12:40:05 iteration 750/1000: current training loss = 0.740953, accuracy = 68.75%\n",
      "2018-06-08 12:40:16 iteration 800/1000: current training loss = 0.857840, accuracy = 65.62%\n",
      "2018-06-08 12:40:27 iteration 850/1000: current training loss = 0.637051, accuracy = 68.75%\n",
      "2018-06-08 12:40:38 iteration 900/1000: current training loss = 1.095624, accuracy = 65.62%\n",
      "2018-06-08 12:40:50 iteration 950/1000: current training loss = 0.869456, accuracy = 46.88%\n",
      "2018-06-08 12:41:01 iteration 1000/1000: current training loss = 0.783475, accuracy = 62.50%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:41:59 end epoch 11/40: acc_train=67.391% acc_val=64.906% acc_test=64.250%\n",
      "\n",
      "2018-06-08 12:41:59 start epoch 12/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:42:00 iteration 1/1000: current training loss = 0.877871, accuracy = 56.25%\n",
      "2018-06-08 12:42:10 iteration 50/1000: current training loss = 0.564224, accuracy = 81.25%\n",
      "2018-06-08 12:42:22 iteration 100/1000: current training loss = 0.692643, accuracy = 78.12%\n",
      "2018-06-08 12:42:33 iteration 150/1000: current training loss = 0.639933, accuracy = 78.12%\n",
      "2018-06-08 12:42:43 iteration 200/1000: current training loss = 0.700968, accuracy = 68.75%\n",
      "2018-06-08 12:42:54 iteration 250/1000: current training loss = 0.965465, accuracy = 59.38%\n",
      "2018-06-08 12:43:05 iteration 300/1000: current training loss = 0.747919, accuracy = 68.75%\n",
      "2018-06-08 12:43:17 iteration 350/1000: current training loss = 1.021977, accuracy = 53.12%\n",
      "2018-06-08 12:43:27 iteration 400/1000: current training loss = 0.711037, accuracy = 65.62%\n",
      "2018-06-08 12:43:38 iteration 450/1000: current training loss = 0.669981, accuracy = 71.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 12:43:51 iteration 500/1000: current training loss = 0.738013, accuracy = 56.25%\n",
      "2018-06-08 12:44:02 iteration 550/1000: current training loss = 0.727702, accuracy = 65.62%\n",
      "2018-06-08 12:44:14 iteration 600/1000: current training loss = 0.865222, accuracy = 75.00%\n",
      "2018-06-08 12:44:26 iteration 650/1000: current training loss = 0.801045, accuracy = 56.25%\n",
      "2018-06-08 12:44:38 iteration 700/1000: current training loss = 0.557987, accuracy = 78.12%\n",
      "2018-06-08 12:44:50 iteration 750/1000: current training loss = 0.968541, accuracy = 59.38%\n",
      "2018-06-08 12:45:01 iteration 800/1000: current training loss = 0.704905, accuracy = 62.50%\n",
      "2018-06-08 12:45:13 iteration 850/1000: current training loss = 0.974019, accuracy = 59.38%\n",
      "2018-06-08 12:45:25 iteration 900/1000: current training loss = 0.619102, accuracy = 71.88%\n",
      "2018-06-08 12:45:35 iteration 950/1000: current training loss = 0.659795, accuracy = 81.25%\n",
      "2018-06-08 12:45:47 iteration 1000/1000: current training loss = 0.804635, accuracy = 71.88%\n",
      "2018-06-08 12:46:43 end epoch 12/40: acc_train=67.016% acc_val=63.750% acc_test=64.281%\n",
      "\n",
      "2018-06-08 12:46:43 start epoch 13/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:46:44 iteration 1/1000: current training loss = 0.852467, accuracy = 62.50%\n",
      "2018-06-08 12:46:54 iteration 50/1000: current training loss = 0.883674, accuracy = 59.38%\n",
      "2018-06-08 12:47:05 iteration 100/1000: current training loss = 0.815759, accuracy = 59.38%\n",
      "2018-06-08 12:47:17 iteration 150/1000: current training loss = 0.620215, accuracy = 75.00%\n",
      "2018-06-08 12:47:28 iteration 200/1000: current training loss = 0.887193, accuracy = 50.00%\n",
      "2018-06-08 12:47:39 iteration 250/1000: current training loss = 0.839078, accuracy = 62.50%\n",
      "2018-06-08 12:47:50 iteration 300/1000: current training loss = 0.811562, accuracy = 59.38%\n",
      "2018-06-08 12:48:01 iteration 350/1000: current training loss = 0.752281, accuracy = 62.50%\n",
      "2018-06-08 12:48:13 iteration 400/1000: current training loss = 0.709999, accuracy = 65.62%\n",
      "2018-06-08 12:48:24 iteration 450/1000: current training loss = 0.581696, accuracy = 78.12%\n",
      "2018-06-08 12:48:35 iteration 500/1000: current training loss = 0.938696, accuracy = 53.12%\n",
      "2018-06-08 12:48:45 iteration 550/1000: current training loss = 0.723176, accuracy = 75.00%\n",
      "2018-06-08 12:48:57 iteration 600/1000: current training loss = 0.923692, accuracy = 62.50%\n",
      "2018-06-08 12:49:08 iteration 650/1000: current training loss = 0.932487, accuracy = 68.75%\n",
      "2018-06-08 12:49:19 iteration 700/1000: current training loss = 0.525022, accuracy = 84.38%\n",
      "2018-06-08 12:49:30 iteration 750/1000: current training loss = 0.970415, accuracy = 59.38%\n",
      "2018-06-08 12:49:42 iteration 800/1000: current training loss = 0.706640, accuracy = 62.50%\n",
      "2018-06-08 12:49:53 iteration 850/1000: current training loss = 0.677748, accuracy = 75.00%\n",
      "2018-06-08 12:50:04 iteration 900/1000: current training loss = 0.689467, accuracy = 71.88%\n",
      "2018-06-08 12:50:16 iteration 950/1000: current training loss = 0.690963, accuracy = 65.62%\n",
      "2018-06-08 12:50:27 iteration 1000/1000: current training loss = 0.712541, accuracy = 68.75%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 12:51:25 end epoch 13/40: acc_train=66.938% acc_val=66.219% acc_test=64.828%\n",
      "\n",
      "2018-06-08 12:51:25 start epoch 14/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:51:25 iteration 1/1000: current training loss = 0.707515, accuracy = 59.38%\n",
      "2018-06-08 12:51:37 iteration 50/1000: current training loss = 0.694160, accuracy = 75.00%\n",
      "2018-06-08 12:51:49 iteration 100/1000: current training loss = 0.716297, accuracy = 71.88%\n",
      "2018-06-08 12:52:00 iteration 150/1000: current training loss = 0.791471, accuracy = 62.50%\n",
      "2018-06-08 12:52:12 iteration 200/1000: current training loss = 0.572403, accuracy = 71.88%\n",
      "2018-06-08 12:52:24 iteration 250/1000: current training loss = 0.706498, accuracy = 68.75%\n",
      "2018-06-08 12:52:35 iteration 300/1000: current training loss = 0.898702, accuracy = 50.00%\n",
      "2018-06-08 12:52:48 iteration 350/1000: current training loss = 0.662391, accuracy = 68.75%\n",
      "2018-06-08 12:52:59 iteration 400/1000: current training loss = 0.600433, accuracy = 75.00%\n",
      "2018-06-08 12:53:11 iteration 450/1000: current training loss = 0.897118, accuracy = 59.38%\n",
      "2018-06-08 12:53:23 iteration 500/1000: current training loss = 0.796371, accuracy = 62.50%\n",
      "2018-06-08 12:53:35 iteration 550/1000: current training loss = 0.835130, accuracy = 65.62%\n",
      "2018-06-08 12:53:47 iteration 600/1000: current training loss = 0.667039, accuracy = 75.00%\n",
      "2018-06-08 12:53:59 iteration 650/1000: current training loss = 0.709183, accuracy = 65.62%\n",
      "2018-06-08 12:54:11 iteration 700/1000: current training loss = 0.724152, accuracy = 62.50%\n",
      "2018-06-08 12:54:23 iteration 750/1000: current training loss = 0.712031, accuracy = 68.75%\n",
      "2018-06-08 12:54:35 iteration 800/1000: current training loss = 0.910993, accuracy = 43.75%\n",
      "2018-06-08 12:54:46 iteration 850/1000: current training loss = 0.677275, accuracy = 65.62%\n",
      "2018-06-08 12:54:57 iteration 900/1000: current training loss = 0.848733, accuracy = 62.50%\n",
      "2018-06-08 12:55:09 iteration 950/1000: current training loss = 0.885255, accuracy = 56.25%\n",
      "2018-06-08 12:55:20 iteration 1000/1000: current training loss = 0.834414, accuracy = 56.25%\n",
      "2018-06-08 12:56:16 end epoch 14/40: acc_train=68.125% acc_val=65.266% acc_test=65.203%\n",
      "\n",
      "2018-06-08 12:56:16 start epoch 15/40, with learning rate = 0.0004000000\n",
      "2018-06-08 12:56:16 iteration 1/1000: current training loss = 0.717586, accuracy = 68.75%\n",
      "2018-06-08 12:56:28 iteration 50/1000: current training loss = 0.662099, accuracy = 75.00%\n",
      "2018-06-08 12:56:39 iteration 100/1000: current training loss = 0.561790, accuracy = 68.75%\n",
      "2018-06-08 12:56:50 iteration 150/1000: current training loss = 0.749033, accuracy = 62.50%\n",
      "2018-06-08 12:57:01 iteration 200/1000: current training loss = 0.894064, accuracy = 59.38%\n",
      "2018-06-08 12:57:12 iteration 250/1000: current training loss = 0.576130, accuracy = 75.00%\n",
      "2018-06-08 12:57:23 iteration 300/1000: current training loss = 0.908103, accuracy = 53.12%\n",
      "2018-06-08 12:57:34 iteration 350/1000: current training loss = 0.804351, accuracy = 59.38%\n",
      "2018-06-08 12:57:45 iteration 400/1000: current training loss = 0.844328, accuracy = 65.62%\n",
      "2018-06-08 12:57:56 iteration 450/1000: current training loss = 0.765610, accuracy = 65.62%\n",
      "2018-06-08 12:58:07 iteration 500/1000: current training loss = 0.684074, accuracy = 71.88%\n",
      "2018-06-08 12:58:19 iteration 550/1000: current training loss = 0.700103, accuracy = 68.75%\n",
      "2018-06-08 12:58:30 iteration 600/1000: current training loss = 0.664071, accuracy = 71.88%\n",
      "2018-06-08 12:58:41 iteration 650/1000: current training loss = 0.618306, accuracy = 71.88%\n",
      "2018-06-08 12:58:54 iteration 700/1000: current training loss = 0.898808, accuracy = 43.75%\n",
      "2018-06-08 12:59:05 iteration 750/1000: current training loss = 0.954568, accuracy = 46.88%\n",
      "2018-06-08 12:59:16 iteration 800/1000: current training loss = 0.760476, accuracy = 71.88%\n",
      "2018-06-08 12:59:28 iteration 850/1000: current training loss = 0.976503, accuracy = 56.25%\n",
      "2018-06-08 12:59:40 iteration 900/1000: current training loss = 0.621698, accuracy = 78.12%\n",
      "2018-06-08 12:59:52 iteration 950/1000: current training loss = 0.629537, accuracy = 71.88%\n",
      "2018-06-08 13:00:03 iteration 1000/1000: current training loss = 0.610047, accuracy = 71.88%\n",
      "2018-06-08 13:01:06 end epoch 15/40: acc_train=67.734% acc_val=64.328% acc_test=64.562%\n",
      "\n",
      "2018-06-08 13:01:06 start epoch 16/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:01:07 iteration 1/1000: current training loss = 0.719402, accuracy = 62.50%\n",
      "2018-06-08 13:01:18 iteration 50/1000: current training loss = 0.806124, accuracy = 62.50%\n",
      "2018-06-08 13:01:28 iteration 100/1000: current training loss = 0.770684, accuracy = 71.88%\n",
      "2018-06-08 13:01:40 iteration 150/1000: current training loss = 0.845600, accuracy = 65.62%\n",
      "2018-06-08 13:01:51 iteration 200/1000: current training loss = 0.891591, accuracy = 53.12%\n",
      "2018-06-08 13:02:02 iteration 250/1000: current training loss = 0.825141, accuracy = 59.38%\n",
      "2018-06-08 13:02:14 iteration 300/1000: current training loss = 0.757897, accuracy = 62.50%\n",
      "2018-06-08 13:02:25 iteration 350/1000: current training loss = 0.658471, accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 13:02:36 iteration 400/1000: current training loss = 0.840888, accuracy = 59.38%\n",
      "2018-06-08 13:02:47 iteration 450/1000: current training loss = 0.811620, accuracy = 62.50%\n",
      "2018-06-08 13:02:59 iteration 500/1000: current training loss = 0.753767, accuracy = 56.25%\n",
      "2018-06-08 13:03:10 iteration 550/1000: current training loss = 0.868338, accuracy = 62.50%\n",
      "2018-06-08 13:03:21 iteration 600/1000: current training loss = 0.797162, accuracy = 68.75%\n",
      "2018-06-08 13:03:32 iteration 650/1000: current training loss = 0.536798, accuracy = 78.12%\n",
      "2018-06-08 13:03:43 iteration 700/1000: current training loss = 0.754617, accuracy = 59.38%\n",
      "2018-06-08 13:03:55 iteration 750/1000: current training loss = 0.917868, accuracy = 56.25%\n",
      "2018-06-08 13:04:06 iteration 800/1000: current training loss = 0.743592, accuracy = 78.12%\n",
      "2018-06-08 13:04:17 iteration 850/1000: current training loss = 0.547415, accuracy = 75.00%\n",
      "2018-06-08 13:04:29 iteration 900/1000: current training loss = 0.733557, accuracy = 56.25%\n",
      "2018-06-08 13:04:40 iteration 950/1000: current training loss = 0.644432, accuracy = 75.00%\n",
      "2018-06-08 13:04:52 iteration 1000/1000: current training loss = 0.733714, accuracy = 65.62%\n",
      "2018-06-08 13:05:48 end epoch 16/40: acc_train=67.125% acc_val=64.391% acc_test=64.797%\n",
      "\n",
      "2018-06-08 13:05:48 start epoch 17/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:05:48 iteration 1/1000: current training loss = 0.686249, accuracy = 71.88%\n",
      "2018-06-08 13:05:59 iteration 50/1000: current training loss = 0.593958, accuracy = 78.12%\n",
      "2018-06-08 13:06:11 iteration 100/1000: current training loss = 0.728066, accuracy = 75.00%\n",
      "2018-06-08 13:06:23 iteration 150/1000: current training loss = 0.669938, accuracy = 71.88%\n",
      "2018-06-08 13:06:34 iteration 200/1000: current training loss = 0.735337, accuracy = 75.00%\n",
      "2018-06-08 13:06:45 iteration 250/1000: current training loss = 0.726904, accuracy = 65.62%\n",
      "2018-06-08 13:06:55 iteration 300/1000: current training loss = 0.913991, accuracy = 46.88%\n",
      "2018-06-08 13:07:06 iteration 350/1000: current training loss = 0.603948, accuracy = 65.62%\n",
      "2018-06-08 13:07:18 iteration 400/1000: current training loss = 0.661321, accuracy = 62.50%\n",
      "2018-06-08 13:07:29 iteration 450/1000: current training loss = 0.687104, accuracy = 71.88%\n",
      "2018-06-08 13:07:40 iteration 500/1000: current training loss = 0.825477, accuracy = 53.12%\n",
      "2018-06-08 13:07:51 iteration 550/1000: current training loss = 0.972139, accuracy = 65.62%\n",
      "2018-06-08 13:08:02 iteration 600/1000: current training loss = 0.748116, accuracy = 65.62%\n",
      "2018-06-08 13:08:13 iteration 650/1000: current training loss = 0.563229, accuracy = 84.38%\n",
      "2018-06-08 13:08:23 iteration 700/1000: current training loss = 0.755676, accuracy = 59.38%\n",
      "2018-06-08 13:08:34 iteration 750/1000: current training loss = 0.821515, accuracy = 65.62%\n",
      "2018-06-08 13:08:45 iteration 800/1000: current training loss = 0.725213, accuracy = 62.50%\n",
      "2018-06-08 13:08:56 iteration 850/1000: current training loss = 0.764574, accuracy = 62.50%\n",
      "2018-06-08 13:09:06 iteration 900/1000: current training loss = 0.689884, accuracy = 68.75%\n",
      "2018-06-08 13:09:18 iteration 950/1000: current training loss = 0.974282, accuracy = 56.25%\n",
      "2018-06-08 13:09:28 iteration 1000/1000: current training loss = 0.723496, accuracy = 68.75%\n",
      "2018-06-08 13:10:25 end epoch 17/40: acc_train=68.453% acc_val=64.578% acc_test=65.312%\n",
      "\n",
      "2018-06-08 13:10:25 start epoch 18/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:10:25 iteration 1/1000: current training loss = 0.818558, accuracy = 62.50%\n",
      "2018-06-08 13:10:37 iteration 50/1000: current training loss = 0.644238, accuracy = 75.00%\n",
      "2018-06-08 13:10:48 iteration 100/1000: current training loss = 0.784894, accuracy = 62.50%\n",
      "2018-06-08 13:10:59 iteration 150/1000: current training loss = 0.735326, accuracy = 68.75%\n",
      "2018-06-08 13:11:09 iteration 200/1000: current training loss = 0.807135, accuracy = 68.75%\n",
      "2018-06-08 13:11:20 iteration 250/1000: current training loss = 0.714476, accuracy = 71.88%\n",
      "2018-06-08 13:11:32 iteration 300/1000: current training loss = 0.698630, accuracy = 65.62%\n",
      "2018-06-08 13:11:43 iteration 350/1000: current training loss = 0.972051, accuracy = 50.00%\n",
      "2018-06-08 13:11:54 iteration 400/1000: current training loss = 0.659096, accuracy = 68.75%\n",
      "2018-06-08 13:12:05 iteration 450/1000: current training loss = 0.708795, accuracy = 71.88%\n",
      "2018-06-08 13:12:16 iteration 500/1000: current training loss = 0.596217, accuracy = 71.88%\n",
      "2018-06-08 13:12:27 iteration 550/1000: current training loss = 0.762292, accuracy = 68.75%\n",
      "2018-06-08 13:12:39 iteration 600/1000: current training loss = 1.056421, accuracy = 53.12%\n",
      "2018-06-08 13:12:50 iteration 650/1000: current training loss = 0.648204, accuracy = 78.12%\n",
      "2018-06-08 13:13:01 iteration 700/1000: current training loss = 0.566562, accuracy = 56.25%\n",
      "2018-06-08 13:13:12 iteration 750/1000: current training loss = 0.765280, accuracy = 62.50%\n",
      "2018-06-08 13:13:24 iteration 800/1000: current training loss = 0.650195, accuracy = 68.75%\n",
      "2018-06-08 13:13:35 iteration 850/1000: current training loss = 0.558573, accuracy = 68.75%\n",
      "2018-06-08 13:13:47 iteration 900/1000: current training loss = 1.233097, accuracy = 43.75%\n",
      "2018-06-08 13:13:59 iteration 950/1000: current training loss = 0.653132, accuracy = 75.00%\n",
      "2018-06-08 13:14:11 iteration 1000/1000: current training loss = 0.883154, accuracy = 59.38%\n",
      "2018-06-08 13:15:14 end epoch 18/40: acc_train=68.062% acc_val=65.266% acc_test=65.609%\n",
      "\n",
      "2018-06-08 13:15:14 start epoch 19/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:15:15 iteration 1/1000: current training loss = 0.648105, accuracy = 75.00%\n",
      "2018-06-08 13:15:26 iteration 50/1000: current training loss = 0.586765, accuracy = 81.25%\n",
      "2018-06-08 13:15:37 iteration 100/1000: current training loss = 0.853927, accuracy = 71.88%\n",
      "2018-06-08 13:15:49 iteration 150/1000: current training loss = 0.507822, accuracy = 81.25%\n",
      "2018-06-08 13:16:00 iteration 200/1000: current training loss = 0.728352, accuracy = 65.62%\n",
      "2018-06-08 13:16:12 iteration 250/1000: current training loss = 0.636833, accuracy = 75.00%\n",
      "2018-06-08 13:16:24 iteration 300/1000: current training loss = 0.759229, accuracy = 65.62%\n",
      "2018-06-08 13:16:35 iteration 350/1000: current training loss = 0.796072, accuracy = 59.38%\n",
      "2018-06-08 13:16:46 iteration 400/1000: current training loss = 0.603554, accuracy = 75.00%\n",
      "2018-06-08 13:16:57 iteration 450/1000: current training loss = 0.625759, accuracy = 71.88%\n",
      "2018-06-08 13:17:09 iteration 500/1000: current training loss = 0.788221, accuracy = 62.50%\n",
      "2018-06-08 13:17:21 iteration 550/1000: current training loss = 0.540107, accuracy = 75.00%\n",
      "2018-06-08 13:17:33 iteration 600/1000: current training loss = 0.931886, accuracy = 59.38%\n",
      "2018-06-08 13:17:45 iteration 650/1000: current training loss = 0.751803, accuracy = 65.62%\n",
      "2018-06-08 13:17:58 iteration 700/1000: current training loss = 0.839510, accuracy = 59.38%\n",
      "2018-06-08 13:18:10 iteration 750/1000: current training loss = 0.647992, accuracy = 71.88%\n",
      "2018-06-08 13:18:22 iteration 800/1000: current training loss = 0.716113, accuracy = 65.62%\n",
      "2018-06-08 13:18:34 iteration 850/1000: current training loss = 0.796032, accuracy = 71.88%\n",
      "2018-06-08 13:18:46 iteration 900/1000: current training loss = 0.575488, accuracy = 78.12%\n",
      "2018-06-08 13:18:59 iteration 950/1000: current training loss = 0.739833, accuracy = 65.62%\n",
      "2018-06-08 13:19:11 iteration 1000/1000: current training loss = 0.819333, accuracy = 65.62%\n",
      "2018-06-08 13:20:12 end epoch 19/40: acc_train=66.109% acc_val=65.203% acc_test=64.750%\n",
      "\n",
      "2018-06-08 13:20:12 start epoch 20/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:20:13 iteration 1/1000: current training loss = 0.771936, accuracy = 59.38%\n",
      "2018-06-08 13:20:25 iteration 50/1000: current training loss = 0.601813, accuracy = 71.88%\n",
      "2018-06-08 13:20:36 iteration 100/1000: current training loss = 0.612850, accuracy = 68.75%\n",
      "2018-06-08 13:20:48 iteration 150/1000: current training loss = 0.829573, accuracy = 56.25%\n",
      "2018-06-08 13:21:00 iteration 200/1000: current training loss = 0.720673, accuracy = 62.50%\n",
      "2018-06-08 13:21:13 iteration 250/1000: current training loss = 0.741270, accuracy = 71.88%\n",
      "2018-06-08 13:21:25 iteration 300/1000: current training loss = 0.669225, accuracy = 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 13:21:36 iteration 350/1000: current training loss = 0.851680, accuracy = 59.38%\n",
      "2018-06-08 13:21:48 iteration 400/1000: current training loss = 0.754382, accuracy = 65.62%\n",
      "2018-06-08 13:22:00 iteration 450/1000: current training loss = 0.889253, accuracy = 62.50%\n",
      "2018-06-08 13:22:12 iteration 500/1000: current training loss = 0.968392, accuracy = 59.38%\n",
      "2018-06-08 13:22:24 iteration 550/1000: current training loss = 0.667658, accuracy = 65.62%\n",
      "2018-06-08 13:22:35 iteration 600/1000: current training loss = 0.606240, accuracy = 81.25%\n",
      "2018-06-08 13:22:48 iteration 650/1000: current training loss = 0.693620, accuracy = 62.50%\n",
      "2018-06-08 13:23:00 iteration 700/1000: current training loss = 0.596498, accuracy = 65.62%\n",
      "2018-06-08 13:23:11 iteration 750/1000: current training loss = 0.677024, accuracy = 68.75%\n",
      "2018-06-08 13:23:23 iteration 800/1000: current training loss = 0.598504, accuracy = 68.75%\n",
      "2018-06-08 13:23:35 iteration 850/1000: current training loss = 0.767549, accuracy = 62.50%\n",
      "2018-06-08 13:23:47 iteration 900/1000: current training loss = 0.799060, accuracy = 65.62%\n",
      "2018-06-08 13:23:59 iteration 950/1000: current training loss = 0.541229, accuracy = 71.88%\n",
      "2018-06-08 13:24:11 iteration 1000/1000: current training loss = 0.691940, accuracy = 68.75%\n",
      "2018-06-08 13:25:15 end epoch 20/40: acc_train=68.531% acc_val=64.406% acc_test=64.656%\n",
      "\n",
      "2018-06-08 13:25:15 start epoch 21/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:25:16 iteration 1/1000: current training loss = 0.836048, accuracy = 56.25%\n",
      "2018-06-08 13:25:28 iteration 50/1000: current training loss = 0.913218, accuracy = 68.75%\n",
      "2018-06-08 13:25:40 iteration 100/1000: current training loss = 0.805099, accuracy = 56.25%\n",
      "2018-06-08 13:25:52 iteration 150/1000: current training loss = 0.837369, accuracy = 65.62%\n",
      "2018-06-08 13:26:04 iteration 200/1000: current training loss = 0.538949, accuracy = 71.88%\n",
      "2018-06-08 13:26:16 iteration 250/1000: current training loss = 0.734128, accuracy = 78.12%\n",
      "2018-06-08 13:26:29 iteration 300/1000: current training loss = 0.615440, accuracy = 71.88%\n",
      "2018-06-08 13:26:41 iteration 350/1000: current training loss = 0.520887, accuracy = 78.12%\n",
      "2018-06-08 13:26:53 iteration 400/1000: current training loss = 0.660092, accuracy = 62.50%\n",
      "2018-06-08 13:27:05 iteration 450/1000: current training loss = 0.608289, accuracy = 81.25%\n",
      "2018-06-08 13:27:17 iteration 500/1000: current training loss = 0.670567, accuracy = 65.62%\n",
      "2018-06-08 13:27:29 iteration 550/1000: current training loss = 0.554159, accuracy = 71.88%\n",
      "2018-06-08 13:27:42 iteration 600/1000: current training loss = 0.761674, accuracy = 62.50%\n",
      "2018-06-08 13:27:53 iteration 650/1000: current training loss = 0.783676, accuracy = 65.62%\n",
      "2018-06-08 13:28:05 iteration 700/1000: current training loss = 0.780111, accuracy = 71.88%\n",
      "2018-06-08 13:28:16 iteration 750/1000: current training loss = 0.727503, accuracy = 62.50%\n",
      "2018-06-08 13:28:28 iteration 800/1000: current training loss = 0.540004, accuracy = 78.12%\n",
      "2018-06-08 13:28:40 iteration 850/1000: current training loss = 0.556972, accuracy = 71.88%\n",
      "2018-06-08 13:28:52 iteration 900/1000: current training loss = 0.582117, accuracy = 78.12%\n",
      "2018-06-08 13:29:05 iteration 950/1000: current training loss = 0.545776, accuracy = 78.12%\n",
      "2018-06-08 13:29:18 iteration 1000/1000: current training loss = 0.588467, accuracy = 75.00%\n",
      "2018-06-08 13:30:20 end epoch 21/40: acc_train=69.250% acc_val=65.328% acc_test=65.750%\n",
      "\n",
      "2018-06-08 13:30:20 start epoch 22/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:30:20 iteration 1/1000: current training loss = 0.598247, accuracy = 71.88%\n",
      "2018-06-08 13:30:32 iteration 50/1000: current training loss = 0.496712, accuracy = 78.12%\n",
      "2018-06-08 13:30:44 iteration 100/1000: current training loss = 0.692551, accuracy = 71.88%\n",
      "2018-06-08 13:30:56 iteration 150/1000: current training loss = 0.782509, accuracy = 65.62%\n",
      "2018-06-08 13:31:08 iteration 200/1000: current training loss = 0.543387, accuracy = 71.88%\n",
      "2018-06-08 13:31:20 iteration 250/1000: current training loss = 0.710653, accuracy = 68.75%\n",
      "2018-06-08 13:31:32 iteration 300/1000: current training loss = 0.619505, accuracy = 78.12%\n",
      "2018-06-08 13:31:43 iteration 350/1000: current training loss = 0.870961, accuracy = 62.50%\n",
      "2018-06-08 13:31:55 iteration 400/1000: current training loss = 0.538346, accuracy = 71.88%\n",
      "2018-06-08 13:32:06 iteration 450/1000: current training loss = 0.698849, accuracy = 71.88%\n",
      "2018-06-08 13:32:18 iteration 500/1000: current training loss = 1.108681, accuracy = 59.38%\n",
      "2018-06-08 13:32:30 iteration 550/1000: current training loss = 0.695002, accuracy = 75.00%\n",
      "2018-06-08 13:32:42 iteration 600/1000: current training loss = 0.900696, accuracy = 56.25%\n",
      "2018-06-08 13:32:55 iteration 650/1000: current training loss = 0.801092, accuracy = 59.38%\n",
      "2018-06-08 13:33:09 iteration 700/1000: current training loss = 0.724378, accuracy = 65.62%\n",
      "2018-06-08 13:33:20 iteration 750/1000: current training loss = 0.915746, accuracy = 65.62%\n",
      "2018-06-08 13:33:32 iteration 800/1000: current training loss = 0.830115, accuracy = 65.62%\n",
      "2018-06-08 13:33:44 iteration 850/1000: current training loss = 0.784255, accuracy = 62.50%\n",
      "2018-06-08 13:33:55 iteration 900/1000: current training loss = 0.542864, accuracy = 78.12%\n",
      "2018-06-08 13:34:08 iteration 950/1000: current training loss = 0.626818, accuracy = 62.50%\n",
      "2018-06-08 13:34:20 iteration 1000/1000: current training loss = 0.654405, accuracy = 71.88%\n",
      "2018-06-08 13:35:22 end epoch 22/40: acc_train=69.875% acc_val=64.828% acc_test=65.219%\n",
      "\n",
      "2018-06-08 13:35:22 start epoch 23/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:35:22 iteration 1/1000: current training loss = 0.729527, accuracy = 62.50%\n",
      "2018-06-08 13:35:33 iteration 50/1000: current training loss = 0.635291, accuracy = 75.00%\n",
      "2018-06-08 13:35:45 iteration 100/1000: current training loss = 0.645820, accuracy = 62.50%\n",
      "2018-06-08 13:35:57 iteration 150/1000: current training loss = 0.709520, accuracy = 75.00%\n",
      "2018-06-08 13:36:09 iteration 200/1000: current training loss = 0.735208, accuracy = 68.75%\n",
      "2018-06-08 13:36:20 iteration 250/1000: current training loss = 0.587977, accuracy = 81.25%\n",
      "2018-06-08 13:36:32 iteration 300/1000: current training loss = 0.800089, accuracy = 56.25%\n",
      "2018-06-08 13:36:44 iteration 350/1000: current training loss = 0.717715, accuracy = 68.75%\n",
      "2018-06-08 13:36:57 iteration 400/1000: current training loss = 0.818534, accuracy = 56.25%\n",
      "2018-06-08 13:37:09 iteration 450/1000: current training loss = 0.646187, accuracy = 65.62%\n",
      "2018-06-08 13:37:21 iteration 500/1000: current training loss = 0.802143, accuracy = 65.62%\n",
      "2018-06-08 13:37:32 iteration 550/1000: current training loss = 0.815095, accuracy = 71.88%\n",
      "2018-06-08 13:37:44 iteration 600/1000: current training loss = 0.615764, accuracy = 68.75%\n",
      "2018-06-08 13:37:56 iteration 650/1000: current training loss = 0.737503, accuracy = 75.00%\n",
      "2018-06-08 13:38:07 iteration 700/1000: current training loss = 0.584985, accuracy = 75.00%\n",
      "2018-06-08 13:38:19 iteration 750/1000: current training loss = 0.584853, accuracy = 71.88%\n",
      "2018-06-08 13:38:32 iteration 800/1000: current training loss = 0.572096, accuracy = 75.00%\n",
      "2018-06-08 13:38:43 iteration 850/1000: current training loss = 0.618404, accuracy = 75.00%\n",
      "2018-06-08 13:38:56 iteration 900/1000: current training loss = 0.700768, accuracy = 71.88%\n",
      "2018-06-08 13:39:07 iteration 950/1000: current training loss = 0.775065, accuracy = 71.88%\n",
      "2018-06-08 13:39:18 iteration 1000/1000: current training loss = 0.805690, accuracy = 56.25%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-06-08 13:40:22 end epoch 23/40: acc_train=69.562% acc_val=66.266% acc_test=65.953%\n",
      "\n",
      "2018-06-08 13:40:22 start epoch 24/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:40:22 iteration 1/1000: current training loss = 0.547708, accuracy = 75.00%\n",
      "2018-06-08 13:40:34 iteration 50/1000: current training loss = 0.791727, accuracy = 68.75%\n",
      "2018-06-08 13:40:46 iteration 100/1000: current training loss = 0.777954, accuracy = 65.62%\n",
      "2018-06-08 13:40:58 iteration 150/1000: current training loss = 0.545573, accuracy = 75.00%\n",
      "2018-06-08 13:41:11 iteration 200/1000: current training loss = 0.658846, accuracy = 68.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 13:41:23 iteration 250/1000: current training loss = 0.646153, accuracy = 65.62%\n",
      "2018-06-08 13:41:35 iteration 300/1000: current training loss = 0.776113, accuracy = 56.25%\n",
      "2018-06-08 13:41:48 iteration 350/1000: current training loss = 0.503597, accuracy = 81.25%\n",
      "2018-06-08 13:41:59 iteration 400/1000: current training loss = 0.910049, accuracy = 59.38%\n",
      "2018-06-08 13:42:11 iteration 450/1000: current training loss = 0.728306, accuracy = 68.75%\n",
      "2018-06-08 13:42:24 iteration 500/1000: current training loss = 0.633463, accuracy = 78.12%\n",
      "2018-06-08 13:42:35 iteration 550/1000: current training loss = 0.593789, accuracy = 75.00%\n",
      "2018-06-08 13:42:47 iteration 600/1000: current training loss = 0.846001, accuracy = 53.12%\n",
      "2018-06-08 13:42:59 iteration 650/1000: current training loss = 0.670992, accuracy = 62.50%\n",
      "2018-06-08 13:43:11 iteration 700/1000: current training loss = 0.648927, accuracy = 78.12%\n",
      "2018-06-08 13:43:23 iteration 750/1000: current training loss = 0.668980, accuracy = 68.75%\n",
      "2018-06-08 13:43:33 iteration 800/1000: current training loss = 0.793357, accuracy = 65.62%\n",
      "2018-06-08 13:43:46 iteration 850/1000: current training loss = 0.551570, accuracy = 81.25%\n",
      "2018-06-08 13:43:57 iteration 900/1000: current training loss = 0.647777, accuracy = 68.75%\n",
      "2018-06-08 13:44:08 iteration 950/1000: current training loss = 0.677093, accuracy = 71.88%\n",
      "2018-06-08 13:44:20 iteration 1000/1000: current training loss = 0.635080, accuracy = 71.88%\n",
      "2018-06-08 13:45:21 end epoch 24/40: acc_train=68.484% acc_val=65.250% acc_test=64.453%\n",
      "\n",
      "2018-06-08 13:45:21 start epoch 25/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:45:21 iteration 1/1000: current training loss = 0.610376, accuracy = 75.00%\n",
      "2018-06-08 13:45:34 iteration 50/1000: current training loss = 0.748276, accuracy = 78.12%\n",
      "2018-06-08 13:45:45 iteration 100/1000: current training loss = 0.893687, accuracy = 65.62%\n",
      "2018-06-08 13:45:56 iteration 150/1000: current training loss = 0.439600, accuracy = 81.25%\n",
      "2018-06-08 13:46:08 iteration 200/1000: current training loss = 0.804366, accuracy = 68.75%\n",
      "2018-06-08 13:46:20 iteration 250/1000: current training loss = 0.824759, accuracy = 62.50%\n",
      "2018-06-08 13:46:31 iteration 300/1000: current training loss = 0.793035, accuracy = 62.50%\n",
      "2018-06-08 13:46:44 iteration 350/1000: current training loss = 0.639469, accuracy = 68.75%\n",
      "2018-06-08 13:46:55 iteration 400/1000: current training loss = 0.796131, accuracy = 71.88%\n",
      "2018-06-08 13:47:08 iteration 450/1000: current training loss = 0.842458, accuracy = 68.75%\n",
      "2018-06-08 13:47:20 iteration 500/1000: current training loss = 0.692191, accuracy = 68.75%\n",
      "2018-06-08 13:47:32 iteration 550/1000: current training loss = 0.591735, accuracy = 65.62%\n",
      "2018-06-08 13:47:43 iteration 600/1000: current training loss = 0.560194, accuracy = 78.12%\n",
      "2018-06-08 13:47:57 iteration 650/1000: current training loss = 0.647445, accuracy = 78.12%\n",
      "2018-06-08 13:48:08 iteration 700/1000: current training loss = 0.934375, accuracy = 50.00%\n",
      "2018-06-08 13:48:20 iteration 750/1000: current training loss = 0.651003, accuracy = 65.62%\n",
      "2018-06-08 13:48:31 iteration 800/1000: current training loss = 0.646698, accuracy = 68.75%\n",
      "2018-06-08 13:48:43 iteration 850/1000: current training loss = 0.761705, accuracy = 59.38%\n",
      "2018-06-08 13:48:54 iteration 900/1000: current training loss = 0.762693, accuracy = 62.50%\n",
      "2018-06-08 13:49:04 iteration 950/1000: current training loss = 0.677015, accuracy = 71.88%\n",
      "2018-06-08 13:49:15 iteration 1000/1000: current training loss = 0.848323, accuracy = 65.62%\n",
      "2018-06-08 13:50:16 end epoch 25/40: acc_train=69.250% acc_val=64.812% acc_test=65.781%\n",
      "\n",
      "2018-06-08 13:50:16 start epoch 26/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:50:17 iteration 1/1000: current training loss = 0.531234, accuracy = 81.25%\n",
      "2018-06-08 13:50:28 iteration 50/1000: current training loss = 0.639398, accuracy = 65.62%\n",
      "2018-06-08 13:50:40 iteration 100/1000: current training loss = 0.603536, accuracy = 75.00%\n",
      "2018-06-08 13:50:52 iteration 150/1000: current training loss = 0.811229, accuracy = 68.75%\n",
      "2018-06-08 13:51:03 iteration 200/1000: current training loss = 0.594263, accuracy = 75.00%\n",
      "2018-06-08 13:51:15 iteration 250/1000: current training loss = 0.519531, accuracy = 84.38%\n",
      "2018-06-08 13:51:26 iteration 300/1000: current training loss = 0.628725, accuracy = 81.25%\n",
      "2018-06-08 13:51:37 iteration 350/1000: current training loss = 0.703504, accuracy = 71.88%\n",
      "2018-06-08 13:51:49 iteration 400/1000: current training loss = 0.610651, accuracy = 62.50%\n",
      "2018-06-08 13:51:59 iteration 450/1000: current training loss = 0.563656, accuracy = 75.00%\n",
      "2018-06-08 13:52:11 iteration 500/1000: current training loss = 0.535896, accuracy = 68.75%\n",
      "2018-06-08 13:52:22 iteration 550/1000: current training loss = 0.778029, accuracy = 65.62%\n",
      "2018-06-08 13:52:33 iteration 600/1000: current training loss = 0.611136, accuracy = 75.00%\n",
      "2018-06-08 13:52:44 iteration 650/1000: current training loss = 0.815039, accuracy = 53.12%\n",
      "2018-06-08 13:52:55 iteration 700/1000: current training loss = 0.842027, accuracy = 59.38%\n",
      "2018-06-08 13:53:06 iteration 750/1000: current training loss = 0.603591, accuracy = 71.88%\n",
      "2018-06-08 13:53:18 iteration 800/1000: current training loss = 0.671055, accuracy = 71.88%\n",
      "2018-06-08 13:53:29 iteration 850/1000: current training loss = 0.582485, accuracy = 71.88%\n",
      "2018-06-08 13:53:41 iteration 900/1000: current training loss = 0.439362, accuracy = 81.25%\n",
      "2018-06-08 13:53:55 iteration 950/1000: current training loss = 0.539455, accuracy = 78.12%\n",
      "2018-06-08 13:54:06 iteration 1000/1000: current training loss = 0.679701, accuracy = 65.62%\n",
      "2018-06-08 13:55:01 end epoch 26/40: acc_train=70.828% acc_val=65.562% acc_test=65.438%\n",
      "\n",
      "2018-06-08 13:55:01 start epoch 27/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:55:02 iteration 1/1000: current training loss = 0.756246, accuracy = 65.62%\n",
      "2018-06-08 13:55:13 iteration 50/1000: current training loss = 0.731573, accuracy = 65.62%\n",
      "2018-06-08 13:55:24 iteration 100/1000: current training loss = 0.710979, accuracy = 62.50%\n",
      "2018-06-08 13:55:36 iteration 150/1000: current training loss = 0.663995, accuracy = 75.00%\n",
      "2018-06-08 13:55:47 iteration 200/1000: current training loss = 0.576665, accuracy = 78.12%\n",
      "2018-06-08 13:55:59 iteration 250/1000: current training loss = 0.518155, accuracy = 78.12%\n",
      "2018-06-08 13:56:09 iteration 300/1000: current training loss = 0.680253, accuracy = 71.88%\n",
      "2018-06-08 13:56:20 iteration 350/1000: current training loss = 0.630553, accuracy = 71.88%\n",
      "2018-06-08 13:56:31 iteration 400/1000: current training loss = 0.769949, accuracy = 68.75%\n",
      "2018-06-08 13:56:45 iteration 450/1000: current training loss = 0.752403, accuracy = 65.62%\n",
      "2018-06-08 13:56:55 iteration 500/1000: current training loss = 0.467244, accuracy = 84.38%\n",
      "2018-06-08 13:57:06 iteration 550/1000: current training loss = 0.768677, accuracy = 65.62%\n",
      "2018-06-08 13:57:18 iteration 600/1000: current training loss = 0.648398, accuracy = 71.88%\n",
      "2018-06-08 13:57:30 iteration 650/1000: current training loss = 0.916248, accuracy = 59.38%\n",
      "2018-06-08 13:57:41 iteration 700/1000: current training loss = 0.697541, accuracy = 68.75%\n",
      "2018-06-08 13:57:52 iteration 750/1000: current training loss = 0.610539, accuracy = 71.88%\n",
      "2018-06-08 13:58:04 iteration 800/1000: current training loss = 0.770718, accuracy = 62.50%\n",
      "2018-06-08 13:58:14 iteration 850/1000: current training loss = 0.738226, accuracy = 75.00%\n",
      "2018-06-08 13:58:25 iteration 900/1000: current training loss = 0.704460, accuracy = 75.00%\n",
      "2018-06-08 13:58:37 iteration 950/1000: current training loss = 0.633649, accuracy = 59.38%\n",
      "2018-06-08 13:58:49 iteration 1000/1000: current training loss = 0.701945, accuracy = 75.00%\n",
      "2018-06-08 13:59:45 end epoch 27/40: acc_train=70.750% acc_val=64.203% acc_test=65.969%\n",
      "\n",
      "2018-06-08 13:59:45 start epoch 28/40, with learning rate = 0.0004000000\n",
      "2018-06-08 13:59:46 iteration 1/1000: current training loss = 0.728120, accuracy = 59.38%\n",
      "2018-06-08 13:59:57 iteration 50/1000: current training loss = 0.603881, accuracy = 71.88%\n",
      "2018-06-08 14:00:08 iteration 100/1000: current training loss = 0.736408, accuracy = 59.38%\n",
      "2018-06-08 14:00:18 iteration 150/1000: current training loss = 0.925181, accuracy = 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 14:00:29 iteration 200/1000: current training loss = 0.627467, accuracy = 71.88%\n",
      "2018-06-08 14:00:40 iteration 250/1000: current training loss = 0.538222, accuracy = 84.38%\n",
      "2018-06-08 14:00:50 iteration 300/1000: current training loss = 1.017553, accuracy = 56.25%\n",
      "2018-06-08 14:01:01 iteration 350/1000: current training loss = 0.653781, accuracy = 71.88%\n",
      "2018-06-08 14:01:12 iteration 400/1000: current training loss = 0.929099, accuracy = 62.50%\n",
      "2018-06-08 14:01:23 iteration 450/1000: current training loss = 0.834757, accuracy = 62.50%\n",
      "2018-06-08 14:01:35 iteration 500/1000: current training loss = 0.765986, accuracy = 68.75%\n",
      "2018-06-08 14:01:45 iteration 550/1000: current training loss = 0.559715, accuracy = 75.00%\n",
      "2018-06-08 14:01:56 iteration 600/1000: current training loss = 0.887909, accuracy = 59.38%\n",
      "2018-06-08 14:02:08 iteration 650/1000: current training loss = 0.662309, accuracy = 71.88%\n",
      "2018-06-08 14:02:18 iteration 700/1000: current training loss = 0.520584, accuracy = 81.25%\n",
      "2018-06-08 14:02:29 iteration 750/1000: current training loss = 0.694248, accuracy = 71.88%\n",
      "2018-06-08 14:02:39 iteration 800/1000: current training loss = 0.652898, accuracy = 71.88%\n",
      "2018-06-08 14:02:49 iteration 850/1000: current training loss = 0.548326, accuracy = 81.25%\n",
      "2018-06-08 14:03:00 iteration 900/1000: current training loss = 0.686927, accuracy = 81.25%\n",
      "2018-06-08 14:03:12 iteration 950/1000: current training loss = 0.485313, accuracy = 87.50%\n",
      "2018-06-08 14:03:23 iteration 1000/1000: current training loss = 0.603510, accuracy = 71.88%\n",
      "2018-06-08 14:04:18 end epoch 28/40: acc_train=72.031% acc_val=64.938% acc_test=65.641%\n",
      "\n",
      "2018-06-08 14:04:18 start epoch 29/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:04:18 iteration 1/1000: current training loss = 0.521576, accuracy = 78.12%\n",
      "2018-06-08 14:04:29 iteration 50/1000: current training loss = 0.546590, accuracy = 65.62%\n",
      "2018-06-08 14:04:41 iteration 100/1000: current training loss = 0.564419, accuracy = 65.62%\n",
      "2018-06-08 14:04:52 iteration 150/1000: current training loss = 0.857020, accuracy = 59.38%\n",
      "2018-06-08 14:05:04 iteration 200/1000: current training loss = 0.571037, accuracy = 71.88%\n",
      "2018-06-08 14:05:15 iteration 250/1000: current training loss = 0.801636, accuracy = 78.12%\n",
      "2018-06-08 14:05:27 iteration 300/1000: current training loss = 0.499095, accuracy = 78.12%\n",
      "2018-06-08 14:05:38 iteration 350/1000: current training loss = 0.693157, accuracy = 68.75%\n",
      "2018-06-08 14:05:49 iteration 400/1000: current training loss = 0.776626, accuracy = 59.38%\n",
      "2018-06-08 14:05:59 iteration 450/1000: current training loss = 0.702915, accuracy = 78.12%\n",
      "2018-06-08 14:06:10 iteration 500/1000: current training loss = 0.786935, accuracy = 56.25%\n",
      "2018-06-08 14:06:20 iteration 550/1000: current training loss = 0.754265, accuracy = 56.25%\n",
      "2018-06-08 14:06:31 iteration 600/1000: current training loss = 0.559292, accuracy = 71.88%\n",
      "2018-06-08 14:06:42 iteration 650/1000: current training loss = 0.538849, accuracy = 84.38%\n",
      "2018-06-08 14:06:53 iteration 700/1000: current training loss = 0.732258, accuracy = 71.88%\n",
      "2018-06-08 14:07:06 iteration 750/1000: current training loss = 0.809167, accuracy = 56.25%\n",
      "2018-06-08 14:07:17 iteration 800/1000: current training loss = 0.464417, accuracy = 75.00%\n",
      "2018-06-08 14:07:29 iteration 850/1000: current training loss = 0.502176, accuracy = 84.38%\n",
      "2018-06-08 14:07:41 iteration 900/1000: current training loss = 0.611938, accuracy = 78.12%\n",
      "2018-06-08 14:07:52 iteration 950/1000: current training loss = 0.595495, accuracy = 71.88%\n",
      "2018-06-08 14:08:04 iteration 1000/1000: current training loss = 0.700241, accuracy = 75.00%\n",
      "2018-06-08 14:09:04 end epoch 29/40: acc_train=71.672% acc_val=65.703% acc_test=65.422%\n",
      "\n",
      "2018-06-08 14:09:04 start epoch 30/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:09:04 iteration 1/1000: current training loss = 0.615581, accuracy = 75.00%\n",
      "2018-06-08 14:09:16 iteration 50/1000: current training loss = 0.702231, accuracy = 71.88%\n",
      "2018-06-08 14:09:29 iteration 100/1000: current training loss = 0.610685, accuracy = 71.88%\n",
      "2018-06-08 14:09:40 iteration 150/1000: current training loss = 0.343739, accuracy = 90.62%\n",
      "2018-06-08 14:09:52 iteration 200/1000: current training loss = 0.662146, accuracy = 68.75%\n",
      "2018-06-08 14:10:04 iteration 250/1000: current training loss = 0.695234, accuracy = 65.62%\n",
      "2018-06-08 14:10:15 iteration 300/1000: current training loss = 0.609718, accuracy = 71.88%\n",
      "2018-06-08 14:10:27 iteration 350/1000: current training loss = 0.678953, accuracy = 65.62%\n",
      "2018-06-08 14:10:38 iteration 400/1000: current training loss = 0.900365, accuracy = 68.75%\n",
      "2018-06-08 14:10:49 iteration 450/1000: current training loss = 0.775060, accuracy = 71.88%\n",
      "2018-06-08 14:11:01 iteration 500/1000: current training loss = 0.589327, accuracy = 87.50%\n",
      "2018-06-08 14:11:14 iteration 550/1000: current training loss = 0.539331, accuracy = 75.00%\n",
      "2018-06-08 14:11:25 iteration 600/1000: current training loss = 0.787057, accuracy = 62.50%\n",
      "2018-06-08 14:11:36 iteration 650/1000: current training loss = 0.743179, accuracy = 62.50%\n",
      "2018-06-08 14:11:48 iteration 700/1000: current training loss = 0.461992, accuracy = 87.50%\n",
      "2018-06-08 14:12:00 iteration 750/1000: current training loss = 0.758246, accuracy = 65.62%\n",
      "2018-06-08 14:12:12 iteration 800/1000: current training loss = 0.657253, accuracy = 78.12%\n",
      "2018-06-08 14:12:25 iteration 850/1000: current training loss = 0.901104, accuracy = 71.88%\n",
      "2018-06-08 14:12:37 iteration 900/1000: current training loss = 0.653021, accuracy = 71.88%\n",
      "2018-06-08 14:12:48 iteration 950/1000: current training loss = 0.748158, accuracy = 62.50%\n",
      "2018-06-08 14:13:00 iteration 1000/1000: current training loss = 0.666726, accuracy = 75.00%\n",
      "2018-06-08 14:14:00 end epoch 30/40: acc_train=71.594% acc_val=65.078% acc_test=65.172%\n",
      "\n",
      "2018-06-08 14:14:00 start epoch 31/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:14:00 iteration 1/1000: current training loss = 0.573034, accuracy = 71.88%\n",
      "2018-06-08 14:14:12 iteration 50/1000: current training loss = 0.394730, accuracy = 84.38%\n",
      "2018-06-08 14:14:25 iteration 100/1000: current training loss = 0.466771, accuracy = 84.38%\n",
      "2018-06-08 14:14:37 iteration 150/1000: current training loss = 0.555615, accuracy = 81.25%\n",
      "2018-06-08 14:14:48 iteration 200/1000: current training loss = 0.485447, accuracy = 78.12%\n",
      "2018-06-08 14:15:00 iteration 250/1000: current training loss = 0.727953, accuracy = 65.62%\n",
      "2018-06-08 14:15:12 iteration 300/1000: current training loss = 0.508307, accuracy = 81.25%\n",
      "2018-06-08 14:15:23 iteration 350/1000: current training loss = 0.671451, accuracy = 68.75%\n",
      "2018-06-08 14:15:35 iteration 400/1000: current training loss = 0.566859, accuracy = 68.75%\n",
      "2018-06-08 14:15:47 iteration 450/1000: current training loss = 0.621345, accuracy = 75.00%\n",
      "2018-06-08 14:15:59 iteration 500/1000: current training loss = 0.570023, accuracy = 78.12%\n",
      "2018-06-08 14:16:11 iteration 550/1000: current training loss = 0.582694, accuracy = 71.88%\n",
      "2018-06-08 14:16:22 iteration 600/1000: current training loss = 0.482373, accuracy = 84.38%\n",
      "2018-06-08 14:16:34 iteration 650/1000: current training loss = 0.601358, accuracy = 78.12%\n",
      "2018-06-08 14:16:46 iteration 700/1000: current training loss = 0.763425, accuracy = 65.62%\n",
      "2018-06-08 14:16:58 iteration 750/1000: current training loss = 0.817537, accuracy = 62.50%\n",
      "2018-06-08 14:17:10 iteration 800/1000: current training loss = 0.628805, accuracy = 78.12%\n",
      "2018-06-08 14:17:22 iteration 850/1000: current training loss = 0.689606, accuracy = 71.88%\n",
      "2018-06-08 14:17:35 iteration 900/1000: current training loss = 0.739847, accuracy = 65.62%\n",
      "2018-06-08 14:17:47 iteration 950/1000: current training loss = 0.730989, accuracy = 75.00%\n",
      "2018-06-08 14:17:58 iteration 1000/1000: current training loss = 0.511582, accuracy = 78.12%\n",
      "2018-06-08 14:18:56 end epoch 31/40: acc_train=70.141% acc_val=65.172% acc_test=66.188%\n",
      "\n",
      "2018-06-08 14:18:56 start epoch 32/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:18:57 iteration 1/1000: current training loss = 0.548683, accuracy = 81.25%\n",
      "2018-06-08 14:19:09 iteration 50/1000: current training loss = 0.590671, accuracy = 68.75%\n",
      "2018-06-08 14:19:21 iteration 100/1000: current training loss = 0.551309, accuracy = 81.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 14:19:33 iteration 150/1000: current training loss = 0.509427, accuracy = 75.00%\n",
      "2018-06-08 14:19:45 iteration 200/1000: current training loss = 0.586727, accuracy = 75.00%\n",
      "2018-06-08 14:19:57 iteration 250/1000: current training loss = 0.516681, accuracy = 78.12%\n",
      "2018-06-08 14:20:09 iteration 300/1000: current training loss = 0.773246, accuracy = 75.00%\n",
      "2018-06-08 14:20:21 iteration 350/1000: current training loss = 0.770884, accuracy = 68.75%\n",
      "2018-06-08 14:20:33 iteration 400/1000: current training loss = 0.968042, accuracy = 56.25%\n",
      "2018-06-08 14:20:44 iteration 450/1000: current training loss = 0.792398, accuracy = 68.75%\n",
      "2018-06-08 14:20:56 iteration 500/1000: current training loss = 0.459999, accuracy = 81.25%\n",
      "2018-06-08 14:21:07 iteration 550/1000: current training loss = 0.457952, accuracy = 90.62%\n",
      "2018-06-08 14:21:19 iteration 600/1000: current training loss = 0.690529, accuracy = 68.75%\n",
      "2018-06-08 14:21:30 iteration 650/1000: current training loss = 0.943922, accuracy = 62.50%\n",
      "2018-06-08 14:21:42 iteration 700/1000: current training loss = 0.621317, accuracy = 68.75%\n",
      "2018-06-08 14:21:55 iteration 750/1000: current training loss = 0.582642, accuracy = 75.00%\n",
      "2018-06-08 14:22:07 iteration 800/1000: current training loss = 0.598554, accuracy = 68.75%\n",
      "2018-06-08 14:22:19 iteration 850/1000: current training loss = 0.835071, accuracy = 59.38%\n",
      "2018-06-08 14:22:30 iteration 900/1000: current training loss = 0.717254, accuracy = 71.88%\n",
      "2018-06-08 14:22:42 iteration 950/1000: current training loss = 0.686302, accuracy = 62.50%\n",
      "2018-06-08 14:22:53 iteration 1000/1000: current training loss = 0.620642, accuracy = 68.75%\n",
      "2018-06-08 14:23:52 end epoch 32/40: acc_train=72.031% acc_val=65.125% acc_test=64.109%\n",
      "\n",
      "2018-06-08 14:23:52 start epoch 33/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:23:53 iteration 1/1000: current training loss = 0.623927, accuracy = 71.88%\n",
      "2018-06-08 14:24:04 iteration 50/1000: current training loss = 0.707822, accuracy = 75.00%\n",
      "2018-06-08 14:24:16 iteration 100/1000: current training loss = 0.640167, accuracy = 65.62%\n",
      "2018-06-08 14:24:27 iteration 150/1000: current training loss = 0.679919, accuracy = 71.88%\n",
      "2018-06-08 14:24:39 iteration 200/1000: current training loss = 0.778471, accuracy = 68.75%\n",
      "2018-06-08 14:24:51 iteration 250/1000: current training loss = 0.537263, accuracy = 78.12%\n",
      "2018-06-08 14:25:02 iteration 300/1000: current training loss = 0.574553, accuracy = 71.88%\n",
      "2018-06-08 14:25:13 iteration 350/1000: current training loss = 0.767550, accuracy = 65.62%\n",
      "2018-06-08 14:25:25 iteration 400/1000: current training loss = 0.551087, accuracy = 75.00%\n",
      "2018-06-08 14:25:37 iteration 450/1000: current training loss = 0.741231, accuracy = 65.62%\n",
      "2018-06-08 14:25:48 iteration 500/1000: current training loss = 0.739329, accuracy = 59.38%\n",
      "2018-06-08 14:26:00 iteration 550/1000: current training loss = 0.643646, accuracy = 65.62%\n",
      "2018-06-08 14:26:10 iteration 600/1000: current training loss = 0.842994, accuracy = 62.50%\n",
      "2018-06-08 14:26:22 iteration 650/1000: current training loss = 0.911106, accuracy = 53.12%\n",
      "2018-06-08 14:26:34 iteration 700/1000: current training loss = 0.619605, accuracy = 71.88%\n",
      "2018-06-08 14:26:45 iteration 750/1000: current training loss = 0.474898, accuracy = 84.38%\n",
      "2018-06-08 14:26:56 iteration 800/1000: current training loss = 0.887392, accuracy = 65.62%\n",
      "2018-06-08 14:27:08 iteration 850/1000: current training loss = 0.619695, accuracy = 71.88%\n",
      "2018-06-08 14:27:20 iteration 900/1000: current training loss = 0.617654, accuracy = 81.25%\n",
      "2018-06-08 14:27:32 iteration 950/1000: current training loss = 0.722149, accuracy = 68.75%\n",
      "2018-06-08 14:27:43 iteration 1000/1000: current training loss = 0.749871, accuracy = 71.88%\n",
      "2018-06-08 14:28:41 end epoch 33/40: acc_train=71.359% acc_val=65.297% acc_test=65.484%\n",
      "\n",
      "2018-06-08 14:28:41 start epoch 34/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:28:42 iteration 1/1000: current training loss = 0.773407, accuracy = 68.75%\n",
      "2018-06-08 14:28:53 iteration 50/1000: current training loss = 0.672576, accuracy = 68.75%\n",
      "2018-06-08 14:29:05 iteration 100/1000: current training loss = 0.729091, accuracy = 65.62%\n",
      "2018-06-08 14:29:16 iteration 150/1000: current training loss = 0.567382, accuracy = 78.12%\n",
      "2018-06-08 14:29:28 iteration 200/1000: current training loss = 0.813804, accuracy = 68.75%\n",
      "2018-06-08 14:29:41 iteration 250/1000: current training loss = 0.729647, accuracy = 62.50%\n",
      "2018-06-08 14:29:52 iteration 300/1000: current training loss = 0.699273, accuracy = 65.62%\n",
      "2018-06-08 14:30:04 iteration 350/1000: current training loss = 0.645437, accuracy = 75.00%\n",
      "2018-06-08 14:30:15 iteration 400/1000: current training loss = 0.412909, accuracy = 84.38%\n",
      "2018-06-08 14:30:27 iteration 450/1000: current training loss = 0.636032, accuracy = 68.75%\n",
      "2018-06-08 14:30:38 iteration 500/1000: current training loss = 0.800383, accuracy = 56.25%\n",
      "2018-06-08 14:30:49 iteration 550/1000: current training loss = 0.530789, accuracy = 81.25%\n",
      "2018-06-08 14:30:59 iteration 600/1000: current training loss = 0.658879, accuracy = 68.75%\n",
      "2018-06-08 14:31:11 iteration 650/1000: current training loss = 0.532322, accuracy = 75.00%\n",
      "2018-06-08 14:31:22 iteration 700/1000: current training loss = 0.532334, accuracy = 78.12%\n",
      "2018-06-08 14:31:34 iteration 750/1000: current training loss = 0.566631, accuracy = 71.88%\n",
      "2018-06-08 14:31:46 iteration 800/1000: current training loss = 0.661411, accuracy = 59.38%\n",
      "2018-06-08 14:31:57 iteration 850/1000: current training loss = 0.564929, accuracy = 68.75%\n",
      "2018-06-08 14:32:10 iteration 900/1000: current training loss = 0.726272, accuracy = 65.62%\n",
      "2018-06-08 14:32:22 iteration 950/1000: current training loss = 0.686783, accuracy = 62.50%\n",
      "2018-06-08 14:32:34 iteration 1000/1000: current training loss = 0.759065, accuracy = 75.00%\n",
      "2018-06-08 14:33:32 end epoch 34/40: acc_train=71.078% acc_val=65.969% acc_test=65.016%\n",
      "\n",
      "2018-06-08 14:33:32 start epoch 35/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:33:32 iteration 1/1000: current training loss = 0.824485, accuracy = 62.50%\n",
      "2018-06-08 14:33:44 iteration 50/1000: current training loss = 0.773836, accuracy = 65.62%\n",
      "2018-06-08 14:33:56 iteration 100/1000: current training loss = 0.634588, accuracy = 78.12%\n",
      "2018-06-08 14:34:08 iteration 150/1000: current training loss = 0.611395, accuracy = 75.00%\n",
      "2018-06-08 14:34:20 iteration 200/1000: current training loss = 0.565933, accuracy = 81.25%\n",
      "2018-06-08 14:34:31 iteration 250/1000: current training loss = 0.482196, accuracy = 87.50%\n",
      "2018-06-08 14:34:42 iteration 300/1000: current training loss = 0.625498, accuracy = 71.88%\n",
      "2018-06-08 14:34:54 iteration 350/1000: current training loss = 0.845301, accuracy = 62.50%\n",
      "2018-06-08 14:35:06 iteration 400/1000: current training loss = 0.691955, accuracy = 75.00%\n",
      "2018-06-08 14:35:19 iteration 450/1000: current training loss = 0.485843, accuracy = 78.12%\n",
      "2018-06-08 14:35:30 iteration 500/1000: current training loss = 0.551647, accuracy = 68.75%\n",
      "2018-06-08 14:35:42 iteration 550/1000: current training loss = 0.624668, accuracy = 71.88%\n",
      "2018-06-08 14:35:54 iteration 600/1000: current training loss = 0.754232, accuracy = 65.62%\n",
      "2018-06-08 14:36:06 iteration 650/1000: current training loss = 0.759317, accuracy = 59.38%\n",
      "2018-06-08 14:36:17 iteration 700/1000: current training loss = 0.638750, accuracy = 81.25%\n",
      "2018-06-08 14:36:30 iteration 750/1000: current training loss = 0.728089, accuracy = 65.62%\n",
      "2018-06-08 14:36:41 iteration 800/1000: current training loss = 0.715283, accuracy = 75.00%\n",
      "2018-06-08 14:36:53 iteration 850/1000: current training loss = 0.568314, accuracy = 78.12%\n",
      "2018-06-08 14:37:04 iteration 900/1000: current training loss = 0.532547, accuracy = 75.00%\n",
      "2018-06-08 14:37:16 iteration 950/1000: current training loss = 0.567790, accuracy = 68.75%\n",
      "2018-06-08 14:37:28 iteration 1000/1000: current training loss = 0.962604, accuracy = 46.88%\n",
      "2018-06-08 14:38:25 end epoch 35/40: acc_train=72.312% acc_val=64.016% acc_test=64.594%\n",
      "\n",
      "2018-06-08 14:38:25 start epoch 36/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:38:26 iteration 1/1000: current training loss = 0.653416, accuracy = 71.88%\n",
      "2018-06-08 14:38:37 iteration 50/1000: current training loss = 0.511069, accuracy = 81.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 14:38:48 iteration 100/1000: current training loss = 0.724761, accuracy = 59.38%\n",
      "2018-06-08 14:39:00 iteration 150/1000: current training loss = 0.630584, accuracy = 62.50%\n",
      "2018-06-08 14:39:11 iteration 200/1000: current training loss = 0.560388, accuracy = 78.12%\n",
      "2018-06-08 14:39:23 iteration 250/1000: current training loss = 0.752191, accuracy = 65.62%\n",
      "2018-06-08 14:39:35 iteration 300/1000: current training loss = 0.520718, accuracy = 78.12%\n",
      "2018-06-08 14:39:46 iteration 350/1000: current training loss = 0.654997, accuracy = 68.75%\n",
      "2018-06-08 14:39:58 iteration 400/1000: current training loss = 0.719185, accuracy = 65.62%\n",
      "2018-06-08 14:40:10 iteration 450/1000: current training loss = 0.838470, accuracy = 71.88%\n",
      "2018-06-08 14:40:22 iteration 500/1000: current training loss = 0.792549, accuracy = 75.00%\n",
      "2018-06-08 14:40:32 iteration 550/1000: current training loss = 0.491112, accuracy = 87.50%\n",
      "2018-06-08 14:40:44 iteration 600/1000: current training loss = 0.745576, accuracy = 68.75%\n",
      "2018-06-08 14:40:55 iteration 650/1000: current training loss = 0.876484, accuracy = 65.62%\n",
      "2018-06-08 14:41:07 iteration 700/1000: current training loss = 0.456902, accuracy = 81.25%\n",
      "2018-06-08 14:41:18 iteration 750/1000: current training loss = 0.623295, accuracy = 75.00%\n",
      "2018-06-08 14:41:29 iteration 800/1000: current training loss = 0.673012, accuracy = 65.62%\n",
      "2018-06-08 14:41:40 iteration 850/1000: current training loss = 0.561096, accuracy = 78.12%\n",
      "2018-06-08 14:41:51 iteration 900/1000: current training loss = 0.599500, accuracy = 78.12%\n",
      "2018-06-08 14:42:03 iteration 950/1000: current training loss = 0.543079, accuracy = 78.12%\n",
      "2018-06-08 14:42:15 iteration 1000/1000: current training loss = 0.675088, accuracy = 62.50%\n",
      "2018-06-08 14:43:13 end epoch 36/40: acc_train=73.281% acc_val=64.281% acc_test=64.109%\n",
      "\n",
      "2018-06-08 14:43:13 start epoch 37/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:43:14 iteration 1/1000: current training loss = 0.686401, accuracy = 68.75%\n",
      "2018-06-08 14:43:24 iteration 50/1000: current training loss = 0.614977, accuracy = 71.88%\n",
      "2018-06-08 14:43:36 iteration 100/1000: current training loss = 0.677432, accuracy = 78.12%\n",
      "2018-06-08 14:43:48 iteration 150/1000: current training loss = 0.668752, accuracy = 71.88%\n",
      "2018-06-08 14:44:00 iteration 200/1000: current training loss = 0.429625, accuracy = 87.50%\n",
      "2018-06-08 14:44:12 iteration 250/1000: current training loss = 0.656373, accuracy = 62.50%\n",
      "2018-06-08 14:44:23 iteration 300/1000: current training loss = 0.458466, accuracy = 87.50%\n",
      "2018-06-08 14:44:34 iteration 350/1000: current training loss = 0.522485, accuracy = 71.88%\n",
      "2018-06-08 14:44:47 iteration 400/1000: current training loss = 0.626752, accuracy = 75.00%\n",
      "2018-06-08 14:44:58 iteration 450/1000: current training loss = 0.856497, accuracy = 62.50%\n",
      "2018-06-08 14:45:10 iteration 500/1000: current training loss = 0.746986, accuracy = 75.00%\n",
      "2018-06-08 14:45:21 iteration 550/1000: current training loss = 0.689745, accuracy = 75.00%\n",
      "2018-06-08 14:45:32 iteration 600/1000: current training loss = 0.905338, accuracy = 56.25%\n",
      "2018-06-08 14:45:44 iteration 650/1000: current training loss = 0.571176, accuracy = 71.88%\n",
      "2018-06-08 14:45:56 iteration 700/1000: current training loss = 0.589409, accuracy = 59.38%\n",
      "2018-06-08 14:46:08 iteration 750/1000: current training loss = 0.781726, accuracy = 62.50%\n",
      "2018-06-08 14:46:20 iteration 800/1000: current training loss = 0.629910, accuracy = 68.75%\n",
      "2018-06-08 14:46:32 iteration 850/1000: current training loss = 0.543640, accuracy = 81.25%\n",
      "2018-06-08 14:46:43 iteration 900/1000: current training loss = 0.802267, accuracy = 62.50%\n",
      "2018-06-08 14:46:55 iteration 950/1000: current training loss = 0.524972, accuracy = 75.00%\n",
      "2018-06-08 14:47:06 iteration 1000/1000: current training loss = 0.549359, accuracy = 78.12%\n",
      "2018-06-08 14:48:03 end epoch 37/40: acc_train=73.422% acc_val=64.188% acc_test=63.969%\n",
      "\n",
      "2018-06-08 14:48:03 start epoch 38/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:48:04 iteration 1/1000: current training loss = 0.371528, accuracy = 90.62%\n",
      "2018-06-08 14:48:16 iteration 50/1000: current training loss = 0.773499, accuracy = 62.50%\n",
      "2018-06-08 14:48:28 iteration 100/1000: current training loss = 0.701883, accuracy = 65.62%\n",
      "2018-06-08 14:48:39 iteration 150/1000: current training loss = 0.601138, accuracy = 78.12%\n",
      "2018-06-08 14:48:51 iteration 200/1000: current training loss = 0.581323, accuracy = 75.00%\n",
      "2018-06-08 14:49:03 iteration 250/1000: current training loss = 0.524363, accuracy = 78.12%\n",
      "2018-06-08 14:49:15 iteration 300/1000: current training loss = 0.669679, accuracy = 65.62%\n",
      "2018-06-08 14:49:25 iteration 350/1000: current training loss = 0.401906, accuracy = 78.12%\n",
      "2018-06-08 14:49:37 iteration 400/1000: current training loss = 0.665741, accuracy = 78.12%\n",
      "2018-06-08 14:49:49 iteration 450/1000: current training loss = 0.495131, accuracy = 75.00%\n",
      "2018-06-08 14:50:00 iteration 500/1000: current training loss = 0.544864, accuracy = 78.12%\n",
      "2018-06-08 14:50:11 iteration 550/1000: current training loss = 0.480073, accuracy = 75.00%\n",
      "2018-06-08 14:50:23 iteration 600/1000: current training loss = 0.763600, accuracy = 62.50%\n",
      "2018-06-08 14:50:34 iteration 650/1000: current training loss = 0.490273, accuracy = 81.25%\n",
      "2018-06-08 14:50:46 iteration 700/1000: current training loss = 0.713815, accuracy = 62.50%\n",
      "2018-06-08 14:50:59 iteration 750/1000: current training loss = 0.779572, accuracy = 75.00%\n",
      "2018-06-08 14:51:11 iteration 800/1000: current training loss = 0.860661, accuracy = 62.50%\n",
      "2018-06-08 14:51:23 iteration 850/1000: current training loss = 0.613557, accuracy = 71.88%\n",
      "2018-06-08 14:51:35 iteration 900/1000: current training loss = 0.718217, accuracy = 68.75%\n",
      "2018-06-08 14:51:47 iteration 950/1000: current training loss = 0.586703, accuracy = 75.00%\n",
      "2018-06-08 14:51:58 iteration 1000/1000: current training loss = 0.785820, accuracy = 68.75%\n",
      "2018-06-08 14:52:57 end epoch 38/40: acc_train=73.734% acc_val=64.750% acc_test=65.297%\n",
      "\n",
      "2018-06-08 14:52:57 start epoch 39/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:52:57 iteration 1/1000: current training loss = 0.430384, accuracy = 84.38%\n",
      "2018-06-08 14:53:08 iteration 50/1000: current training loss = 0.670169, accuracy = 71.88%\n",
      "2018-06-08 14:53:19 iteration 100/1000: current training loss = 0.639619, accuracy = 68.75%\n",
      "2018-06-08 14:53:30 iteration 150/1000: current training loss = 0.422220, accuracy = 87.50%\n",
      "2018-06-08 14:53:43 iteration 200/1000: current training loss = 0.615084, accuracy = 71.88%\n",
      "2018-06-08 14:53:54 iteration 250/1000: current training loss = 0.424375, accuracy = 81.25%\n",
      "2018-06-08 14:54:06 iteration 300/1000: current training loss = 0.581480, accuracy = 84.38%\n",
      "2018-06-08 14:54:18 iteration 350/1000: current training loss = 0.621398, accuracy = 75.00%\n",
      "2018-06-08 14:54:30 iteration 400/1000: current training loss = 0.617053, accuracy = 68.75%\n",
      "2018-06-08 14:54:42 iteration 450/1000: current training loss = 0.830090, accuracy = 62.50%\n",
      "2018-06-08 14:54:53 iteration 500/1000: current training loss = 0.724776, accuracy = 68.75%\n",
      "2018-06-08 14:55:04 iteration 550/1000: current training loss = 0.525557, accuracy = 81.25%\n",
      "2018-06-08 14:55:15 iteration 600/1000: current training loss = 0.434058, accuracy = 84.38%\n",
      "2018-06-08 14:55:27 iteration 650/1000: current training loss = 0.575754, accuracy = 78.12%\n",
      "2018-06-08 14:55:39 iteration 700/1000: current training loss = 0.482590, accuracy = 81.25%\n",
      "2018-06-08 14:55:50 iteration 750/1000: current training loss = 0.744443, accuracy = 71.88%\n",
      "2018-06-08 14:56:02 iteration 800/1000: current training loss = 0.510425, accuracy = 81.25%\n",
      "2018-06-08 14:56:13 iteration 850/1000: current training loss = 0.704292, accuracy = 65.62%\n",
      "2018-06-08 14:56:25 iteration 900/1000: current training loss = 0.643546, accuracy = 65.62%\n",
      "2018-06-08 14:56:37 iteration 950/1000: current training loss = 0.623735, accuracy = 65.62%\n",
      "2018-06-08 14:56:48 iteration 1000/1000: current training loss = 0.525313, accuracy = 78.12%\n",
      "2018-06-08 14:57:47 end epoch 39/40: acc_train=74.703% acc_val=64.328% acc_test=65.453%\n",
      "\n",
      "2018-06-08 14:57:47 start epoch 40/40, with learning rate = 0.0004000000\n",
      "2018-06-08 14:57:47 iteration 1/1000: current training loss = 0.648715, accuracy = 78.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 14:57:58 iteration 50/1000: current training loss = 0.492230, accuracy = 81.25%\n",
      "2018-06-08 14:58:10 iteration 100/1000: current training loss = 0.588687, accuracy = 75.00%\n",
      "2018-06-08 14:58:22 iteration 150/1000: current training loss = 0.628864, accuracy = 75.00%\n",
      "2018-06-08 14:58:33 iteration 200/1000: current training loss = 0.687604, accuracy = 59.38%\n",
      "2018-06-08 14:58:44 iteration 250/1000: current training loss = 0.588050, accuracy = 71.88%\n",
      "2018-06-08 14:58:56 iteration 300/1000: current training loss = 0.615026, accuracy = 65.62%\n",
      "2018-06-08 14:59:07 iteration 350/1000: current training loss = 0.598101, accuracy = 75.00%\n",
      "2018-06-08 14:59:19 iteration 400/1000: current training loss = 0.467385, accuracy = 78.12%\n",
      "2018-06-08 14:59:30 iteration 450/1000: current training loss = 0.679060, accuracy = 75.00%\n",
      "2018-06-08 14:59:42 iteration 500/1000: current training loss = 0.653222, accuracy = 65.62%\n",
      "2018-06-08 14:59:54 iteration 550/1000: current training loss = 0.677991, accuracy = 71.88%\n",
      "2018-06-08 15:00:06 iteration 600/1000: current training loss = 1.009954, accuracy = 62.50%\n",
      "2018-06-08 15:00:18 iteration 650/1000: current training loss = 0.776982, accuracy = 62.50%\n",
      "2018-06-08 15:00:30 iteration 700/1000: current training loss = 0.597526, accuracy = 75.00%\n",
      "2018-06-08 15:00:42 iteration 750/1000: current training loss = 0.600403, accuracy = 75.00%\n",
      "2018-06-08 15:00:54 iteration 800/1000: current training loss = 0.564297, accuracy = 71.88%\n",
      "2018-06-08 15:01:06 iteration 850/1000: current training loss = 0.706979, accuracy = 71.88%\n",
      "2018-06-08 15:01:18 iteration 900/1000: current training loss = 0.621241, accuracy = 71.88%\n",
      "2018-06-08 15:01:30 iteration 950/1000: current training loss = 0.645488, accuracy = 65.62%\n",
      "2018-06-08 15:01:41 iteration 1000/1000: current training loss = 0.762816, accuracy = 68.75%\n",
      "2018-06-08 15:02:43 end epoch 40/40: acc_train=73.422% acc_val=64.406% acc_test=65.594%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "loss_train_his=[]\n",
    "loss_val_his=[]\n",
    "loss_test_his=[]\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    max_acc=None\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'start epoch %d/%d, with learning rate = %.10f' % (epoch+1,max_epoch,sess.run(learning_rate)))\n",
    "        \n",
    "        train.start_epoch()\n",
    "#         num_iteration=num_train//bs\n",
    "        num_iteration=1000   # equalvalent to sampling a subset of training set to train\n",
    "        for it in range(num_iteration):\n",
    "            output,document_sizes,sentence_sizes,labels=train.next_batch(bs)\n",
    "            feed_dict={X_sentence:output,y:labels,sentence_length:sentence_sizes.reshape(-1,),\n",
    "                       document_length:document_sizes,is_training:True,dropout:0.0}\n",
    "            loss_num,acc_num,_=sess.run([loss,accuracy,train_step],feed_dict=feed_dict)\n",
    "            if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                      'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f, accuracy = %.2f%%' % (loss_num,acc_num*100.0))\n",
    "        \n",
    "        loss_train,acc_train=eval(train_eval,200)  # sample some documents to test\n",
    "        loss_val,acc_val=eval(validation,200)\n",
    "        loss_test,acc_test=eval(test,200)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        loss_train_his.append(loss_train)\n",
    "        loss_val_his.append(loss_val)\n",
    "        loss_test_his.append(loss_test)\n",
    "        \n",
    "        if max_acc==None or acc_val>max_acc:\n",
    "            max_acc=acc_val\n",
    "            save_path = saver.save(sess, \"parameters/HAN.ckpt\")\n",
    "            print(\"Currently maximum accuracy on validation set, model saved in path: %s\" % save_path)\n",
    "        \n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.savefig('model-acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),loss_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),loss_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),loss_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss function on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.savefig('model-loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-08 15:20:23,376 : INFO : Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set is 69.312%\n",
      "Accuracy on validation set is 64.922%\n",
      "Accuracy on testing set is 65.969%\n",
      "\n",
      "Total loss on training set is 0.693348\n",
      "Total loss on validation set is 0.780674\n",
      "Total loss on testing set is 0.771007\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"parameters/HAN.ckpt\")\n",
    "    loss_train,acc_train=eval(train_eval,1000)\n",
    "    loss_val,acc_val=eval(validation,1000)\n",
    "    loss_test,acc_test=eval(test,1000)\n",
    "    print('Accuracy on training set is %.3f%%' % (acc_train*100.0))\n",
    "    print('Accuracy on validation set is %.3f%%' % (acc_val*100.0))\n",
    "    print('Accuracy on testing set is %.3f%%' % (acc_test*100.0))\n",
    "    print()\n",
    "    print('Total loss on training set is %f' % loss_train)\n",
    "    print('Total loss on validation set is %f' % loss_val)\n",
    "    print('Total loss on testing set is %f' % loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
