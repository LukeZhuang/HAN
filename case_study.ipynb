{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import random\n",
    "from HAN import HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-08 15:29:17,098 : INFO : loading projection weights from data/yelp2013/yelp_2013_50.vector\n",
      "2018-06-08 15:29:19,489 : INFO : loaded (43531, 50) matrix from data/yelp2013/yelp_2013_50.vector\n"
     ]
    }
   ],
   "source": [
    "year='2013'\n",
    "word2vec_size=50\n",
    "cut_long_sentence=200\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 43531\n"
     ]
    }
   ],
   "source": [
    "f_vocab=open('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vocab')\n",
    "vocab_set=[]\n",
    "for line in f_vocab:\n",
    "    vocab_set.append(line.strip().split(' ')[0])\n",
    "vocab_set=set(vocab_set)\n",
    "f_vocab.close()\n",
    "print('vocabulary size =',len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_sentence=tf.placeholder(dtype=tf.float32,shape=[None,None,None,word2vec_size]) # shape=(batch_size,doc_size,sen_size,vocab)\n",
    "y=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "sentence_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size*document_size,)\n",
    "document_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size,)\n",
    "is_training=tf.placeholder(dtype=tf.bool,shape=None)\n",
    "dropout=tf.placeholder(dtype=tf.float32,shape=None)\n",
    "\n",
    "\n",
    "model=HAN(num_classes,word2vec_size,lamb=0.0,disTill=2.0)\n",
    "loss,predict,accuracy,attention_low,attention_high=model(X_sentence,y,sentence_length,document_length,dropout,is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(doc):\n",
    "    this_doc=[]\n",
    "    sents=doc.split('\\t')\n",
    "    for s in sents:\n",
    "        this_sentence=[]\n",
    "        words=s.split(' ')\n",
    "        for w in words:\n",
    "            if w in vocab_set:\n",
    "                this_sentence.append(w)\n",
    "            else:\n",
    "                this_sentence.append('<UNKNOWN>')\n",
    "        this_doc.append(this_sentence)\n",
    "        document_sizes=np.array([len(this_doc)])\n",
    "        sentence_sizes=np.array([[len(t) for t in this_doc]])\n",
    "        document_size=np.max(document_sizes)\n",
    "        sentence_size=np.max(sentence_sizes)\n",
    "        \n",
    "        output=np.zeros((1,document_size,sentence_size,word2vec_size))\n",
    "        \n",
    "        for (id_s,s) in enumerate(this_doc):\n",
    "            for (id_w,w) in enumerate(s):\n",
    "                if id_w>=cut_long_sentence:  # cut too long sentences\n",
    "                    break\n",
    "                output[0,id_s,id_w,:]=word2vec_model[w]\n",
    "        \n",
    "        \n",
    "    return this_doc,document_sizes,sentence_sizes,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_dev=open('data/yelp'+year+'/yelp-'+year+'-dev-nonvocab.txt')  # randomly select some cases from dev set\n",
    "# cases=[]\n",
    "# for line in f_dev:\n",
    "#     cases.append(line.strip().split('\\t\\t'))\n",
    "# random.shuffle(cases)\n",
    "# cases=cases[:10]\n",
    "\n",
    "f_dev=open('data/yelp'+year+'/test.txt')\n",
    "cases=[]\n",
    "for line in f_dev:\n",
    "    cases.append(line.strip().split('\\t\\t'))\n",
    "random.shuffle(cases)\n",
    "cases=cases[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-08 15:30:42,010 : INFO : Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case #1:\n",
      "Original sentence:\n",
      "a hidden gem, service is awesome, prices are reasonable, food is very good, we came in with a large group of motorcycles, they greeted us and made room for us right away, i felt like they were sincerely glad to have our business and i will be back for sure\n",
      "\n",
      "Exact label: 4, predict label: 5\n",
      "\n",
      "Sentence attention:\n",
      "0.38% a hidden gem\n",
      "69.98% service is awesome\n",
      "10.55% prices are reasonable\n",
      "18.96% food is very good\n",
      "0.07% we came in with a large group of motorcycles\n",
      "0.06% they greeted us and made room for us right away\n",
      "0.00% i felt like they were sincerely glad to have our business and i will be back for sure\n",
      "\n",
      "Word attention:\n",
      "('0.16%', 'a') ('78.37%', 'hidden') ('21.47%', 'gem')\n",
      "('0.00%', 'service') ('0.03%', 'is') ('99.97%', 'awesome')\n",
      "('0.00%', 'prices') ('0.00%', 'are') ('100.00%', 'reasonable')\n",
      "('0.00%', 'food') ('0.00%', 'is') ('0.00%', 'very') ('100.00%', 'good')\n",
      "('0.00%', 'we') ('0.00%', 'came') ('0.00%', 'in') ('0.00%', 'with') ('0.78%', 'a') ('92.26%', 'large') ('2.93%', 'group') ('3.61%', 'of') ('0.43%', 'motorcycles')\n",
      "('4.81%', 'they') ('93.06%', 'greeted') ('0.00%', 'us') ('0.00%', 'and') ('0.00%', 'made') ('0.00%', 'room') ('0.02%', 'for') ('0.00%', 'us') ('2.10%', 'right') ('0.00%', 'away')\n",
      "('0.00%', 'i') ('0.00%', 'felt') ('0.55%', 'like') ('0.00%', 'they') ('0.00%', 'were') ('0.05%', 'sincerely') ('0.39%', 'glad') ('0.00%', 'to') ('0.00%', 'have') ('0.00%', 'our') ('0.00%', 'business') ('0.00%', 'and') ('0.00%', 'i') ('0.01%', 'will') ('0.11%', 'be') ('98.81%', 'back') ('0.09%', 'for') ('0.00%', 'sure')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"parameters/HAN.ckpt\")\n",
    "    cor=0\n",
    "    for id,(label,sent) in enumerate(cases):\n",
    "        this_doc,document_sizes,sentence_sizes,output=make_input(sent)\n",
    "        \n",
    "        slen=output.shape[2]\n",
    "        \n",
    "        label=np.array([int(label)-1])\n",
    "        feed_dict={X_sentence:output,y:label,sentence_length:sentence_sizes.reshape(-1,),\n",
    "                   document_length:document_sizes,is_training:False,dropout:0.0}\n",
    "        pre,al,ah=sess.run([predict,attention_low,attention_high],feed_dict=feed_dict)\n",
    "        print('Case #'+str(id+1)+':')\n",
    "        print('Original sentence:')\n",
    "        print(', '.join(sent.split('\\t')))\n",
    "        print()\n",
    "        print('Exact label: %d, predict label: %d' % (label[0]+1,pre[0]+1))\n",
    "        print()\n",
    "        print('Sentence attention:')\n",
    "        for (id_s,s) in enumerate(this_doc):\n",
    "            print('%.2f%%' % (ah[0,id_s]*100.0),' '.join(s))\n",
    "        print()\n",
    "        print('Word attention:')\n",
    "        for (id_s,s) in enumerate(this_doc):\n",
    "            print(' '.join([str(('%.2f%%' % (al[id_s,id_w]*100.0),w)) for (id_w,w) in enumerate(s)]))\n",
    "#             print(' '.join([str(('%.2f%%' % (al[id_s,id_w]*100.0),s[id_w] if id_w<len(s) else '_')) for id_w in range(slen)]))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
