{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.base import Layer\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-20 13:40:18,321 : INFO : loading projection weights from data/yelp2013/yelp_2013_50.vector\n",
      "2018-05-20 13:40:20,629 : INFO : loaded (43530, 50) matrix from data/yelp2013/yelp_2013_50.vector\n"
     ]
    }
   ],
   "source": [
    "year='2013'\n",
    "word2vec_size=50\n",
    "cut_long_sentence=200\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-20 13:40:20,634 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "_=word2vec_model.most_similar('ridiculous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 43531\n"
     ]
    }
   ],
   "source": [
    "# f_vocab=open('data/yelp'+year+'/yelp_'+year+'_500.vocab')\n",
    "# index2chr={}\n",
    "# chr2index={}\n",
    "# index2chr[0]='<UNKNOWN>'\n",
    "# chr2index['<UNKNOWN>']=0\n",
    "# cnt=1\n",
    "# for line in f_vocab:\n",
    "#     w=line.strip().split(' ')[0]\n",
    "#     index2chr[cnt]=w\n",
    "#     chr2index[w]=cnt\n",
    "#     cnt+=1\n",
    "# f_vocab.close()\n",
    "# vocab_size=cnt\n",
    "# print('vocabulary size =',vocab_size)\n",
    "\n",
    "f_vocab=open('data/yelp'+year+'/yelp_'+year+'_'+str(word2vec_size)+'.vector')\n",
    "vocab_set=[]\n",
    "for line in f_vocab:\n",
    "    vocab_set.append(line.strip().split(' ')[0])\n",
    "vocab_set=set(vocab_set)\n",
    "f_vocab.close()\n",
    "print('vocabulary size =',len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data(f):\n",
    "    longest_doc=-1\n",
    "    longest_sen=-1\n",
    "    documents=[]\n",
    "    for line in f:\n",
    "        this_document=[]\n",
    "        cols=line.strip().split('\\t\\t')\n",
    "        sents=cols[1].split('\\t')\n",
    "        for s in sents:\n",
    "            this_sentence=[]\n",
    "            words=s.split(' ')\n",
    "            for w in words:\n",
    "                if w in vocab_set:\n",
    "                    this_sentence.append(w)\n",
    "                else:\n",
    "                    this_sentence.append('<UNKNOWN>')\n",
    "            longest_sen=max(longest_sen,len(this_sentence))\n",
    "            this_document.append(this_sentence)\n",
    "        longest_doc=max(longest_doc,len(this_document))\n",
    "        documents.append((this_document,int(cols[0])))\n",
    "    return documents,longest_doc,longest_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train=open('data/yelp'+year+'/yelp-'+year+'-train-nonvocab.txt')\n",
    "documents_train,longest_doc_train,longest_sen_train=raw_data(f_train)\n",
    "f_train.close()\n",
    "f_train=None\n",
    "\n",
    "f_val=open('data/yelp'+year+'/yelp-'+year+'-dev-nonvocab.txt')\n",
    "documents_val,longest_doc_val,longest_sen_val=raw_data(f_val)\n",
    "f_val.close()\n",
    "f_val=None\n",
    "\n",
    "f_test=open('data/yelp'+year+'/yelp-'+year+'-test-nonvocab.txt')\n",
    "documents_test,longest_doc_test,longest_sen_test=raw_data(f_test)\n",
    "f_test.close()\n",
    "f_test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum document length = 100\n",
      "maximum sentence length = 653\n",
      "size of train data = 268006\n",
      "size of validation data = 33499\n",
      "size of test data = 33503\n"
     ]
    }
   ],
   "source": [
    "max_doc_length=max(longest_doc_train,longest_doc_val,longest_doc_test)\n",
    "max_sent_length=max(longest_sen_train,longest_sen_val,longest_sen_test)\n",
    "print('maximum document length =',max_doc_length)\n",
    "print('maximum sentence length =',max_sent_length)\n",
    "\n",
    "# documents_train=documents_train[:128]\n",
    "\n",
    "num_train=len(documents_train)\n",
    "num_val=len(documents_val)\n",
    "num_test=len(documents_test)\n",
    "\n",
    "print('size of train data =',num_train)\n",
    "print('size of validation data =',num_val)\n",
    "print('size of test data =',num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_np(data):\n",
    "    '''\n",
    "    from: https://stackoverflow.com/questions/32037893/numpy-fix-array-with-rows-of-different-lengths-by-filling-the-empty-elements-wi\n",
    "    '''\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(lens.max()) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self,data):\n",
    "        self.cursor=0\n",
    "        self.data=data\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.cursor=0\n",
    "    \n",
    "    def start_epoch(self):\n",
    "        self.cursor=0\n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "    def next_batch(self,batch_size=64):\n",
    "        batch=self.data[self.cursor:self.cursor+batch_size]\n",
    "        labels=np.array([d[1] for d in batch])-1\n",
    "        self.cursor+=batch_size\n",
    "        document_sizes=np.array([len(d[0]) for d in batch])\n",
    "        document_size=np.max(document_sizes)\n",
    "        \n",
    "        sentence_sizes=np.array([[min(len(s),cut_long_sentence) for s in d[0]] for d in batch])\n",
    "#         sentence_size = max(map(max, sentence_sizes))\n",
    "        sentence_sizes=fill_np(sentence_sizes)\n",
    "        sentence_size=np.max(sentence_sizes)\n",
    "        \n",
    "        # shape=(batch_size, document_len, sentence_len, word2vec_len)\n",
    "        output=np.zeros((batch_size,document_size,sentence_size,word2vec_size))\n",
    "        \n",
    "        for (id_d,d) in enumerate(batch):\n",
    "            for (id_s,s) in enumerate(d[0]):\n",
    "                for (id_w,w) in enumerate(s):\n",
    "                    if id_w>=cut_long_sentence:  # cut too long sentences\n",
    "                        break\n",
    "                    if w in word2vec_model:\n",
    "                        output[id_d,id_s,id_w,:]=word2vec_model[w]\n",
    "                    else:\n",
    "                        output[id_d,id_s,id_w,:]=np.ones(word2vec_size)\n",
    "        \n",
    "        return output,document_sizes,sentence_sizes,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=Dataset(documents_train)\n",
    "train_eval=Dataset(documents_train)\n",
    "validation=Dataset(documents_val)\n",
    "test=Dataset(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 13  8 37  6  7  7  3 22  4  4 18  7  6  3  3  1 12  4 19  4  6  2 13\n",
      "  3  6  1  9  2  6  7  6  2  7 17  5  5  7  4 11 15  1  8  4  5 14  7 26\n",
      "  8 15 20 11 11 12  3  2  2  2  1 12  6  9 17 14]\n",
      "[[3 9 10 ... 0 0 0]\n",
      " [8 13 11 ... 0 0 0]\n",
      " [26 18 15 ... 0 0 0]\n",
      " ...\n",
      " [13 1 7 ... 0 0 0]\n",
      " [14 13 13 ... 0 0 0]\n",
      " [27 21 21 ... 0 0 0]]\n",
      "(64,)\n",
      "(64, 37)\n",
      "(64, 37, 65, 50)\n",
      "[3 3 4 3 0 4 3 1 4 3 1 2 3 4 1 4 4 0 4 3 3 0 2 1 3 1 3 3 4 4 1 4 2 2 0 3 4\n",
      " 2 4 1 3 0 4 2 3 3 4 4 1 3 4 4 3 3 4 3 1 2 2 4 4 4 1 2]\n"
     ]
    }
   ],
   "source": [
    "train.start_epoch()\n",
    "corpora,l,a,labels=train.next_batch(64)\n",
    "print(l)\n",
    "print(a)\n",
    "print(l.shape)\n",
    "print(a.shape)\n",
    "print(corpora.shape)\n",
    "# print(corpora)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Part I: word level attention\n",
    "gru_hidden_size_low=128\n",
    "attention_hidden_low=128\n",
    "\n",
    "y=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "y_onehot=tf.one_hot(y,num_classes)\n",
    "\n",
    "X_sentence=tf.placeholder(dtype=tf.float32,shape=[None,None,None,word2vec_size]) # shape=(batch_size,doc_size,sen_size,vocab)\n",
    "batch_size=tf.shape(X_sentence)[0]\n",
    "document_size=tf.shape(X_sentence)[1]\n",
    "sentence_size=tf.shape(X_sentence)[2]\n",
    "X_sentence_r=tf.reshape(X_sentence,(batch_size*document_size,sentence_size,word2vec_size)) # shape=(batch_size*doc_size,sen_size,vocab)\n",
    "\n",
    "sentence_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size*document_size,)\n",
    "\n",
    "cell_fw_low=tf.contrib.rnn.GRUCell(gru_hidden_size_low)\n",
    "cell_bw_low=tf.contrib.rnn.GRUCell(gru_hidden_size_low)\n",
    "\n",
    "word_attention_net_low=tf.layers.Dense(units=attention_hidden_low,name='word_attention_net_low')\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"word_level\") as vs:\n",
    "    # shape=(batch_size*doc_size,sen_size,gru_size)\n",
    "    low_output=tf.nn.bidirectional_dynamic_rnn(cell_fw_low,cell_bw_low,\n",
    "                                               dtype=tf.float32,\n",
    "                                               inputs=X_sentence_r,\n",
    "                                               sequence_length=sentence_length,\n",
    "                                               time_major=False)\n",
    "\n",
    "low_output=tf.concat(low_output[0],axis=2)  # shape=(batch_size*doc_size,sen_size,2*gru_size)\n",
    "\n",
    "uw=tf.get_variable(\"uw\", dtype=tf.float32, \n",
    "                   shape=(attention_hidden_low,1),\n",
    "                   initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0))\n",
    "\n",
    "low_output_r=tf.reshape(low_output,(-1,2*gru_hidden_size_low))  # shape=(batch_size*doc_size*sen_size,2*gru_size)\n",
    "low_output_r=word_attention_net_low(low_output_r)  # shape=(batch_size*doc_size*sen_size,attention_hidden_low)\n",
    "score_low=tf.matmul(low_output_r,uw)  # shape=(batch_size*doc_size*sen_size,1)\n",
    "score_low=tf.reshape(score_low,(batch_size*document_size,sentence_size))  # shape=(batch_size*doc_size,sen_size)\n",
    "attention_low=tf.nn.softmax(score_low,dim=1)\n",
    "sentence_vector=(tf.reshape(attention_low,(batch_size*document_size,sentence_size,1))*low_output)  # shape: same as low_output\n",
    "sentence_vector=tf.reduce_sum(sentence_vector,axis=1)  # shape=(batch_size*doc_size,2*gru_size)\n",
    "sentence_vector=tf.reshape(sentence_vector,(batch_size,document_size,2*gru_hidden_size_low))  # shape=(batch_size,doc_size,2*gru_size)\n",
    "\n",
    "\n",
    "# Part II: sentence level attention\n",
    "gru_hidden_size_high=128\n",
    "attention_hidden_high=128\n",
    "\n",
    "document_length=tf.placeholder(dtype=tf.int32,shape=[None])  # it should have shape=(batch_size,)\n",
    "\n",
    "cell_fw_high=tf.contrib.rnn.GRUCell(gru_hidden_size_high)\n",
    "cell_bw_high=tf.contrib.rnn.GRUCell(gru_hidden_size_high)\n",
    "\n",
    "word_attention_net_high=tf.layers.Dense(units=attention_hidden_high,name='word_attention_net_high')\n",
    "\n",
    "with tf.variable_scope(\"sentence_level\") as vs:\n",
    "    high_output=tf.nn.bidirectional_dynamic_rnn(cell_fw_high,cell_bw_high,\n",
    "                                               dtype=tf.float32,\n",
    "                                               inputs=sentence_vector,\n",
    "                                               sequence_length=document_length,\n",
    "                                               time_major=False)\n",
    "high_output=tf.concat(high_output[0],axis=2)  # shape=(batch_size,doc_size,2*gru_size)\n",
    "\n",
    "us=tf.get_variable(\"us\", dtype=tf.float32, \n",
    "                   shape=(attention_hidden_high,1),\n",
    "                   initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0))\n",
    "\n",
    "high_output_r=tf.reshape(high_output,(-1,2*gru_hidden_size_high))  # shape=(batch_size*doc_size,2*gru_size)\n",
    "high_output_r=word_attention_net_high(high_output_r)  # shape=(batch_size*doc_size,attention)\n",
    "score_high=tf.matmul(high_output_r,us)  # shape=(batch_size*doc_size,1)\n",
    "score_high=tf.reshape(score_high,(batch_size,document_size))  # shape=(batch_size,doc_size)\n",
    "attention_high=tf.nn.softmax(score_high,dim=1)  # shape=(batch_size,doc_size)\n",
    "output=(tf.reshape(attention_high,(batch_size,document_size,1))*high_output)  # shape=(batch_size,doc_size,2*gru_size)\n",
    "output=tf.reduce_sum(output,axis=1)  # shape=(batch_size,2*gru_size)\n",
    "\n",
    "\n",
    "# Part III: predict\n",
    "score=tf.layers.dense(output,units=num_classes,name='predict_net')\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y_onehot,logits=score)\n",
    "predict=tf.cast(tf.argmax(score,axis=1),dtype=tf.int32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(y,predict),dtype=tf.float32))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4e-4\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,num_train//128, 0.9, staircase=True)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=starter_learning_rate)\n",
    "# train_step = optimizier.minimize(total_loss,global_step=global_step)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch=50\n",
    "print_every=50\n",
    "bs=32\n",
    "\n",
    "def eval(dataset,num_iteration):\n",
    "    tot_loss=0\n",
    "    tot_accuracy=0\n",
    "    dataset.start_epoch()\n",
    "    for it in range(num_iteration):\n",
    "        output,document_sizes,sentence_sizes,labels=dataset.next_batch(bs)\n",
    "        feed_dict={X_sentence:output,y:labels,sentence_length:sentence_sizes.reshape(-1,),document_length:document_sizes}\n",
    "        loss_num,acc_num=sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "        tot_loss+=loss_num\n",
    "        tot_accuracy+=acc_num\n",
    "    tot_loss/=num_iteration\n",
    "    tot_accuracy/=num_iteration\n",
    "    return tot_loss,tot_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 13:40:46 start epoch 1/50, with learning rate = 0.0004000000\n",
      "2018-05-20 13:40:47 iteration 1/1000: current training loss = 1.614959, accuracy = 21.88%\n",
      "2018-05-20 13:40:58 iteration 50/1000: current training loss = 1.259617, accuracy = 40.62%\n",
      "2018-05-20 13:41:09 iteration 100/1000: current training loss = 1.230234, accuracy = 53.12%\n",
      "2018-05-20 13:41:21 iteration 150/1000: current training loss = 1.198083, accuracy = 50.00%\n",
      "2018-05-20 13:41:33 iteration 200/1000: current training loss = 1.109334, accuracy = 46.88%\n",
      "2018-05-20 13:41:44 iteration 250/1000: current training loss = 1.069014, accuracy = 62.50%\n",
      "2018-05-20 13:41:56 iteration 300/1000: current training loss = 1.051053, accuracy = 46.88%\n",
      "2018-05-20 13:42:08 iteration 350/1000: current training loss = 0.857837, accuracy = 68.75%\n",
      "2018-05-20 13:42:19 iteration 400/1000: current training loss = 1.063239, accuracy = 53.12%\n",
      "2018-05-20 13:42:30 iteration 450/1000: current training loss = 1.148658, accuracy = 43.75%\n",
      "2018-05-20 13:42:42 iteration 500/1000: current training loss = 0.995343, accuracy = 59.38%\n",
      "2018-05-20 13:42:54 iteration 550/1000: current training loss = 1.225877, accuracy = 46.88%\n",
      "2018-05-20 13:43:05 iteration 600/1000: current training loss = 0.989797, accuracy = 50.00%\n",
      "2018-05-20 13:43:17 iteration 650/1000: current training loss = 1.206199, accuracy = 40.62%\n",
      "2018-05-20 13:43:28 iteration 700/1000: current training loss = 0.911082, accuracy = 56.25%\n",
      "2018-05-20 13:43:40 iteration 750/1000: current training loss = 0.977908, accuracy = 59.38%\n",
      "2018-05-20 13:43:52 iteration 800/1000: current training loss = 1.208438, accuracy = 46.88%\n",
      "2018-05-20 13:44:02 iteration 850/1000: current training loss = 1.163243, accuracy = 46.88%\n",
      "2018-05-20 13:44:13 iteration 900/1000: current training loss = 1.032195, accuracy = 56.25%\n",
      "2018-05-20 13:44:24 iteration 950/1000: current training loss = 0.963955, accuracy = 53.12%\n",
      "2018-05-20 13:44:36 iteration 1000/1000: current training loss = 1.268232, accuracy = 56.25%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 13:45:07 end epoch 1/50: acc_train=56.625% acc_val=56.063% acc_test=56.875%\n",
      "\n",
      "2018-05-20 13:45:07 start epoch 2/50, with learning rate = 0.0004000000\n",
      "2018-05-20 13:45:07 iteration 1/1000: current training loss = 1.025061, accuracy = 53.12%\n",
      "2018-05-20 13:45:18 iteration 50/1000: current training loss = 1.027766, accuracy = 59.38%\n",
      "2018-05-20 13:45:29 iteration 100/1000: current training loss = 0.986870, accuracy = 56.25%\n",
      "2018-05-20 13:45:41 iteration 150/1000: current training loss = 0.898838, accuracy = 62.50%\n",
      "2018-05-20 13:45:52 iteration 200/1000: current training loss = 0.862054, accuracy = 53.12%\n",
      "2018-05-20 13:46:03 iteration 250/1000: current training loss = 0.817325, accuracy = 71.88%\n",
      "2018-05-20 13:46:15 iteration 300/1000: current training loss = 0.964485, accuracy = 43.75%\n",
      "2018-05-20 13:46:27 iteration 350/1000: current training loss = 0.762798, accuracy = 59.38%\n",
      "2018-05-20 13:46:40 iteration 400/1000: current training loss = 0.949141, accuracy = 62.50%\n",
      "2018-05-20 13:46:52 iteration 450/1000: current training loss = 0.848135, accuracy = 62.50%\n",
      "2018-05-20 13:47:04 iteration 500/1000: current training loss = 0.769841, accuracy = 68.75%\n",
      "2018-05-20 13:47:15 iteration 550/1000: current training loss = 1.151775, accuracy = 46.88%\n",
      "2018-05-20 13:47:27 iteration 600/1000: current training loss = 0.946220, accuracy = 59.38%\n",
      "2018-05-20 13:47:39 iteration 650/1000: current training loss = 1.105094, accuracy = 59.38%\n",
      "2018-05-20 13:47:50 iteration 700/1000: current training loss = 0.992320, accuracy = 59.38%\n",
      "2018-05-20 13:48:01 iteration 750/1000: current training loss = 1.282698, accuracy = 37.50%\n",
      "2018-05-20 13:48:12 iteration 800/1000: current training loss = 0.870682, accuracy = 53.12%\n",
      "2018-05-20 13:48:24 iteration 850/1000: current training loss = 0.922528, accuracy = 56.25%\n",
      "2018-05-20 13:48:34 iteration 900/1000: current training loss = 0.947603, accuracy = 62.50%\n",
      "2018-05-20 13:48:45 iteration 950/1000: current training loss = 1.083122, accuracy = 34.38%\n",
      "2018-05-20 13:48:57 iteration 1000/1000: current training loss = 0.846756, accuracy = 50.00%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 13:49:28 end epoch 2/50: acc_train=59.656% acc_val=61.781% acc_test=59.594%\n",
      "\n",
      "2018-05-20 13:49:28 start epoch 3/50, with learning rate = 0.0004000000\n",
      "2018-05-20 13:49:29 iteration 1/1000: current training loss = 0.924766, accuracy = 56.25%\n",
      "2018-05-20 13:49:40 iteration 50/1000: current training loss = 0.869501, accuracy = 62.50%\n",
      "2018-05-20 13:49:52 iteration 100/1000: current training loss = 0.981441, accuracy = 43.75%\n",
      "2018-05-20 13:50:03 iteration 150/1000: current training loss = 0.724261, accuracy = 75.00%\n",
      "2018-05-20 13:50:15 iteration 200/1000: current training loss = 0.716323, accuracy = 71.88%\n",
      "2018-05-20 13:50:26 iteration 250/1000: current training loss = 0.997611, accuracy = 56.25%\n",
      "2018-05-20 13:50:38 iteration 300/1000: current training loss = 0.858813, accuracy = 62.50%\n",
      "2018-05-20 13:50:50 iteration 350/1000: current training loss = 1.062354, accuracy = 50.00%\n",
      "2018-05-20 13:51:02 iteration 400/1000: current training loss = 1.028669, accuracy = 56.25%\n",
      "2018-05-20 13:51:13 iteration 450/1000: current training loss = 0.935001, accuracy = 50.00%\n",
      "2018-05-20 13:51:24 iteration 500/1000: current training loss = 0.827828, accuracy = 62.50%\n",
      "2018-05-20 13:51:36 iteration 550/1000: current training loss = 1.094337, accuracy = 50.00%\n",
      "2018-05-20 13:51:48 iteration 600/1000: current training loss = 0.857767, accuracy = 56.25%\n",
      "2018-05-20 13:52:00 iteration 650/1000: current training loss = 1.060927, accuracy = 43.75%\n",
      "2018-05-20 13:52:11 iteration 700/1000: current training loss = 0.968609, accuracy = 53.12%\n",
      "2018-05-20 13:52:23 iteration 750/1000: current training loss = 0.875779, accuracy = 56.25%\n",
      "2018-05-20 13:52:34 iteration 800/1000: current training loss = 0.955625, accuracy = 56.25%\n",
      "2018-05-20 13:52:46 iteration 850/1000: current training loss = 0.720204, accuracy = 68.75%\n",
      "2018-05-20 13:52:57 iteration 900/1000: current training loss = 0.755386, accuracy = 71.88%\n",
      "2018-05-20 13:53:09 iteration 950/1000: current training loss = 0.805942, accuracy = 62.50%\n",
      "2018-05-20 13:53:20 iteration 1000/1000: current training loss = 0.715650, accuracy = 75.00%\n",
      "2018-05-20 13:53:51 end epoch 3/50: acc_train=62.000% acc_val=59.344% acc_test=60.000%\n",
      "\n",
      "2018-05-20 13:53:51 start epoch 4/50, with learning rate = 0.0004000000\n",
      "2018-05-20 13:53:51 iteration 1/1000: current training loss = 0.681930, accuracy = 68.75%\n",
      "2018-05-20 13:54:03 iteration 50/1000: current training loss = 0.865361, accuracy = 62.50%\n",
      "2018-05-20 13:54:14 iteration 100/1000: current training loss = 0.855694, accuracy = 59.38%\n",
      "2018-05-20 13:54:26 iteration 150/1000: current training loss = 0.923921, accuracy = 59.38%\n",
      "2018-05-20 13:54:37 iteration 200/1000: current training loss = 0.886117, accuracy = 65.62%\n",
      "2018-05-20 13:54:49 iteration 250/1000: current training loss = 0.967280, accuracy = 56.25%\n",
      "2018-05-20 13:54:59 iteration 300/1000: current training loss = 0.827756, accuracy = 65.62%\n",
      "2018-05-20 13:55:11 iteration 350/1000: current training loss = 0.878828, accuracy = 62.50%\n",
      "2018-05-20 13:55:23 iteration 400/1000: current training loss = 0.788142, accuracy = 71.88%\n",
      "2018-05-20 13:55:34 iteration 450/1000: current training loss = 0.916044, accuracy = 50.00%\n",
      "2018-05-20 13:55:45 iteration 500/1000: current training loss = 0.892091, accuracy = 65.62%\n",
      "2018-05-20 13:55:57 iteration 550/1000: current training loss = 0.917713, accuracy = 50.00%\n",
      "2018-05-20 13:56:09 iteration 600/1000: current training loss = 0.736995, accuracy = 75.00%\n",
      "2018-05-20 13:56:21 iteration 650/1000: current training loss = 0.960962, accuracy = 65.62%\n",
      "2018-05-20 13:56:33 iteration 700/1000: current training loss = 0.971719, accuracy = 62.50%\n",
      "2018-05-20 13:56:45 iteration 750/1000: current training loss = 0.707105, accuracy = 75.00%\n",
      "2018-05-20 13:56:57 iteration 800/1000: current training loss = 0.552111, accuracy = 87.50%\n",
      "2018-05-20 13:57:09 iteration 850/1000: current training loss = 0.897826, accuracy = 53.12%\n",
      "2018-05-20 13:57:20 iteration 900/1000: current training loss = 0.987017, accuracy = 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 13:57:32 iteration 950/1000: current training loss = 0.800732, accuracy = 71.88%\n",
      "2018-05-20 13:57:43 iteration 1000/1000: current training loss = 0.797087, accuracy = 59.38%\n",
      "2018-05-20 13:58:14 end epoch 4/50: acc_train=62.969% acc_val=60.938% acc_test=61.094%\n",
      "\n",
      "2018-05-20 13:58:14 start epoch 5/50, with learning rate = 0.0004000000\n",
      "2018-05-20 13:58:14 iteration 1/1000: current training loss = 0.810357, accuracy = 65.62%\n",
      "2018-05-20 13:58:26 iteration 50/1000: current training loss = 0.876180, accuracy = 53.12%\n",
      "2018-05-20 13:58:38 iteration 100/1000: current training loss = 0.742609, accuracy = 75.00%\n",
      "2018-05-20 13:58:50 iteration 150/1000: current training loss = 0.804843, accuracy = 78.12%\n",
      "2018-05-20 13:59:01 iteration 200/1000: current training loss = 0.821496, accuracy = 53.12%\n",
      "2018-05-20 13:59:13 iteration 250/1000: current training loss = 0.898050, accuracy = 68.75%\n",
      "2018-05-20 13:59:24 iteration 300/1000: current training loss = 0.563858, accuracy = 81.25%\n",
      "2018-05-20 13:59:36 iteration 350/1000: current training loss = 1.039973, accuracy = 59.38%\n",
      "2018-05-20 13:59:47 iteration 400/1000: current training loss = 1.104597, accuracy = 46.88%\n",
      "2018-05-20 13:59:58 iteration 450/1000: current training loss = 0.983512, accuracy = 43.75%\n",
      "2018-05-20 14:00:09 iteration 500/1000: current training loss = 0.752283, accuracy = 65.62%\n",
      "2018-05-20 14:00:21 iteration 550/1000: current training loss = 0.987030, accuracy = 62.50%\n",
      "2018-05-20 14:00:32 iteration 600/1000: current training loss = 0.861365, accuracy = 59.38%\n",
      "2018-05-20 14:00:44 iteration 650/1000: current training loss = 0.751987, accuracy = 62.50%\n",
      "2018-05-20 14:00:55 iteration 700/1000: current training loss = 0.740663, accuracy = 65.62%\n",
      "2018-05-20 14:01:07 iteration 750/1000: current training loss = 0.846750, accuracy = 53.12%\n",
      "2018-05-20 14:01:18 iteration 800/1000: current training loss = 0.940818, accuracy = 56.25%\n",
      "2018-05-20 14:01:29 iteration 850/1000: current training loss = 0.866645, accuracy = 71.88%\n",
      "2018-05-20 14:01:40 iteration 900/1000: current training loss = 0.906542, accuracy = 62.50%\n",
      "2018-05-20 14:01:52 iteration 950/1000: current training loss = 0.736690, accuracy = 65.62%\n",
      "2018-05-20 14:02:04 iteration 1000/1000: current training loss = 0.806638, accuracy = 59.38%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 14:02:36 end epoch 5/50: acc_train=62.000% acc_val=62.094% acc_test=61.281%\n",
      "\n",
      "2018-05-20 14:02:36 start epoch 6/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:02:36 iteration 1/1000: current training loss = 1.043074, accuracy = 40.62%\n",
      "2018-05-20 14:02:47 iteration 50/1000: current training loss = 0.956874, accuracy = 59.38%\n",
      "2018-05-20 14:02:58 iteration 100/1000: current training loss = 0.691312, accuracy = 71.88%\n",
      "2018-05-20 14:03:10 iteration 150/1000: current training loss = 0.752649, accuracy = 78.12%\n",
      "2018-05-20 14:03:21 iteration 200/1000: current training loss = 0.730922, accuracy = 68.75%\n",
      "2018-05-20 14:03:33 iteration 250/1000: current training loss = 0.746303, accuracy = 65.62%\n",
      "2018-05-20 14:03:45 iteration 300/1000: current training loss = 0.799237, accuracy = 65.62%\n",
      "2018-05-20 14:03:56 iteration 350/1000: current training loss = 0.814053, accuracy = 68.75%\n",
      "2018-05-20 14:04:08 iteration 400/1000: current training loss = 0.678218, accuracy = 78.12%\n",
      "2018-05-20 14:04:20 iteration 450/1000: current training loss = 0.900471, accuracy = 62.50%\n",
      "2018-05-20 14:04:32 iteration 500/1000: current training loss = 0.745802, accuracy = 75.00%\n",
      "2018-05-20 14:04:44 iteration 550/1000: current training loss = 0.934720, accuracy = 50.00%\n",
      "2018-05-20 14:04:55 iteration 600/1000: current training loss = 0.825191, accuracy = 56.25%\n",
      "2018-05-20 14:05:07 iteration 650/1000: current training loss = 0.730081, accuracy = 68.75%\n",
      "2018-05-20 14:05:19 iteration 700/1000: current training loss = 0.907476, accuracy = 53.12%\n",
      "2018-05-20 14:05:30 iteration 750/1000: current training loss = 0.935931, accuracy = 50.00%\n",
      "2018-05-20 14:05:41 iteration 800/1000: current training loss = 0.672438, accuracy = 62.50%\n",
      "2018-05-20 14:05:53 iteration 850/1000: current training loss = 0.837180, accuracy = 65.62%\n",
      "2018-05-20 14:06:04 iteration 900/1000: current training loss = 0.849785, accuracy = 59.38%\n",
      "2018-05-20 14:06:16 iteration 950/1000: current training loss = 0.872968, accuracy = 59.38%\n",
      "2018-05-20 14:06:28 iteration 1000/1000: current training loss = 0.814003, accuracy = 56.25%\n",
      "2018-05-20 14:06:59 end epoch 6/50: acc_train=62.781% acc_val=60.406% acc_test=62.344%\n",
      "\n",
      "2018-05-20 14:06:59 start epoch 7/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:07:00 iteration 1/1000: current training loss = 0.791108, accuracy = 65.62%\n",
      "2018-05-20 14:07:11 iteration 50/1000: current training loss = 1.043662, accuracy = 56.25%\n",
      "2018-05-20 14:07:23 iteration 100/1000: current training loss = 0.818424, accuracy = 59.38%\n",
      "2018-05-20 14:07:34 iteration 150/1000: current training loss = 0.855285, accuracy = 59.38%\n",
      "2018-05-20 14:07:45 iteration 200/1000: current training loss = 0.621967, accuracy = 75.00%\n",
      "2018-05-20 14:07:57 iteration 250/1000: current training loss = 0.677034, accuracy = 71.88%\n",
      "2018-05-20 14:08:09 iteration 300/1000: current training loss = 0.956065, accuracy = 56.25%\n",
      "2018-05-20 14:08:20 iteration 350/1000: current training loss = 0.685150, accuracy = 75.00%\n",
      "2018-05-20 14:08:31 iteration 400/1000: current training loss = 0.882500, accuracy = 62.50%\n",
      "2018-05-20 14:08:43 iteration 450/1000: current training loss = 0.807363, accuracy = 68.75%\n",
      "2018-05-20 14:08:55 iteration 500/1000: current training loss = 0.829969, accuracy = 56.25%\n",
      "2018-05-20 14:09:06 iteration 550/1000: current training loss = 0.890205, accuracy = 59.38%\n",
      "2018-05-20 14:09:17 iteration 600/1000: current training loss = 0.981501, accuracy = 59.38%\n",
      "2018-05-20 14:09:28 iteration 650/1000: current training loss = 0.872005, accuracy = 62.50%\n",
      "2018-05-20 14:09:40 iteration 700/1000: current training loss = 0.953295, accuracy = 59.38%\n",
      "2018-05-20 14:09:51 iteration 750/1000: current training loss = 1.061977, accuracy = 56.25%\n",
      "2018-05-20 14:10:02 iteration 800/1000: current training loss = 0.756895, accuracy = 59.38%\n",
      "2018-05-20 14:10:14 iteration 850/1000: current training loss = 0.852947, accuracy = 65.62%\n",
      "2018-05-20 14:10:25 iteration 900/1000: current training loss = 0.938011, accuracy = 56.25%\n",
      "2018-05-20 14:10:36 iteration 950/1000: current training loss = 0.782822, accuracy = 50.00%\n",
      "2018-05-20 14:10:47 iteration 1000/1000: current training loss = 0.747645, accuracy = 53.12%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 14:11:18 end epoch 7/50: acc_train=64.438% acc_val=63.156% acc_test=62.438%\n",
      "\n",
      "2018-05-20 14:11:18 start epoch 8/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:11:19 iteration 1/1000: current training loss = 0.996057, accuracy = 62.50%\n",
      "2018-05-20 14:11:30 iteration 50/1000: current training loss = 0.815735, accuracy = 56.25%\n",
      "2018-05-20 14:11:41 iteration 100/1000: current training loss = 1.077569, accuracy = 62.50%\n",
      "2018-05-20 14:11:52 iteration 150/1000: current training loss = 0.853917, accuracy = 62.50%\n",
      "2018-05-20 14:12:04 iteration 200/1000: current training loss = 0.898668, accuracy = 53.12%\n",
      "2018-05-20 14:12:15 iteration 250/1000: current training loss = 0.904769, accuracy = 65.62%\n",
      "2018-05-20 14:12:27 iteration 300/1000: current training loss = 0.961657, accuracy = 53.12%\n",
      "2018-05-20 14:12:38 iteration 350/1000: current training loss = 0.992491, accuracy = 59.38%\n",
      "2018-05-20 14:12:50 iteration 400/1000: current training loss = 1.079179, accuracy = 59.38%\n",
      "2018-05-20 14:13:01 iteration 450/1000: current training loss = 0.652210, accuracy = 71.88%\n",
      "2018-05-20 14:13:12 iteration 500/1000: current training loss = 0.886604, accuracy = 53.12%\n",
      "2018-05-20 14:13:24 iteration 550/1000: current training loss = 1.003759, accuracy = 53.12%\n",
      "2018-05-20 14:13:35 iteration 600/1000: current training loss = 0.910734, accuracy = 59.38%\n",
      "2018-05-20 14:13:46 iteration 650/1000: current training loss = 0.927939, accuracy = 53.12%\n",
      "2018-05-20 14:13:58 iteration 700/1000: current training loss = 0.788766, accuracy = 59.38%\n",
      "2018-05-20 14:14:09 iteration 750/1000: current training loss = 0.848429, accuracy = 59.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 14:14:21 iteration 800/1000: current training loss = 0.922935, accuracy = 53.12%\n",
      "2018-05-20 14:14:32 iteration 850/1000: current training loss = 0.812742, accuracy = 68.75%\n",
      "2018-05-20 14:14:44 iteration 900/1000: current training loss = 0.689809, accuracy = 71.88%\n",
      "2018-05-20 14:14:56 iteration 950/1000: current training loss = 0.859563, accuracy = 56.25%\n",
      "2018-05-20 14:15:07 iteration 1000/1000: current training loss = 0.739509, accuracy = 59.38%\n",
      "2018-05-20 14:15:38 end epoch 8/50: acc_train=65.000% acc_val=61.938% acc_test=62.719%\n",
      "\n",
      "2018-05-20 14:15:38 start epoch 9/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:15:39 iteration 1/1000: current training loss = 0.867972, accuracy = 56.25%\n",
      "2018-05-20 14:15:49 iteration 50/1000: current training loss = 0.691615, accuracy = 78.12%\n",
      "2018-05-20 14:16:01 iteration 100/1000: current training loss = 0.854423, accuracy = 68.75%\n",
      "2018-05-20 14:16:13 iteration 150/1000: current training loss = 0.741152, accuracy = 68.75%\n",
      "2018-05-20 14:16:24 iteration 200/1000: current training loss = 0.860411, accuracy = 62.50%\n",
      "2018-05-20 14:16:35 iteration 250/1000: current training loss = 0.816271, accuracy = 59.38%\n",
      "2018-05-20 14:16:46 iteration 300/1000: current training loss = 1.009566, accuracy = 56.25%\n",
      "2018-05-20 14:16:57 iteration 350/1000: current training loss = 0.825963, accuracy = 65.62%\n",
      "2018-05-20 14:17:09 iteration 400/1000: current training loss = 0.980591, accuracy = 43.75%\n",
      "2018-05-20 14:17:22 iteration 450/1000: current training loss = 0.728393, accuracy = 62.50%\n",
      "2018-05-20 14:17:33 iteration 500/1000: current training loss = 1.081598, accuracy = 40.62%\n",
      "2018-05-20 14:17:44 iteration 550/1000: current training loss = 0.684477, accuracy = 75.00%\n",
      "2018-05-20 14:17:56 iteration 600/1000: current training loss = 0.851790, accuracy = 46.88%\n",
      "2018-05-20 14:18:07 iteration 650/1000: current training loss = 0.749552, accuracy = 62.50%\n",
      "2018-05-20 14:18:19 iteration 700/1000: current training loss = 0.743912, accuracy = 65.62%\n",
      "2018-05-20 14:18:31 iteration 750/1000: current training loss = 0.604095, accuracy = 65.62%\n",
      "2018-05-20 14:18:42 iteration 800/1000: current training loss = 0.723646, accuracy = 65.62%\n",
      "2018-05-20 14:18:53 iteration 850/1000: current training loss = 0.855191, accuracy = 56.25%\n",
      "2018-05-20 14:19:05 iteration 900/1000: current training loss = 0.780951, accuracy = 65.62%\n",
      "2018-05-20 14:19:16 iteration 950/1000: current training loss = 0.884220, accuracy = 53.12%\n",
      "2018-05-20 14:19:28 iteration 1000/1000: current training loss = 0.652345, accuracy = 75.00%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 14:19:59 end epoch 9/50: acc_train=65.031% acc_val=64.281% acc_test=62.469%\n",
      "\n",
      "2018-05-20 14:19:59 start epoch 10/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:20:00 iteration 1/1000: current training loss = 0.893099, accuracy = 62.50%\n",
      "2018-05-20 14:20:10 iteration 50/1000: current training loss = 0.798236, accuracy = 59.38%\n",
      "2018-05-20 14:20:22 iteration 100/1000: current training loss = 0.788694, accuracy = 75.00%\n",
      "2018-05-20 14:20:34 iteration 150/1000: current training loss = 0.721188, accuracy = 68.75%\n",
      "2018-05-20 14:20:45 iteration 200/1000: current training loss = 0.715396, accuracy = 71.88%\n",
      "2018-05-20 14:20:57 iteration 250/1000: current training loss = 0.726679, accuracy = 75.00%\n",
      "2018-05-20 14:21:09 iteration 300/1000: current training loss = 0.889438, accuracy = 62.50%\n",
      "2018-05-20 14:21:21 iteration 350/1000: current training loss = 1.063256, accuracy = 43.75%\n",
      "2018-05-20 14:21:32 iteration 400/1000: current training loss = 0.690148, accuracy = 59.38%\n",
      "2018-05-20 14:21:43 iteration 450/1000: current training loss = 0.930468, accuracy = 68.75%\n",
      "2018-05-20 14:21:55 iteration 500/1000: current training loss = 0.999530, accuracy = 59.38%\n",
      "2018-05-20 14:22:07 iteration 550/1000: current training loss = 0.866223, accuracy = 56.25%\n",
      "2018-05-20 14:22:19 iteration 600/1000: current training loss = 0.869817, accuracy = 68.75%\n",
      "2018-05-20 14:22:30 iteration 650/1000: current training loss = 1.162113, accuracy = 50.00%\n",
      "2018-05-20 14:22:42 iteration 700/1000: current training loss = 0.693624, accuracy = 59.38%\n",
      "2018-05-20 14:22:55 iteration 750/1000: current training loss = 0.747668, accuracy = 59.38%\n",
      "2018-05-20 14:23:06 iteration 800/1000: current training loss = 0.859037, accuracy = 53.12%\n",
      "2018-05-20 14:23:18 iteration 850/1000: current training loss = 0.795567, accuracy = 68.75%\n",
      "2018-05-20 14:23:29 iteration 900/1000: current training loss = 0.937753, accuracy = 53.12%\n",
      "2018-05-20 14:23:41 iteration 950/1000: current training loss = 0.781735, accuracy = 75.00%\n",
      "2018-05-20 14:23:53 iteration 1000/1000: current training loss = 0.746091, accuracy = 65.62%\n",
      "2018-05-20 14:24:25 end epoch 10/50: acc_train=63.375% acc_val=62.813% acc_test=62.375%\n",
      "\n",
      "2018-05-20 14:24:25 start epoch 11/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:24:25 iteration 1/1000: current training loss = 0.980817, accuracy = 46.88%\n",
      "2018-05-20 14:24:36 iteration 50/1000: current training loss = 0.684692, accuracy = 62.50%\n",
      "2018-05-20 14:24:48 iteration 100/1000: current training loss = 0.849200, accuracy = 62.50%\n",
      "2018-05-20 14:24:59 iteration 150/1000: current training loss = 0.652036, accuracy = 68.75%\n",
      "2018-05-20 14:25:10 iteration 200/1000: current training loss = 0.647210, accuracy = 65.62%\n",
      "2018-05-20 14:25:22 iteration 250/1000: current training loss = 0.623356, accuracy = 68.75%\n",
      "2018-05-20 14:25:33 iteration 300/1000: current training loss = 0.779770, accuracy = 59.38%\n",
      "2018-05-20 14:25:44 iteration 350/1000: current training loss = 0.912638, accuracy = 50.00%\n",
      "2018-05-20 14:25:55 iteration 400/1000: current training loss = 0.722072, accuracy = 71.88%\n",
      "2018-05-20 14:26:06 iteration 450/1000: current training loss = 0.903855, accuracy = 62.50%\n",
      "2018-05-20 14:26:18 iteration 500/1000: current training loss = 0.795401, accuracy = 65.62%\n",
      "2018-05-20 14:26:29 iteration 550/1000: current training loss = 0.666467, accuracy = 71.88%\n",
      "2018-05-20 14:26:40 iteration 600/1000: current training loss = 0.940340, accuracy = 53.12%\n",
      "2018-05-20 14:26:51 iteration 650/1000: current training loss = 0.705730, accuracy = 75.00%\n",
      "2018-05-20 14:27:04 iteration 700/1000: current training loss = 0.757787, accuracy = 56.25%\n",
      "2018-05-20 14:27:15 iteration 750/1000: current training loss = 0.670880, accuracy = 56.25%\n",
      "2018-05-20 14:27:28 iteration 800/1000: current training loss = 0.918275, accuracy = 62.50%\n",
      "2018-05-20 14:27:40 iteration 850/1000: current training loss = 0.654398, accuracy = 75.00%\n",
      "2018-05-20 14:27:51 iteration 900/1000: current training loss = 0.851589, accuracy = 62.50%\n",
      "2018-05-20 14:28:03 iteration 950/1000: current training loss = 0.649103, accuracy = 78.12%\n",
      "2018-05-20 14:28:14 iteration 1000/1000: current training loss = 0.719284, accuracy = 75.00%\n",
      "2018-05-20 14:28:45 end epoch 11/50: acc_train=65.000% acc_val=63.406% acc_test=63.344%\n",
      "\n",
      "2018-05-20 14:28:45 start epoch 12/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:28:46 iteration 1/1000: current training loss = 0.917068, accuracy = 59.38%\n",
      "2018-05-20 14:28:56 iteration 50/1000: current training loss = 0.652386, accuracy = 68.75%\n",
      "2018-05-20 14:29:07 iteration 100/1000: current training loss = 0.752194, accuracy = 65.62%\n",
      "2018-05-20 14:29:19 iteration 150/1000: current training loss = 0.801810, accuracy = 68.75%\n",
      "2018-05-20 14:29:30 iteration 200/1000: current training loss = 0.863033, accuracy = 56.25%\n",
      "2018-05-20 14:29:41 iteration 250/1000: current training loss = 0.866976, accuracy = 68.75%\n",
      "2018-05-20 14:29:52 iteration 300/1000: current training loss = 1.096906, accuracy = 65.62%\n",
      "2018-05-20 14:30:03 iteration 350/1000: current training loss = 0.772899, accuracy = 62.50%\n",
      "2018-05-20 14:30:14 iteration 400/1000: current training loss = 0.823792, accuracy = 62.50%\n",
      "2018-05-20 14:30:26 iteration 450/1000: current training loss = 0.790005, accuracy = 71.88%\n",
      "2018-05-20 14:30:37 iteration 500/1000: current training loss = 0.662495, accuracy = 68.75%\n",
      "2018-05-20 14:30:49 iteration 550/1000: current training loss = 0.898051, accuracy = 56.25%\n",
      "2018-05-20 14:31:00 iteration 600/1000: current training loss = 0.722620, accuracy = 68.75%\n",
      "2018-05-20 14:31:11 iteration 650/1000: current training loss = 0.779523, accuracy = 68.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 14:31:23 iteration 700/1000: current training loss = 0.782810, accuracy = 62.50%\n",
      "2018-05-20 14:31:34 iteration 750/1000: current training loss = 0.746678, accuracy = 62.50%\n",
      "2018-05-20 14:31:45 iteration 800/1000: current training loss = 0.695906, accuracy = 68.75%\n",
      "2018-05-20 14:31:56 iteration 850/1000: current training loss = 0.610116, accuracy = 68.75%\n",
      "2018-05-20 14:32:07 iteration 900/1000: current training loss = 0.844425, accuracy = 71.88%\n",
      "2018-05-20 14:32:19 iteration 950/1000: current training loss = 0.634873, accuracy = 75.00%\n",
      "2018-05-20 14:32:30 iteration 1000/1000: current training loss = 0.904529, accuracy = 56.25%\n",
      "2018-05-20 14:33:00 end epoch 12/50: acc_train=66.375% acc_val=63.000% acc_test=63.875%\n",
      "\n",
      "2018-05-20 14:33:00 start epoch 13/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:33:01 iteration 1/1000: current training loss = 0.899096, accuracy = 56.25%\n",
      "2018-05-20 14:33:12 iteration 50/1000: current training loss = 0.768805, accuracy = 59.38%\n",
      "2018-05-20 14:33:23 iteration 100/1000: current training loss = 0.638279, accuracy = 68.75%\n",
      "2018-05-20 14:33:34 iteration 150/1000: current training loss = 0.893926, accuracy = 59.38%\n",
      "2018-05-20 14:33:46 iteration 200/1000: current training loss = 0.929774, accuracy = 46.88%\n",
      "2018-05-20 14:33:56 iteration 250/1000: current training loss = 0.653339, accuracy = 75.00%\n",
      "2018-05-20 14:34:07 iteration 300/1000: current training loss = 0.872004, accuracy = 68.75%\n",
      "2018-05-20 14:34:18 iteration 350/1000: current training loss = 0.930148, accuracy = 62.50%\n",
      "2018-05-20 14:34:28 iteration 400/1000: current training loss = 0.992464, accuracy = 62.50%\n",
      "2018-05-20 14:34:40 iteration 450/1000: current training loss = 1.065292, accuracy = 56.25%\n",
      "2018-05-20 14:34:50 iteration 500/1000: current training loss = 0.714052, accuracy = 75.00%\n",
      "2018-05-20 14:35:01 iteration 550/1000: current training loss = 0.755984, accuracy = 65.62%\n",
      "2018-05-20 14:35:12 iteration 600/1000: current training loss = 0.706785, accuracy = 71.88%\n",
      "2018-05-20 14:35:22 iteration 650/1000: current training loss = 0.958224, accuracy = 50.00%\n",
      "2018-05-20 14:35:34 iteration 700/1000: current training loss = 0.695823, accuracy = 78.12%\n",
      "2018-05-20 14:35:44 iteration 750/1000: current training loss = 0.765037, accuracy = 62.50%\n",
      "2018-05-20 14:35:55 iteration 800/1000: current training loss = 0.724046, accuracy = 71.88%\n",
      "2018-05-20 14:36:06 iteration 850/1000: current training loss = 0.890136, accuracy = 50.00%\n",
      "2018-05-20 14:36:17 iteration 900/1000: current training loss = 0.714123, accuracy = 78.12%\n",
      "2018-05-20 14:36:28 iteration 950/1000: current training loss = 0.539920, accuracy = 78.12%\n",
      "2018-05-20 14:36:38 iteration 1000/1000: current training loss = 0.727916, accuracy = 75.00%\n",
      "2018-05-20 14:37:06 end epoch 13/50: acc_train=64.625% acc_val=63.281% acc_test=64.062%\n",
      "\n",
      "2018-05-20 14:37:06 start epoch 14/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:37:07 iteration 1/1000: current training loss = 0.872243, accuracy = 56.25%\n",
      "2018-05-20 14:37:18 iteration 50/1000: current training loss = 0.694528, accuracy = 78.12%\n",
      "2018-05-20 14:37:29 iteration 100/1000: current training loss = 0.772454, accuracy = 68.75%\n",
      "2018-05-20 14:37:40 iteration 150/1000: current training loss = 1.047505, accuracy = 46.88%\n",
      "2018-05-20 14:37:50 iteration 200/1000: current training loss = 0.942059, accuracy = 56.25%\n",
      "2018-05-20 14:38:00 iteration 250/1000: current training loss = 0.661439, accuracy = 62.50%\n",
      "2018-05-20 14:38:11 iteration 300/1000: current training loss = 0.747751, accuracy = 71.88%\n",
      "2018-05-20 14:38:22 iteration 350/1000: current training loss = 0.814880, accuracy = 65.62%\n",
      "2018-05-20 14:38:33 iteration 400/1000: current training loss = 0.617474, accuracy = 81.25%\n",
      "2018-05-20 14:38:44 iteration 450/1000: current training loss = 0.740377, accuracy = 71.88%\n",
      "2018-05-20 14:38:55 iteration 500/1000: current training loss = 0.704859, accuracy = 71.88%\n",
      "2018-05-20 14:39:07 iteration 550/1000: current training loss = 0.868149, accuracy = 50.00%\n",
      "2018-05-20 14:39:18 iteration 600/1000: current training loss = 0.601396, accuracy = 71.88%\n",
      "2018-05-20 14:39:29 iteration 650/1000: current training loss = 0.969792, accuracy = 46.88%\n",
      "2018-05-20 14:39:40 iteration 700/1000: current training loss = 0.743708, accuracy = 81.25%\n",
      "2018-05-20 14:39:51 iteration 750/1000: current training loss = 0.778203, accuracy = 56.25%\n",
      "2018-05-20 14:40:02 iteration 800/1000: current training loss = 0.800429, accuracy = 65.62%\n",
      "2018-05-20 14:40:13 iteration 850/1000: current training loss = 1.017445, accuracy = 50.00%\n",
      "2018-05-20 14:40:24 iteration 900/1000: current training loss = 0.637480, accuracy = 81.25%\n",
      "2018-05-20 14:40:35 iteration 950/1000: current training loss = 0.759366, accuracy = 65.62%\n",
      "2018-05-20 14:40:47 iteration 1000/1000: current training loss = 0.873059, accuracy = 68.75%\n",
      "2018-05-20 14:41:16 end epoch 14/50: acc_train=65.156% acc_val=64.094% acc_test=64.938%\n",
      "\n",
      "2018-05-20 14:41:16 start epoch 15/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:41:17 iteration 1/1000: current training loss = 0.611825, accuracy = 78.12%\n",
      "2018-05-20 14:41:27 iteration 50/1000: current training loss = 0.676324, accuracy = 68.75%\n",
      "2018-05-20 14:41:38 iteration 100/1000: current training loss = 0.770954, accuracy = 59.38%\n",
      "2018-05-20 14:41:49 iteration 150/1000: current training loss = 0.819595, accuracy = 53.12%\n",
      "2018-05-20 14:42:00 iteration 200/1000: current training loss = 0.982223, accuracy = 56.25%\n",
      "2018-05-20 14:42:11 iteration 250/1000: current training loss = 0.589583, accuracy = 71.88%\n",
      "2018-05-20 14:42:22 iteration 300/1000: current training loss = 0.707643, accuracy = 71.88%\n",
      "2018-05-20 14:42:34 iteration 350/1000: current training loss = 0.700471, accuracy = 68.75%\n",
      "2018-05-20 14:42:45 iteration 400/1000: current training loss = 0.626943, accuracy = 75.00%\n",
      "2018-05-20 14:42:56 iteration 450/1000: current training loss = 1.111592, accuracy = 53.12%\n",
      "2018-05-20 14:43:07 iteration 500/1000: current training loss = 0.781900, accuracy = 78.12%\n",
      "2018-05-20 14:43:17 iteration 550/1000: current training loss = 0.570304, accuracy = 68.75%\n",
      "2018-05-20 14:43:29 iteration 600/1000: current training loss = 1.070684, accuracy = 56.25%\n",
      "2018-05-20 14:43:40 iteration 650/1000: current training loss = 0.729122, accuracy = 75.00%\n",
      "2018-05-20 14:43:50 iteration 700/1000: current training loss = 0.616459, accuracy = 75.00%\n",
      "2018-05-20 14:44:02 iteration 750/1000: current training loss = 0.566152, accuracy = 81.25%\n",
      "2018-05-20 14:44:12 iteration 800/1000: current training loss = 0.732495, accuracy = 65.62%\n",
      "2018-05-20 14:44:23 iteration 850/1000: current training loss = 0.819995, accuracy = 68.75%\n",
      "2018-05-20 14:44:34 iteration 900/1000: current training loss = 0.934767, accuracy = 46.88%\n",
      "2018-05-20 14:44:44 iteration 950/1000: current training loss = 0.848718, accuracy = 65.62%\n",
      "2018-05-20 14:44:55 iteration 1000/1000: current training loss = 0.753658, accuracy = 68.75%\n",
      "2018-05-20 14:45:25 end epoch 15/50: acc_train=66.906% acc_val=63.969% acc_test=64.906%\n",
      "\n",
      "2018-05-20 14:45:25 start epoch 16/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:45:26 iteration 1/1000: current training loss = 0.960436, accuracy = 53.12%\n",
      "2018-05-20 14:45:37 iteration 50/1000: current training loss = 0.979617, accuracy = 59.38%\n",
      "2018-05-20 14:45:48 iteration 100/1000: current training loss = 0.655617, accuracy = 71.88%\n",
      "2018-05-20 14:45:59 iteration 150/1000: current training loss = 0.569720, accuracy = 84.38%\n",
      "2018-05-20 14:46:10 iteration 200/1000: current training loss = 0.588441, accuracy = 78.12%\n",
      "2018-05-20 14:46:21 iteration 250/1000: current training loss = 0.888934, accuracy = 62.50%\n",
      "2018-05-20 14:46:32 iteration 300/1000: current training loss = 0.792285, accuracy = 62.50%\n",
      "2018-05-20 14:46:43 iteration 350/1000: current training loss = 0.683532, accuracy = 78.12%\n",
      "2018-05-20 14:46:54 iteration 400/1000: current training loss = 0.979137, accuracy = 56.25%\n",
      "2018-05-20 14:47:05 iteration 450/1000: current training loss = 0.615152, accuracy = 65.62%\n",
      "2018-05-20 14:47:17 iteration 500/1000: current training loss = 0.657733, accuracy = 68.75%\n",
      "2018-05-20 14:47:27 iteration 550/1000: current training loss = 0.783086, accuracy = 62.50%\n",
      "2018-05-20 14:47:39 iteration 600/1000: current training loss = 0.764297, accuracy = 56.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 14:47:51 iteration 650/1000: current training loss = 0.841532, accuracy = 56.25%\n",
      "2018-05-20 14:48:03 iteration 700/1000: current training loss = 0.818974, accuracy = 56.25%\n",
      "2018-05-20 14:48:15 iteration 750/1000: current training loss = 0.732875, accuracy = 56.25%\n",
      "2018-05-20 14:48:25 iteration 800/1000: current training loss = 0.987175, accuracy = 56.25%\n",
      "2018-05-20 14:48:37 iteration 850/1000: current training loss = 0.692873, accuracy = 65.62%\n",
      "2018-05-20 14:48:48 iteration 900/1000: current training loss = 0.707580, accuracy = 71.88%\n",
      "2018-05-20 14:49:00 iteration 950/1000: current training loss = 0.632121, accuracy = 75.00%\n",
      "2018-05-20 14:49:10 iteration 1000/1000: current training loss = 0.686996, accuracy = 68.75%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 14:49:43 end epoch 16/50: acc_train=66.000% acc_val=64.469% acc_test=64.312%\n",
      "\n",
      "2018-05-20 14:49:43 start epoch 17/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:49:44 iteration 1/1000: current training loss = 0.844112, accuracy = 62.50%\n",
      "2018-05-20 14:49:55 iteration 50/1000: current training loss = 0.703791, accuracy = 68.75%\n",
      "2018-05-20 14:50:06 iteration 100/1000: current training loss = 0.877391, accuracy = 71.88%\n",
      "2018-05-20 14:50:17 iteration 150/1000: current training loss = 0.777433, accuracy = 56.25%\n",
      "2018-05-20 14:50:29 iteration 200/1000: current training loss = 0.834995, accuracy = 65.62%\n",
      "2018-05-20 14:50:40 iteration 250/1000: current training loss = 0.894848, accuracy = 59.38%\n",
      "2018-05-20 14:50:51 iteration 300/1000: current training loss = 1.131476, accuracy = 50.00%\n",
      "2018-05-20 14:51:03 iteration 350/1000: current training loss = 0.762065, accuracy = 62.50%\n",
      "2018-05-20 14:51:14 iteration 400/1000: current training loss = 0.873615, accuracy = 68.75%\n",
      "2018-05-20 14:51:26 iteration 450/1000: current training loss = 0.865373, accuracy = 53.12%\n",
      "2018-05-20 14:51:38 iteration 500/1000: current training loss = 0.864673, accuracy = 68.75%\n",
      "2018-05-20 14:51:49 iteration 550/1000: current training loss = 0.707790, accuracy = 75.00%\n",
      "2018-05-20 14:52:01 iteration 600/1000: current training loss = 0.719457, accuracy = 65.62%\n",
      "2018-05-20 14:52:12 iteration 650/1000: current training loss = 0.852142, accuracy = 56.25%\n",
      "2018-05-20 14:52:23 iteration 700/1000: current training loss = 0.885168, accuracy = 56.25%\n",
      "2018-05-20 14:52:35 iteration 750/1000: current training loss = 1.065575, accuracy = 50.00%\n",
      "2018-05-20 14:52:46 iteration 800/1000: current training loss = 0.563914, accuracy = 78.12%\n",
      "2018-05-20 14:52:58 iteration 850/1000: current training loss = 0.686695, accuracy = 71.88%\n",
      "2018-05-20 14:53:09 iteration 900/1000: current training loss = 0.833078, accuracy = 62.50%\n",
      "2018-05-20 14:53:20 iteration 950/1000: current training loss = 0.848336, accuracy = 53.12%\n",
      "2018-05-20 14:53:32 iteration 1000/1000: current training loss = 0.649155, accuracy = 84.38%\n",
      "2018-05-20 14:54:02 end epoch 17/50: acc_train=64.406% acc_val=63.500% acc_test=65.000%\n",
      "\n",
      "2018-05-20 14:54:02 start epoch 18/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:54:03 iteration 1/1000: current training loss = 0.786530, accuracy = 71.88%\n",
      "2018-05-20 14:54:13 iteration 50/1000: current training loss = 0.581953, accuracy = 81.25%\n",
      "2018-05-20 14:54:23 iteration 100/1000: current training loss = 0.824226, accuracy = 56.25%\n",
      "2018-05-20 14:54:35 iteration 150/1000: current training loss = 0.652001, accuracy = 71.88%\n",
      "2018-05-20 14:54:46 iteration 200/1000: current training loss = 0.591265, accuracy = 75.00%\n",
      "2018-05-20 14:54:56 iteration 250/1000: current training loss = 0.744936, accuracy = 65.62%\n",
      "2018-05-20 14:55:07 iteration 300/1000: current training loss = 0.630746, accuracy = 68.75%\n",
      "2018-05-20 14:55:18 iteration 350/1000: current training loss = 0.878488, accuracy = 56.25%\n",
      "2018-05-20 14:55:30 iteration 400/1000: current training loss = 0.609331, accuracy = 59.38%\n",
      "2018-05-20 14:55:42 iteration 450/1000: current training loss = 0.919614, accuracy = 62.50%\n",
      "2018-05-20 14:55:53 iteration 500/1000: current training loss = 0.677749, accuracy = 71.88%\n",
      "2018-05-20 14:56:04 iteration 550/1000: current training loss = 0.925554, accuracy = 53.12%\n",
      "2018-05-20 14:56:15 iteration 600/1000: current training loss = 0.844337, accuracy = 59.38%\n",
      "2018-05-20 14:56:26 iteration 650/1000: current training loss = 0.923129, accuracy = 56.25%\n",
      "2018-05-20 14:56:36 iteration 700/1000: current training loss = 0.822624, accuracy = 62.50%\n",
      "2018-05-20 14:56:47 iteration 750/1000: current training loss = 0.987313, accuracy = 59.38%\n",
      "2018-05-20 14:56:58 iteration 800/1000: current training loss = 1.054047, accuracy = 50.00%\n",
      "2018-05-20 14:57:08 iteration 850/1000: current training loss = 0.617857, accuracy = 78.12%\n",
      "2018-05-20 14:57:20 iteration 900/1000: current training loss = 0.784914, accuracy = 71.88%\n",
      "2018-05-20 14:57:32 iteration 950/1000: current training loss = 0.545599, accuracy = 71.88%\n",
      "2018-05-20 14:57:43 iteration 1000/1000: current training loss = 0.765781, accuracy = 59.38%\n",
      "2018-05-20 14:58:12 end epoch 18/50: acc_train=66.906% acc_val=62.844% acc_test=64.438%\n",
      "\n",
      "2018-05-20 14:58:12 start epoch 19/50, with learning rate = 0.0004000000\n",
      "2018-05-20 14:58:13 iteration 1/1000: current training loss = 0.794369, accuracy = 68.75%\n",
      "2018-05-20 14:58:23 iteration 50/1000: current training loss = 0.584477, accuracy = 75.00%\n",
      "2018-05-20 14:58:33 iteration 100/1000: current training loss = 0.805032, accuracy = 62.50%\n",
      "2018-05-20 14:58:44 iteration 150/1000: current training loss = 0.732674, accuracy = 65.62%\n",
      "2018-05-20 14:58:54 iteration 200/1000: current training loss = 0.738052, accuracy = 62.50%\n",
      "2018-05-20 14:59:06 iteration 250/1000: current training loss = 0.636907, accuracy = 78.12%\n",
      "2018-05-20 14:59:17 iteration 300/1000: current training loss = 0.734814, accuracy = 84.38%\n",
      "2018-05-20 14:59:29 iteration 350/1000: current training loss = 0.937058, accuracy = 65.62%\n",
      "2018-05-20 14:59:41 iteration 400/1000: current training loss = 0.681934, accuracy = 65.62%\n",
      "2018-05-20 14:59:52 iteration 450/1000: current training loss = 1.007291, accuracy = 43.75%\n",
      "2018-05-20 15:00:03 iteration 500/1000: current training loss = 0.513355, accuracy = 84.38%\n",
      "2018-05-20 15:00:14 iteration 550/1000: current training loss = 0.762332, accuracy = 62.50%\n",
      "2018-05-20 15:00:25 iteration 600/1000: current training loss = 0.905005, accuracy = 56.25%\n",
      "2018-05-20 15:00:36 iteration 650/1000: current training loss = 0.797613, accuracy = 68.75%\n",
      "2018-05-20 15:00:49 iteration 700/1000: current training loss = 0.872258, accuracy = 50.00%\n",
      "2018-05-20 15:01:00 iteration 750/1000: current training loss = 0.660807, accuracy = 71.88%\n",
      "2018-05-20 15:01:11 iteration 800/1000: current training loss = 0.839993, accuracy = 59.38%\n",
      "2018-05-20 15:01:23 iteration 850/1000: current training loss = 0.654843, accuracy = 62.50%\n",
      "2018-05-20 15:01:34 iteration 900/1000: current training loss = 0.806803, accuracy = 62.50%\n",
      "2018-05-20 15:01:45 iteration 950/1000: current training loss = 0.877649, accuracy = 59.38%\n",
      "2018-05-20 15:01:55 iteration 1000/1000: current training loss = 0.602212, accuracy = 75.00%\n",
      "2018-05-20 15:02:25 end epoch 19/50: acc_train=65.125% acc_val=62.781% acc_test=64.500%\n",
      "\n",
      "2018-05-20 15:02:25 start epoch 20/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:02:26 iteration 1/1000: current training loss = 0.689231, accuracy = 56.25%\n",
      "2018-05-20 15:02:36 iteration 50/1000: current training loss = 0.780691, accuracy = 62.50%\n",
      "2018-05-20 15:02:47 iteration 100/1000: current training loss = 0.767096, accuracy = 68.75%\n",
      "2018-05-20 15:02:58 iteration 150/1000: current training loss = 0.796338, accuracy = 62.50%\n",
      "2018-05-20 15:03:09 iteration 200/1000: current training loss = 0.759068, accuracy = 65.62%\n",
      "2018-05-20 15:03:20 iteration 250/1000: current training loss = 1.266060, accuracy = 40.62%\n",
      "2018-05-20 15:03:31 iteration 300/1000: current training loss = 0.919714, accuracy = 62.50%\n",
      "2018-05-20 15:03:43 iteration 350/1000: current training loss = 0.802600, accuracy = 68.75%\n",
      "2018-05-20 15:03:54 iteration 400/1000: current training loss = 0.883999, accuracy = 62.50%\n",
      "2018-05-20 15:04:05 iteration 450/1000: current training loss = 0.839149, accuracy = 56.25%\n",
      "2018-05-20 15:04:16 iteration 500/1000: current training loss = 0.780712, accuracy = 68.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 15:04:27 iteration 550/1000: current training loss = 0.750092, accuracy = 68.75%\n",
      "2018-05-20 15:04:38 iteration 600/1000: current training loss = 0.679046, accuracy = 62.50%\n",
      "2018-05-20 15:04:50 iteration 650/1000: current training loss = 0.730031, accuracy = 68.75%\n",
      "2018-05-20 15:05:01 iteration 700/1000: current training loss = 0.640692, accuracy = 75.00%\n",
      "2018-05-20 15:05:13 iteration 750/1000: current training loss = 0.823910, accuracy = 53.12%\n",
      "2018-05-20 15:05:24 iteration 800/1000: current training loss = 0.670029, accuracy = 71.88%\n",
      "2018-05-20 15:05:35 iteration 850/1000: current training loss = 0.781637, accuracy = 62.50%\n",
      "2018-05-20 15:05:45 iteration 900/1000: current training loss = 0.931728, accuracy = 50.00%\n",
      "2018-05-20 15:05:56 iteration 950/1000: current training loss = 0.716243, accuracy = 68.75%\n",
      "2018-05-20 15:06:07 iteration 1000/1000: current training loss = 0.923297, accuracy = 65.62%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 15:06:37 end epoch 20/50: acc_train=66.344% acc_val=64.875% acc_test=65.188%\n",
      "\n",
      "2018-05-20 15:06:37 start epoch 21/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:06:38 iteration 1/1000: current training loss = 0.721495, accuracy = 62.50%\n",
      "2018-05-20 15:06:49 iteration 50/1000: current training loss = 0.810663, accuracy = 53.12%\n",
      "2018-05-20 15:07:00 iteration 100/1000: current training loss = 0.642212, accuracy = 71.88%\n",
      "2018-05-20 15:07:11 iteration 150/1000: current training loss = 0.763383, accuracy = 65.62%\n",
      "2018-05-20 15:07:22 iteration 200/1000: current training loss = 1.008591, accuracy = 46.88%\n",
      "2018-05-20 15:07:34 iteration 250/1000: current training loss = 0.967186, accuracy = 50.00%\n",
      "2018-05-20 15:07:45 iteration 300/1000: current training loss = 0.792867, accuracy = 75.00%\n",
      "2018-05-20 15:07:56 iteration 350/1000: current training loss = 0.792565, accuracy = 71.88%\n",
      "2018-05-20 15:08:06 iteration 400/1000: current training loss = 0.530788, accuracy = 81.25%\n",
      "2018-05-20 15:08:17 iteration 450/1000: current training loss = 0.675005, accuracy = 62.50%\n",
      "2018-05-20 15:08:29 iteration 500/1000: current training loss = 0.802085, accuracy = 62.50%\n",
      "2018-05-20 15:08:40 iteration 550/1000: current training loss = 0.825494, accuracy = 65.62%\n",
      "2018-05-20 15:08:51 iteration 600/1000: current training loss = 0.750062, accuracy = 68.75%\n",
      "2018-05-20 15:09:02 iteration 650/1000: current training loss = 0.849120, accuracy = 59.38%\n",
      "2018-05-20 15:09:13 iteration 700/1000: current training loss = 0.648289, accuracy = 71.88%\n",
      "2018-05-20 15:09:24 iteration 750/1000: current training loss = 0.553642, accuracy = 75.00%\n",
      "2018-05-20 15:09:35 iteration 800/1000: current training loss = 0.724729, accuracy = 71.88%\n",
      "2018-05-20 15:09:47 iteration 850/1000: current training loss = 0.787702, accuracy = 56.25%\n",
      "2018-05-20 15:09:58 iteration 900/1000: current training loss = 0.670745, accuracy = 71.88%\n",
      "2018-05-20 15:10:09 iteration 950/1000: current training loss = 0.652605, accuracy = 71.88%\n",
      "2018-05-20 15:10:20 iteration 1000/1000: current training loss = 0.864130, accuracy = 59.38%\n",
      "Currently maximum accuracy on validation set, model saved in path: parameters/HAN.ckpt\n",
      "2018-05-20 15:10:51 end epoch 21/50: acc_train=67.281% acc_val=66.719% acc_test=63.813%\n",
      "\n",
      "2018-05-20 15:10:51 start epoch 22/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:10:52 iteration 1/1000: current training loss = 0.626822, accuracy = 71.88%\n",
      "2018-05-20 15:11:02 iteration 50/1000: current training loss = 0.822161, accuracy = 56.25%\n",
      "2018-05-20 15:11:13 iteration 100/1000: current training loss = 0.828723, accuracy = 65.62%\n",
      "2018-05-20 15:11:24 iteration 150/1000: current training loss = 0.763863, accuracy = 62.50%\n",
      "2018-05-20 15:11:35 iteration 200/1000: current training loss = 0.678006, accuracy = 75.00%\n",
      "2018-05-20 15:11:46 iteration 250/1000: current training loss = 0.742219, accuracy = 71.88%\n",
      "2018-05-20 15:11:57 iteration 300/1000: current training loss = 0.906029, accuracy = 56.25%\n",
      "2018-05-20 15:12:09 iteration 350/1000: current training loss = 0.701998, accuracy = 68.75%\n",
      "2018-05-20 15:12:20 iteration 400/1000: current training loss = 0.784937, accuracy = 68.75%\n",
      "2018-05-20 15:12:32 iteration 450/1000: current training loss = 0.766453, accuracy = 71.88%\n",
      "2018-05-20 15:12:43 iteration 500/1000: current training loss = 0.910158, accuracy = 59.38%\n",
      "2018-05-20 15:12:54 iteration 550/1000: current training loss = 0.797462, accuracy = 62.50%\n",
      "2018-05-20 15:13:05 iteration 600/1000: current training loss = 0.509396, accuracy = 78.12%\n",
      "2018-05-20 15:13:16 iteration 650/1000: current training loss = 0.656744, accuracy = 75.00%\n",
      "2018-05-20 15:13:27 iteration 700/1000: current training loss = 0.665846, accuracy = 68.75%\n",
      "2018-05-20 15:13:38 iteration 750/1000: current training loss = 0.795861, accuracy = 65.62%\n",
      "2018-05-20 15:13:49 iteration 800/1000: current training loss = 0.698514, accuracy = 68.75%\n",
      "2018-05-20 15:14:00 iteration 850/1000: current training loss = 0.702207, accuracy = 75.00%\n",
      "2018-05-20 15:14:10 iteration 900/1000: current training loss = 0.655027, accuracy = 78.12%\n",
      "2018-05-20 15:14:19 iteration 950/1000: current training loss = 0.578564, accuracy = 71.88%\n",
      "2018-05-20 15:14:29 iteration 1000/1000: current training loss = 0.657470, accuracy = 68.75%\n",
      "2018-05-20 15:14:58 end epoch 22/50: acc_train=66.156% acc_val=63.344% acc_test=64.375%\n",
      "\n",
      "2018-05-20 15:14:58 start epoch 23/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:14:59 iteration 1/1000: current training loss = 0.751008, accuracy = 68.75%\n",
      "2018-05-20 15:15:10 iteration 50/1000: current training loss = 0.713556, accuracy = 65.62%\n",
      "2018-05-20 15:15:21 iteration 100/1000: current training loss = 1.085740, accuracy = 65.62%\n",
      "2018-05-20 15:15:32 iteration 150/1000: current training loss = 0.786852, accuracy = 68.75%\n",
      "2018-05-20 15:15:43 iteration 200/1000: current training loss = 0.786713, accuracy = 53.12%\n",
      "2018-05-20 15:15:55 iteration 250/1000: current training loss = 0.488271, accuracy = 81.25%\n",
      "2018-05-20 15:16:06 iteration 300/1000: current training loss = 0.679031, accuracy = 71.88%\n",
      "2018-05-20 15:16:17 iteration 350/1000: current training loss = 0.846676, accuracy = 59.38%\n",
      "2018-05-20 15:16:28 iteration 400/1000: current training loss = 0.684734, accuracy = 62.50%\n",
      "2018-05-20 15:16:40 iteration 450/1000: current training loss = 0.659985, accuracy = 78.12%\n",
      "2018-05-20 15:16:51 iteration 500/1000: current training loss = 0.879383, accuracy = 56.25%\n",
      "2018-05-20 15:17:01 iteration 550/1000: current training loss = 0.677933, accuracy = 75.00%\n",
      "2018-05-20 15:17:12 iteration 600/1000: current training loss = 0.853771, accuracy = 65.62%\n",
      "2018-05-20 15:17:23 iteration 650/1000: current training loss = 0.698093, accuracy = 75.00%\n",
      "2018-05-20 15:17:35 iteration 700/1000: current training loss = 0.749553, accuracy = 68.75%\n",
      "2018-05-20 15:17:46 iteration 750/1000: current training loss = 0.682743, accuracy = 71.88%\n",
      "2018-05-20 15:17:58 iteration 800/1000: current training loss = 0.599881, accuracy = 65.62%\n",
      "2018-05-20 15:18:09 iteration 850/1000: current training loss = 0.596325, accuracy = 71.88%\n",
      "2018-05-20 15:18:20 iteration 900/1000: current training loss = 0.720235, accuracy = 68.75%\n",
      "2018-05-20 15:18:31 iteration 950/1000: current training loss = 0.755345, accuracy = 65.62%\n",
      "2018-05-20 15:18:43 iteration 1000/1000: current training loss = 0.845239, accuracy = 56.25%\n",
      "2018-05-20 15:19:12 end epoch 23/50: acc_train=66.031% acc_val=63.750% acc_test=65.875%\n",
      "\n",
      "2018-05-20 15:19:12 start epoch 24/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:19:13 iteration 1/1000: current training loss = 0.820445, accuracy = 56.25%\n",
      "2018-05-20 15:19:22 iteration 50/1000: current training loss = 0.846691, accuracy = 68.75%\n",
      "2018-05-20 15:19:33 iteration 100/1000: current training loss = 0.615633, accuracy = 78.12%\n",
      "2018-05-20 15:19:44 iteration 150/1000: current training loss = 0.730156, accuracy = 71.88%\n",
      "2018-05-20 15:19:55 iteration 200/1000: current training loss = 0.643564, accuracy = 71.88%\n",
      "2018-05-20 15:20:07 iteration 250/1000: current training loss = 0.661820, accuracy = 62.50%\n",
      "2018-05-20 15:20:18 iteration 300/1000: current training loss = 0.488048, accuracy = 87.50%\n",
      "2018-05-20 15:20:29 iteration 350/1000: current training loss = 0.623103, accuracy = 71.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 15:20:40 iteration 400/1000: current training loss = 0.712983, accuracy = 65.62%\n",
      "2018-05-20 15:20:51 iteration 450/1000: current training loss = 0.971018, accuracy = 43.75%\n",
      "2018-05-20 15:21:02 iteration 500/1000: current training loss = 0.600835, accuracy = 78.12%\n",
      "2018-05-20 15:21:13 iteration 550/1000: current training loss = 0.653538, accuracy = 75.00%\n",
      "2018-05-20 15:21:24 iteration 600/1000: current training loss = 0.686180, accuracy = 68.75%\n",
      "2018-05-20 15:21:35 iteration 650/1000: current training loss = 0.785255, accuracy = 56.25%\n",
      "2018-05-20 15:21:46 iteration 700/1000: current training loss = 0.724797, accuracy = 68.75%\n",
      "2018-05-20 15:21:58 iteration 750/1000: current training loss = 0.671804, accuracy = 71.88%\n",
      "2018-05-20 15:22:09 iteration 800/1000: current training loss = 0.904970, accuracy = 56.25%\n",
      "2018-05-20 15:22:20 iteration 850/1000: current training loss = 0.793571, accuracy = 56.25%\n",
      "2018-05-20 15:22:31 iteration 900/1000: current training loss = 0.878021, accuracy = 56.25%\n",
      "2018-05-20 15:22:42 iteration 950/1000: current training loss = 0.623332, accuracy = 68.75%\n",
      "2018-05-20 15:22:54 iteration 1000/1000: current training loss = 0.655027, accuracy = 75.00%\n",
      "2018-05-20 15:23:24 end epoch 24/50: acc_train=65.750% acc_val=63.625% acc_test=63.750%\n",
      "\n",
      "2018-05-20 15:23:24 start epoch 25/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:23:24 iteration 1/1000: current training loss = 0.739958, accuracy = 62.50%\n",
      "2018-05-20 15:23:35 iteration 50/1000: current training loss = 0.838449, accuracy = 62.50%\n",
      "2018-05-20 15:23:47 iteration 100/1000: current training loss = 0.727901, accuracy = 71.88%\n",
      "2018-05-20 15:23:58 iteration 150/1000: current training loss = 0.738687, accuracy = 68.75%\n",
      "2018-05-20 15:24:09 iteration 200/1000: current training loss = 0.600095, accuracy = 65.62%\n",
      "2018-05-20 15:24:20 iteration 250/1000: current training loss = 0.876803, accuracy = 62.50%\n",
      "2018-05-20 15:24:31 iteration 300/1000: current training loss = 0.837316, accuracy = 53.12%\n",
      "2018-05-20 15:24:42 iteration 350/1000: current training loss = 0.668673, accuracy = 62.50%\n",
      "2018-05-20 15:24:53 iteration 400/1000: current training loss = 0.615922, accuracy = 68.75%\n",
      "2018-05-20 15:25:05 iteration 450/1000: current training loss = 0.823275, accuracy = 65.62%\n",
      "2018-05-20 15:25:17 iteration 500/1000: current training loss = 0.582852, accuracy = 78.12%\n",
      "2018-05-20 15:25:28 iteration 550/1000: current training loss = 0.762187, accuracy = 75.00%\n",
      "2018-05-20 15:25:40 iteration 600/1000: current training loss = 0.745982, accuracy = 68.75%\n",
      "2018-05-20 15:25:52 iteration 650/1000: current training loss = 0.934267, accuracy = 56.25%\n",
      "2018-05-20 15:26:02 iteration 700/1000: current training loss = 0.653209, accuracy = 75.00%\n",
      "2018-05-20 15:26:13 iteration 750/1000: current training loss = 0.783052, accuracy = 53.12%\n",
      "2018-05-20 15:26:25 iteration 800/1000: current training loss = 0.756284, accuracy = 59.38%\n",
      "2018-05-20 15:26:35 iteration 850/1000: current training loss = 0.715471, accuracy = 75.00%\n",
      "2018-05-20 15:26:46 iteration 900/1000: current training loss = 0.686061, accuracy = 68.75%\n",
      "2018-05-20 15:26:58 iteration 950/1000: current training loss = 0.719581, accuracy = 71.88%\n",
      "2018-05-20 15:27:09 iteration 1000/1000: current training loss = 0.767450, accuracy = 53.12%\n",
      "2018-05-20 15:27:39 end epoch 25/50: acc_train=67.031% acc_val=63.625% acc_test=65.094%\n",
      "\n",
      "2018-05-20 15:27:39 start epoch 26/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:27:40 iteration 1/1000: current training loss = 0.997004, accuracy = 65.62%\n",
      "2018-05-20 15:27:52 iteration 50/1000: current training loss = 0.797334, accuracy = 65.62%\n",
      "2018-05-20 15:28:02 iteration 100/1000: current training loss = 0.823176, accuracy = 62.50%\n",
      "2018-05-20 15:28:13 iteration 150/1000: current training loss = 0.691202, accuracy = 68.75%\n",
      "2018-05-20 15:28:24 iteration 200/1000: current training loss = 0.736500, accuracy = 56.25%\n",
      "2018-05-20 15:28:35 iteration 250/1000: current training loss = 0.785550, accuracy = 65.62%\n",
      "2018-05-20 15:28:47 iteration 300/1000: current training loss = 0.728267, accuracy = 78.12%\n",
      "2018-05-20 15:28:57 iteration 350/1000: current training loss = 0.935695, accuracy = 56.25%\n",
      "2018-05-20 15:29:08 iteration 400/1000: current training loss = 0.572036, accuracy = 71.88%\n",
      "2018-05-20 15:29:19 iteration 450/1000: current training loss = 0.612276, accuracy = 75.00%\n",
      "2018-05-20 15:29:30 iteration 500/1000: current training loss = 0.861267, accuracy = 65.62%\n",
      "2018-05-20 15:29:41 iteration 550/1000: current training loss = 0.615232, accuracy = 81.25%\n",
      "2018-05-20 15:29:52 iteration 600/1000: current training loss = 0.694493, accuracy = 81.25%\n",
      "2018-05-20 15:30:03 iteration 650/1000: current training loss = 0.677742, accuracy = 62.50%\n",
      "2018-05-20 15:30:15 iteration 700/1000: current training loss = 0.693724, accuracy = 62.50%\n",
      "2018-05-20 15:30:26 iteration 750/1000: current training loss = 0.703672, accuracy = 68.75%\n",
      "2018-05-20 15:30:38 iteration 800/1000: current training loss = 0.568985, accuracy = 75.00%\n",
      "2018-05-20 15:30:49 iteration 850/1000: current training loss = 0.595465, accuracy = 65.62%\n",
      "2018-05-20 15:30:59 iteration 900/1000: current training loss = 0.793458, accuracy = 65.62%\n",
      "2018-05-20 15:31:10 iteration 950/1000: current training loss = 0.612649, accuracy = 65.62%\n",
      "2018-05-20 15:31:20 iteration 1000/1000: current training loss = 0.689816, accuracy = 62.50%\n",
      "2018-05-20 15:31:49 end epoch 26/50: acc_train=68.156% acc_val=64.562% acc_test=64.750%\n",
      "\n",
      "2018-05-20 15:31:49 start epoch 27/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:31:50 iteration 1/1000: current training loss = 0.761877, accuracy = 62.50%\n",
      "2018-05-20 15:32:01 iteration 50/1000: current training loss = 0.615588, accuracy = 71.88%\n",
      "2018-05-20 15:32:12 iteration 100/1000: current training loss = 0.552839, accuracy = 75.00%\n",
      "2018-05-20 15:32:23 iteration 150/1000: current training loss = 0.696383, accuracy = 68.75%\n",
      "2018-05-20 15:32:34 iteration 200/1000: current training loss = 0.793587, accuracy = 71.88%\n",
      "2018-05-20 15:32:46 iteration 250/1000: current training loss = 0.622815, accuracy = 68.75%\n",
      "2018-05-20 15:32:56 iteration 300/1000: current training loss = 0.650457, accuracy = 62.50%\n",
      "2018-05-20 15:33:08 iteration 350/1000: current training loss = 0.737151, accuracy = 68.75%\n",
      "2018-05-20 15:33:19 iteration 400/1000: current training loss = 0.515641, accuracy = 71.88%\n",
      "2018-05-20 15:33:30 iteration 450/1000: current training loss = 0.633878, accuracy = 75.00%\n",
      "2018-05-20 15:33:42 iteration 500/1000: current training loss = 0.712339, accuracy = 65.62%\n",
      "2018-05-20 15:33:53 iteration 550/1000: current training loss = 0.650934, accuracy = 75.00%\n",
      "2018-05-20 15:34:03 iteration 600/1000: current training loss = 0.761376, accuracy = 71.88%\n",
      "2018-05-20 15:34:14 iteration 650/1000: current training loss = 0.688791, accuracy = 62.50%\n",
      "2018-05-20 15:34:25 iteration 700/1000: current training loss = 0.716677, accuracy = 78.12%\n",
      "2018-05-20 15:34:36 iteration 750/1000: current training loss = 0.723859, accuracy = 68.75%\n",
      "2018-05-20 15:34:47 iteration 800/1000: current training loss = 0.823508, accuracy = 68.75%\n",
      "2018-05-20 15:34:58 iteration 850/1000: current training loss = 0.629606, accuracy = 68.75%\n",
      "2018-05-20 15:35:09 iteration 900/1000: current training loss = 0.751449, accuracy = 68.75%\n",
      "2018-05-20 15:35:19 iteration 950/1000: current training loss = 0.639974, accuracy = 71.88%\n",
      "2018-05-20 15:35:30 iteration 1000/1000: current training loss = 0.666819, accuracy = 71.88%\n",
      "2018-05-20 15:36:01 end epoch 27/50: acc_train=68.156% acc_val=64.594% acc_test=65.438%\n",
      "\n",
      "2018-05-20 15:36:01 start epoch 28/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:36:01 iteration 1/1000: current training loss = 0.778708, accuracy = 59.38%\n",
      "2018-05-20 15:36:13 iteration 50/1000: current training loss = 0.497658, accuracy = 81.25%\n",
      "2018-05-20 15:36:24 iteration 100/1000: current training loss = 0.906456, accuracy = 56.25%\n",
      "2018-05-20 15:36:36 iteration 150/1000: current training loss = 0.500834, accuracy = 84.38%\n",
      "2018-05-20 15:36:47 iteration 200/1000: current training loss = 0.728793, accuracy = 62.50%\n",
      "2018-05-20 15:36:57 iteration 250/1000: current training loss = 0.749260, accuracy = 68.75%\n",
      "2018-05-20 15:37:08 iteration 300/1000: current training loss = 0.615636, accuracy = 71.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 15:37:19 iteration 350/1000: current training loss = 0.840761, accuracy = 68.75%\n",
      "2018-05-20 15:37:30 iteration 400/1000: current training loss = 0.625748, accuracy = 68.75%\n",
      "2018-05-20 15:37:42 iteration 450/1000: current training loss = 0.611673, accuracy = 71.88%\n",
      "2018-05-20 15:37:53 iteration 500/1000: current training loss = 0.713622, accuracy = 75.00%\n",
      "2018-05-20 15:38:04 iteration 550/1000: current training loss = 0.566361, accuracy = 75.00%\n",
      "2018-05-20 15:38:14 iteration 600/1000: current training loss = 0.888797, accuracy = 65.62%\n",
      "2018-05-20 15:38:25 iteration 650/1000: current training loss = 0.678261, accuracy = 68.75%\n",
      "2018-05-20 15:38:37 iteration 700/1000: current training loss = 0.818451, accuracy = 71.88%\n",
      "2018-05-20 15:38:47 iteration 750/1000: current training loss = 0.677155, accuracy = 68.75%\n",
      "2018-05-20 15:38:58 iteration 800/1000: current training loss = 0.811930, accuracy = 71.88%\n",
      "2018-05-20 15:39:09 iteration 850/1000: current training loss = 0.976102, accuracy = 59.38%\n",
      "2018-05-20 15:39:19 iteration 900/1000: current training loss = 0.757781, accuracy = 65.62%\n",
      "2018-05-20 15:39:31 iteration 950/1000: current training loss = 0.762694, accuracy = 59.38%\n",
      "2018-05-20 15:39:43 iteration 1000/1000: current training loss = 0.661960, accuracy = 78.12%\n",
      "2018-05-20 15:40:13 end epoch 28/50: acc_train=68.656% acc_val=64.875% acc_test=64.625%\n",
      "\n",
      "2018-05-20 15:40:13 start epoch 29/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:40:13 iteration 1/1000: current training loss = 0.726371, accuracy = 62.50%\n",
      "2018-05-20 15:40:24 iteration 50/1000: current training loss = 0.639774, accuracy = 71.88%\n",
      "2018-05-20 15:40:35 iteration 100/1000: current training loss = 0.691036, accuracy = 75.00%\n",
      "2018-05-20 15:40:46 iteration 150/1000: current training loss = 0.884788, accuracy = 50.00%\n",
      "2018-05-20 15:40:56 iteration 200/1000: current training loss = 0.815021, accuracy = 68.75%\n",
      "2018-05-20 15:41:07 iteration 250/1000: current training loss = 0.688284, accuracy = 75.00%\n",
      "2018-05-20 15:41:19 iteration 300/1000: current training loss = 0.744820, accuracy = 71.88%\n",
      "2018-05-20 15:41:30 iteration 350/1000: current training loss = 0.607882, accuracy = 84.38%\n",
      "2018-05-20 15:41:40 iteration 400/1000: current training loss = 0.889735, accuracy = 59.38%\n",
      "2018-05-20 15:41:50 iteration 450/1000: current training loss = 0.695682, accuracy = 75.00%\n",
      "2018-05-20 15:42:01 iteration 500/1000: current training loss = 0.542424, accuracy = 87.50%\n",
      "2018-05-20 15:42:12 iteration 550/1000: current training loss = 0.758923, accuracy = 62.50%\n",
      "2018-05-20 15:42:23 iteration 600/1000: current training loss = 0.886637, accuracy = 53.12%\n",
      "2018-05-20 15:42:35 iteration 650/1000: current training loss = 0.523391, accuracy = 75.00%\n",
      "2018-05-20 15:42:45 iteration 700/1000: current training loss = 0.934858, accuracy = 65.62%\n",
      "2018-05-20 15:42:56 iteration 750/1000: current training loss = 0.620226, accuracy = 75.00%\n",
      "2018-05-20 15:43:07 iteration 800/1000: current training loss = 0.678678, accuracy = 65.62%\n",
      "2018-05-20 15:43:19 iteration 850/1000: current training loss = 0.799174, accuracy = 65.62%\n",
      "2018-05-20 15:43:29 iteration 900/1000: current training loss = 0.801278, accuracy = 75.00%\n",
      "2018-05-20 15:43:40 iteration 950/1000: current training loss = 0.675422, accuracy = 71.88%\n",
      "2018-05-20 15:43:51 iteration 1000/1000: current training loss = 0.816879, accuracy = 50.00%\n",
      "2018-05-20 15:44:21 end epoch 29/50: acc_train=68.719% acc_val=64.344% acc_test=65.250%\n",
      "\n",
      "2018-05-20 15:44:21 start epoch 30/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:44:21 iteration 1/1000: current training loss = 0.811775, accuracy = 71.88%\n",
      "2018-05-20 15:44:33 iteration 50/1000: current training loss = 0.885213, accuracy = 62.50%\n",
      "2018-05-20 15:44:43 iteration 100/1000: current training loss = 0.766844, accuracy = 68.75%\n",
      "2018-05-20 15:44:55 iteration 150/1000: current training loss = 0.535582, accuracy = 78.12%\n",
      "2018-05-20 15:45:05 iteration 200/1000: current training loss = 0.692150, accuracy = 68.75%\n",
      "2018-05-20 15:45:16 iteration 250/1000: current training loss = 0.745746, accuracy = 68.75%\n",
      "2018-05-20 15:45:27 iteration 300/1000: current training loss = 0.771949, accuracy = 62.50%\n",
      "2018-05-20 15:45:38 iteration 350/1000: current training loss = 0.736256, accuracy = 62.50%\n",
      "2018-05-20 15:45:49 iteration 400/1000: current training loss = 0.745428, accuracy = 68.75%\n",
      "2018-05-20 15:46:00 iteration 450/1000: current training loss = 0.765473, accuracy = 56.25%\n",
      "2018-05-20 15:46:11 iteration 500/1000: current training loss = 0.859391, accuracy = 68.75%\n",
      "2018-05-20 15:46:21 iteration 550/1000: current training loss = 0.567706, accuracy = 75.00%\n",
      "2018-05-20 15:46:33 iteration 600/1000: current training loss = 0.820940, accuracy = 65.62%\n",
      "2018-05-20 15:46:44 iteration 650/1000: current training loss = 0.765551, accuracy = 53.12%\n",
      "2018-05-20 15:46:54 iteration 700/1000: current training loss = 0.669402, accuracy = 75.00%\n",
      "2018-05-20 15:47:05 iteration 750/1000: current training loss = 0.617328, accuracy = 87.50%\n",
      "2018-05-20 15:47:15 iteration 800/1000: current training loss = 0.755608, accuracy = 62.50%\n",
      "2018-05-20 15:47:27 iteration 850/1000: current training loss = 0.550560, accuracy = 71.88%\n",
      "2018-05-20 15:47:37 iteration 900/1000: current training loss = 0.904118, accuracy = 53.12%\n",
      "2018-05-20 15:47:49 iteration 950/1000: current training loss = 0.639644, accuracy = 78.12%\n",
      "2018-05-20 15:48:00 iteration 1000/1000: current training loss = 1.124090, accuracy = 56.25%\n",
      "2018-05-20 15:48:30 end epoch 30/50: acc_train=66.000% acc_val=64.125% acc_test=63.719%\n",
      "\n",
      "2018-05-20 15:48:30 start epoch 31/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:48:30 iteration 1/1000: current training loss = 0.491012, accuracy = 81.25%\n",
      "2018-05-20 15:48:41 iteration 50/1000: current training loss = 0.648097, accuracy = 75.00%\n",
      "2018-05-20 15:48:52 iteration 100/1000: current training loss = 0.615315, accuracy = 87.50%\n",
      "2018-05-20 15:49:03 iteration 150/1000: current training loss = 0.555228, accuracy = 75.00%\n",
      "2018-05-20 15:49:15 iteration 200/1000: current training loss = 0.753561, accuracy = 56.25%\n",
      "2018-05-20 15:49:26 iteration 250/1000: current training loss = 0.770673, accuracy = 62.50%\n",
      "2018-05-20 15:49:37 iteration 300/1000: current training loss = 1.245052, accuracy = 53.12%\n",
      "2018-05-20 15:49:48 iteration 350/1000: current training loss = 0.742663, accuracy = 59.38%\n",
      "2018-05-20 15:49:59 iteration 400/1000: current training loss = 0.679513, accuracy = 62.50%\n",
      "2018-05-20 15:50:10 iteration 450/1000: current training loss = 0.703923, accuracy = 71.88%\n",
      "2018-05-20 15:50:21 iteration 500/1000: current training loss = 0.632769, accuracy = 68.75%\n",
      "2018-05-20 15:50:32 iteration 550/1000: current training loss = 0.825949, accuracy = 65.62%\n",
      "2018-05-20 15:50:43 iteration 600/1000: current training loss = 0.616272, accuracy = 75.00%\n",
      "2018-05-20 15:50:54 iteration 650/1000: current training loss = 0.643316, accuracy = 62.50%\n",
      "2018-05-20 15:51:05 iteration 700/1000: current training loss = 0.745594, accuracy = 68.75%\n",
      "2018-05-20 15:51:16 iteration 750/1000: current training loss = 0.828447, accuracy = 71.88%\n",
      "2018-05-20 15:51:27 iteration 800/1000: current training loss = 0.570435, accuracy = 75.00%\n",
      "2018-05-20 15:51:38 iteration 850/1000: current training loss = 0.762089, accuracy = 59.38%\n",
      "2018-05-20 15:51:50 iteration 900/1000: current training loss = 0.743626, accuracy = 68.75%\n",
      "2018-05-20 15:52:00 iteration 950/1000: current training loss = 0.694914, accuracy = 62.50%\n",
      "2018-05-20 15:52:11 iteration 1000/1000: current training loss = 0.691344, accuracy = 75.00%\n",
      "2018-05-20 15:52:42 end epoch 31/50: acc_train=67.688% acc_val=64.219% acc_test=64.656%\n",
      "\n",
      "2018-05-20 15:52:42 start epoch 32/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:52:43 iteration 1/1000: current training loss = 0.601634, accuracy = 75.00%\n",
      "2018-05-20 15:52:53 iteration 50/1000: current training loss = 0.533384, accuracy = 78.12%\n",
      "2018-05-20 15:53:04 iteration 100/1000: current training loss = 0.898894, accuracy = 56.25%\n",
      "2018-05-20 15:53:16 iteration 150/1000: current training loss = 0.571008, accuracy = 78.12%\n",
      "2018-05-20 15:53:27 iteration 200/1000: current training loss = 0.609013, accuracy = 68.75%\n",
      "2018-05-20 15:53:39 iteration 250/1000: current training loss = 0.721112, accuracy = 68.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 15:53:51 iteration 300/1000: current training loss = 0.798193, accuracy = 59.38%\n",
      "2018-05-20 15:54:03 iteration 350/1000: current training loss = 0.840909, accuracy = 59.38%\n",
      "2018-05-20 15:54:14 iteration 400/1000: current training loss = 0.764233, accuracy = 68.75%\n",
      "2018-05-20 15:54:25 iteration 450/1000: current training loss = 0.648263, accuracy = 71.88%\n",
      "2018-05-20 15:54:37 iteration 500/1000: current training loss = 0.629546, accuracy = 65.62%\n",
      "2018-05-20 15:54:49 iteration 550/1000: current training loss = 0.722377, accuracy = 71.88%\n",
      "2018-05-20 15:55:02 iteration 600/1000: current training loss = 0.905816, accuracy = 56.25%\n",
      "2018-05-20 15:55:13 iteration 650/1000: current training loss = 1.058613, accuracy = 46.88%\n",
      "2018-05-20 15:55:24 iteration 700/1000: current training loss = 0.794561, accuracy = 71.88%\n",
      "2018-05-20 15:55:35 iteration 750/1000: current training loss = 0.758402, accuracy = 68.75%\n",
      "2018-05-20 15:55:47 iteration 800/1000: current training loss = 0.596999, accuracy = 71.88%\n",
      "2018-05-20 15:55:58 iteration 850/1000: current training loss = 0.897763, accuracy = 68.75%\n",
      "2018-05-20 15:56:10 iteration 900/1000: current training loss = 0.716899, accuracy = 62.50%\n",
      "2018-05-20 15:56:22 iteration 950/1000: current training loss = 0.791910, accuracy = 68.75%\n",
      "2018-05-20 15:56:34 iteration 1000/1000: current training loss = 0.717367, accuracy = 62.50%\n",
      "2018-05-20 15:57:05 end epoch 32/50: acc_train=68.531% acc_val=64.156% acc_test=64.594%\n",
      "\n",
      "2018-05-20 15:57:05 start epoch 33/50, with learning rate = 0.0004000000\n",
      "2018-05-20 15:57:06 iteration 1/1000: current training loss = 0.838264, accuracy = 65.62%\n",
      "2018-05-20 15:57:17 iteration 50/1000: current training loss = 0.707934, accuracy = 62.50%\n",
      "2018-05-20 15:57:28 iteration 100/1000: current training loss = 0.731745, accuracy = 59.38%\n",
      "2018-05-20 15:57:39 iteration 150/1000: current training loss = 0.692400, accuracy = 71.88%\n",
      "2018-05-20 15:57:50 iteration 200/1000: current training loss = 0.655048, accuracy = 71.88%\n",
      "2018-05-20 15:58:02 iteration 250/1000: current training loss = 0.787086, accuracy = 56.25%\n",
      "2018-05-20 15:58:14 iteration 300/1000: current training loss = 0.769909, accuracy = 65.62%\n",
      "2018-05-20 15:58:24 iteration 350/1000: current training loss = 0.656969, accuracy = 56.25%\n",
      "2018-05-20 15:58:36 iteration 400/1000: current training loss = 0.534171, accuracy = 75.00%\n",
      "2018-05-20 15:58:48 iteration 450/1000: current training loss = 0.831061, accuracy = 62.50%\n",
      "2018-05-20 15:58:59 iteration 500/1000: current training loss = 0.793495, accuracy = 65.62%\n",
      "2018-05-20 15:59:10 iteration 550/1000: current training loss = 0.629318, accuracy = 68.75%\n",
      "2018-05-20 15:59:21 iteration 600/1000: current training loss = 0.841880, accuracy = 53.12%\n",
      "2018-05-20 15:59:32 iteration 650/1000: current training loss = 0.617323, accuracy = 65.62%\n",
      "2018-05-20 15:59:43 iteration 700/1000: current training loss = 0.741515, accuracy = 62.50%\n",
      "2018-05-20 15:59:54 iteration 750/1000: current training loss = 0.961978, accuracy = 62.50%\n",
      "2018-05-20 16:00:04 iteration 800/1000: current training loss = 0.731093, accuracy = 71.88%\n",
      "2018-05-20 16:00:17 iteration 850/1000: current training loss = 0.635127, accuracy = 75.00%\n",
      "2018-05-20 16:00:28 iteration 900/1000: current training loss = 0.769757, accuracy = 59.38%\n",
      "2018-05-20 16:00:39 iteration 950/1000: current training loss = 0.658391, accuracy = 75.00%\n",
      "2018-05-20 16:00:51 iteration 1000/1000: current training loss = 0.745253, accuracy = 78.12%\n",
      "2018-05-20 16:01:21 end epoch 33/50: acc_train=67.969% acc_val=63.062% acc_test=64.406%\n",
      "\n",
      "2018-05-20 16:01:21 start epoch 34/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:01:21 iteration 1/1000: current training loss = 0.888074, accuracy = 56.25%\n",
      "2018-05-20 16:01:31 iteration 50/1000: current training loss = 0.775211, accuracy = 50.00%\n",
      "2018-05-20 16:01:42 iteration 100/1000: current training loss = 0.677747, accuracy = 68.75%\n",
      "2018-05-20 16:01:54 iteration 150/1000: current training loss = 0.652360, accuracy = 68.75%\n",
      "2018-05-20 16:02:05 iteration 200/1000: current training loss = 0.915388, accuracy = 62.50%\n",
      "2018-05-20 16:02:16 iteration 250/1000: current training loss = 0.689482, accuracy = 62.50%\n",
      "2018-05-20 16:02:27 iteration 300/1000: current training loss = 0.624686, accuracy = 75.00%\n",
      "2018-05-20 16:02:38 iteration 350/1000: current training loss = 0.548297, accuracy = 81.25%\n",
      "2018-05-20 16:02:49 iteration 400/1000: current training loss = 0.730159, accuracy = 56.25%\n",
      "2018-05-20 16:03:00 iteration 450/1000: current training loss = 0.940197, accuracy = 59.38%\n",
      "2018-05-20 16:03:11 iteration 500/1000: current training loss = 0.625184, accuracy = 75.00%\n",
      "2018-05-20 16:03:23 iteration 550/1000: current training loss = 0.928462, accuracy = 56.25%\n",
      "2018-05-20 16:03:34 iteration 600/1000: current training loss = 0.449680, accuracy = 84.38%\n",
      "2018-05-20 16:03:46 iteration 650/1000: current training loss = 0.843637, accuracy = 53.12%\n",
      "2018-05-20 16:03:57 iteration 700/1000: current training loss = 0.819518, accuracy = 62.50%\n",
      "2018-05-20 16:04:07 iteration 750/1000: current training loss = 0.729715, accuracy = 53.12%\n",
      "2018-05-20 16:04:19 iteration 800/1000: current training loss = 0.609278, accuracy = 65.62%\n",
      "2018-05-20 16:04:30 iteration 850/1000: current training loss = 0.703803, accuracy = 65.62%\n",
      "2018-05-20 16:04:40 iteration 900/1000: current training loss = 0.763938, accuracy = 65.62%\n",
      "2018-05-20 16:04:52 iteration 950/1000: current training loss = 0.562737, accuracy = 78.12%\n",
      "2018-05-20 16:05:03 iteration 1000/1000: current training loss = 0.821082, accuracy = 56.25%\n",
      "2018-05-20 16:05:32 end epoch 34/50: acc_train=68.094% acc_val=64.438% acc_test=64.594%\n",
      "\n",
      "2018-05-20 16:05:32 start epoch 35/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:05:33 iteration 1/1000: current training loss = 0.724121, accuracy = 68.75%\n",
      "2018-05-20 16:05:44 iteration 50/1000: current training loss = 0.769758, accuracy = 59.38%\n",
      "2018-05-20 16:05:55 iteration 100/1000: current training loss = 0.567311, accuracy = 78.12%\n",
      "2018-05-20 16:06:06 iteration 150/1000: current training loss = 0.557879, accuracy = 71.88%\n",
      "2018-05-20 16:06:17 iteration 200/1000: current training loss = 0.603317, accuracy = 78.12%\n",
      "2018-05-20 16:06:29 iteration 250/1000: current training loss = 0.637465, accuracy = 78.12%\n",
      "2018-05-20 16:06:39 iteration 300/1000: current training loss = 0.795601, accuracy = 65.62%\n",
      "2018-05-20 16:06:51 iteration 350/1000: current training loss = 0.619284, accuracy = 68.75%\n",
      "2018-05-20 16:07:01 iteration 400/1000: current training loss = 0.856745, accuracy = 62.50%\n",
      "2018-05-20 16:07:12 iteration 450/1000: current training loss = 0.629009, accuracy = 68.75%\n",
      "2018-05-20 16:07:23 iteration 500/1000: current training loss = 0.766098, accuracy = 65.62%\n",
      "2018-05-20 16:07:34 iteration 550/1000: current training loss = 0.949645, accuracy = 53.12%\n",
      "2018-05-20 16:07:46 iteration 600/1000: current training loss = 0.949738, accuracy = 56.25%\n",
      "2018-05-20 16:07:57 iteration 650/1000: current training loss = 0.723365, accuracy = 68.75%\n",
      "2018-05-20 16:08:07 iteration 700/1000: current training loss = 0.778751, accuracy = 56.25%\n",
      "2018-05-20 16:08:18 iteration 750/1000: current training loss = 0.725625, accuracy = 59.38%\n",
      "2018-05-20 16:08:29 iteration 800/1000: current training loss = 0.624301, accuracy = 68.75%\n",
      "2018-05-20 16:08:40 iteration 850/1000: current training loss = 0.684780, accuracy = 68.75%\n",
      "2018-05-20 16:08:51 iteration 900/1000: current training loss = 0.638286, accuracy = 71.88%\n",
      "2018-05-20 16:09:02 iteration 950/1000: current training loss = 0.679784, accuracy = 68.75%\n",
      "2018-05-20 16:09:13 iteration 1000/1000: current training loss = 0.768248, accuracy = 68.75%\n",
      "2018-05-20 16:09:43 end epoch 35/50: acc_train=68.531% acc_val=65.094% acc_test=65.844%\n",
      "\n",
      "2018-05-20 16:09:43 start epoch 36/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:09:44 iteration 1/1000: current training loss = 0.614997, accuracy = 75.00%\n",
      "2018-05-20 16:09:55 iteration 50/1000: current training loss = 0.854690, accuracy = 56.25%\n",
      "2018-05-20 16:10:06 iteration 100/1000: current training loss = 0.855404, accuracy = 65.62%\n",
      "2018-05-20 16:10:17 iteration 150/1000: current training loss = 0.563126, accuracy = 68.75%\n",
      "2018-05-20 16:10:28 iteration 200/1000: current training loss = 0.686936, accuracy = 71.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 16:10:38 iteration 250/1000: current training loss = 0.787521, accuracy = 62.50%\n",
      "2018-05-20 16:10:49 iteration 300/1000: current training loss = 0.781817, accuracy = 75.00%\n",
      "2018-05-20 16:11:01 iteration 350/1000: current training loss = 0.810135, accuracy = 59.38%\n",
      "2018-05-20 16:11:11 iteration 400/1000: current training loss = 0.799271, accuracy = 62.50%\n",
      "2018-05-20 16:11:23 iteration 450/1000: current training loss = 0.720487, accuracy = 62.50%\n",
      "2018-05-20 16:11:35 iteration 500/1000: current training loss = 0.837819, accuracy = 68.75%\n",
      "2018-05-20 16:11:45 iteration 550/1000: current training loss = 0.588817, accuracy = 65.62%\n",
      "2018-05-20 16:11:57 iteration 600/1000: current training loss = 0.944247, accuracy = 56.25%\n",
      "2018-05-20 16:12:07 iteration 650/1000: current training loss = 0.820434, accuracy = 62.50%\n",
      "2018-05-20 16:12:18 iteration 700/1000: current training loss = 0.810586, accuracy = 62.50%\n",
      "2018-05-20 16:12:28 iteration 750/1000: current training loss = 0.775841, accuracy = 65.62%\n",
      "2018-05-20 16:12:40 iteration 800/1000: current training loss = 0.715501, accuracy = 59.38%\n",
      "2018-05-20 16:12:51 iteration 850/1000: current training loss = 0.528589, accuracy = 75.00%\n",
      "2018-05-20 16:13:02 iteration 900/1000: current training loss = 0.615358, accuracy = 75.00%\n",
      "2018-05-20 16:13:13 iteration 950/1000: current training loss = 0.726460, accuracy = 71.88%\n",
      "2018-05-20 16:13:24 iteration 1000/1000: current training loss = 0.709940, accuracy = 75.00%\n",
      "2018-05-20 16:13:55 end epoch 36/50: acc_train=67.344% acc_val=64.000% acc_test=63.125%\n",
      "\n",
      "2018-05-20 16:13:55 start epoch 37/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:13:55 iteration 1/1000: current training loss = 0.657211, accuracy = 68.75%\n",
      "2018-05-20 16:14:06 iteration 50/1000: current training loss = 0.662397, accuracy = 65.62%\n",
      "2018-05-20 16:14:17 iteration 100/1000: current training loss = 0.831965, accuracy = 68.75%\n",
      "2018-05-20 16:14:28 iteration 150/1000: current training loss = 0.686708, accuracy = 68.75%\n",
      "2018-05-20 16:14:39 iteration 200/1000: current training loss = 0.834610, accuracy = 53.12%\n",
      "2018-05-20 16:14:50 iteration 250/1000: current training loss = 0.733956, accuracy = 65.62%\n",
      "2018-05-20 16:15:02 iteration 300/1000: current training loss = 0.743075, accuracy = 71.88%\n",
      "2018-05-20 16:15:14 iteration 350/1000: current training loss = 0.554745, accuracy = 75.00%\n",
      "2018-05-20 16:15:25 iteration 400/1000: current training loss = 0.943398, accuracy = 62.50%\n",
      "2018-05-20 16:15:35 iteration 450/1000: current training loss = 0.561042, accuracy = 68.75%\n",
      "2018-05-20 16:15:46 iteration 500/1000: current training loss = 0.640366, accuracy = 78.12%\n",
      "2018-05-20 16:15:57 iteration 550/1000: current training loss = 0.838366, accuracy = 62.50%\n",
      "2018-05-20 16:16:07 iteration 600/1000: current training loss = 0.701905, accuracy = 78.12%\n",
      "2018-05-20 16:16:18 iteration 650/1000: current training loss = 0.643972, accuracy = 65.62%\n",
      "2018-05-20 16:16:30 iteration 700/1000: current training loss = 0.612330, accuracy = 65.62%\n",
      "2018-05-20 16:16:40 iteration 750/1000: current training loss = 0.601342, accuracy = 78.12%\n",
      "2018-05-20 16:16:52 iteration 800/1000: current training loss = 0.739129, accuracy = 75.00%\n",
      "2018-05-20 16:17:03 iteration 850/1000: current training loss = 0.604502, accuracy = 65.62%\n",
      "2018-05-20 16:17:14 iteration 900/1000: current training loss = 0.773930, accuracy = 62.50%\n",
      "2018-05-20 16:17:25 iteration 950/1000: current training loss = 0.700286, accuracy = 71.88%\n",
      "2018-05-20 16:17:36 iteration 1000/1000: current training loss = 0.780077, accuracy = 59.38%\n",
      "2018-05-20 16:18:07 end epoch 37/50: acc_train=69.156% acc_val=64.219% acc_test=65.250%\n",
      "\n",
      "2018-05-20 16:18:07 start epoch 38/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:18:07 iteration 1/1000: current training loss = 0.692834, accuracy = 71.88%\n",
      "2018-05-20 16:18:18 iteration 50/1000: current training loss = 0.692631, accuracy = 71.88%\n",
      "2018-05-20 16:18:30 iteration 100/1000: current training loss = 0.933743, accuracy = 59.38%\n",
      "2018-05-20 16:18:41 iteration 150/1000: current training loss = 0.667506, accuracy = 78.12%\n",
      "2018-05-20 16:18:52 iteration 200/1000: current training loss = 0.795975, accuracy = 68.75%\n",
      "2018-05-20 16:19:03 iteration 250/1000: current training loss = 0.900380, accuracy = 56.25%\n",
      "2018-05-20 16:19:14 iteration 300/1000: current training loss = 0.622692, accuracy = 71.88%\n",
      "2018-05-20 16:19:25 iteration 350/1000: current training loss = 0.667304, accuracy = 75.00%\n",
      "2018-05-20 16:19:37 iteration 400/1000: current training loss = 0.415248, accuracy = 84.38%\n",
      "2018-05-20 16:19:48 iteration 450/1000: current training loss = 0.702113, accuracy = 62.50%\n",
      "2018-05-20 16:19:59 iteration 500/1000: current training loss = 0.634185, accuracy = 71.88%\n",
      "2018-05-20 16:20:10 iteration 550/1000: current training loss = 0.888066, accuracy = 71.88%\n",
      "2018-05-20 16:20:21 iteration 600/1000: current training loss = 0.593125, accuracy = 81.25%\n",
      "2018-05-20 16:20:32 iteration 650/1000: current training loss = 0.553193, accuracy = 75.00%\n",
      "2018-05-20 16:20:43 iteration 700/1000: current training loss = 0.838152, accuracy = 71.88%\n",
      "2018-05-20 16:20:53 iteration 750/1000: current training loss = 0.708344, accuracy = 75.00%\n",
      "2018-05-20 16:21:05 iteration 800/1000: current training loss = 0.526633, accuracy = 81.25%\n",
      "2018-05-20 16:21:16 iteration 850/1000: current training loss = 0.753057, accuracy = 68.75%\n",
      "2018-05-20 16:21:27 iteration 900/1000: current training loss = 0.756410, accuracy = 65.62%\n",
      "2018-05-20 16:21:38 iteration 950/1000: current training loss = 0.740648, accuracy = 53.12%\n",
      "2018-05-20 16:21:49 iteration 1000/1000: current training loss = 0.768745, accuracy = 65.62%\n",
      "2018-05-20 16:22:20 end epoch 38/50: acc_train=70.781% acc_val=63.406% acc_test=63.844%\n",
      "\n",
      "2018-05-20 16:22:20 start epoch 39/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:22:20 iteration 1/1000: current training loss = 0.929431, accuracy = 59.38%\n",
      "2018-05-20 16:22:31 iteration 50/1000: current training loss = 0.875485, accuracy = 59.38%\n",
      "2018-05-20 16:22:43 iteration 100/1000: current training loss = 1.102780, accuracy = 56.25%\n",
      "2018-05-20 16:22:54 iteration 150/1000: current training loss = 0.661623, accuracy = 75.00%\n",
      "2018-05-20 16:23:04 iteration 200/1000: current training loss = 0.561019, accuracy = 75.00%\n",
      "2018-05-20 16:23:15 iteration 250/1000: current training loss = 0.812401, accuracy = 65.62%\n",
      "2018-05-20 16:23:26 iteration 300/1000: current training loss = 0.814016, accuracy = 50.00%\n",
      "2018-05-20 16:23:38 iteration 350/1000: current training loss = 0.765362, accuracy = 68.75%\n",
      "2018-05-20 16:23:49 iteration 400/1000: current training loss = 0.844477, accuracy = 62.50%\n",
      "2018-05-20 16:23:59 iteration 450/1000: current training loss = 0.836333, accuracy = 50.00%\n",
      "2018-05-20 16:24:11 iteration 500/1000: current training loss = 0.694154, accuracy = 65.62%\n",
      "2018-05-20 16:24:22 iteration 550/1000: current training loss = 0.643517, accuracy = 75.00%\n",
      "2018-05-20 16:24:34 iteration 600/1000: current training loss = 0.708002, accuracy = 75.00%\n",
      "2018-05-20 16:24:45 iteration 650/1000: current training loss = 0.707114, accuracy = 68.75%\n",
      "2018-05-20 16:24:57 iteration 700/1000: current training loss = 0.673642, accuracy = 75.00%\n",
      "2018-05-20 16:25:08 iteration 750/1000: current training loss = 0.727332, accuracy = 71.88%\n",
      "2018-05-20 16:25:19 iteration 800/1000: current training loss = 0.756398, accuracy = 68.75%\n",
      "2018-05-20 16:25:30 iteration 850/1000: current training loss = 0.665818, accuracy = 75.00%\n",
      "2018-05-20 16:25:42 iteration 900/1000: current training loss = 0.798159, accuracy = 62.50%\n",
      "2018-05-20 16:25:53 iteration 950/1000: current training loss = 0.532167, accuracy = 71.88%\n",
      "2018-05-20 16:26:04 iteration 1000/1000: current training loss = 0.634969, accuracy = 78.12%\n",
      "2018-05-20 16:26:34 end epoch 39/50: acc_train=69.656% acc_val=66.094% acc_test=65.281%\n",
      "\n",
      "2018-05-20 16:26:34 start epoch 40/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:26:35 iteration 1/1000: current training loss = 0.564434, accuracy = 78.12%\n",
      "2018-05-20 16:26:46 iteration 50/1000: current training loss = 0.811717, accuracy = 71.88%\n",
      "2018-05-20 16:26:57 iteration 100/1000: current training loss = 0.518548, accuracy = 81.25%\n",
      "2018-05-20 16:27:08 iteration 150/1000: current training loss = 0.620375, accuracy = 78.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 16:27:19 iteration 200/1000: current training loss = 0.609184, accuracy = 68.75%\n",
      "2018-05-20 16:27:30 iteration 250/1000: current training loss = 0.564245, accuracy = 75.00%\n",
      "2018-05-20 16:27:41 iteration 300/1000: current training loss = 0.830622, accuracy = 68.75%\n",
      "2018-05-20 16:27:52 iteration 350/1000: current training loss = 0.745079, accuracy = 75.00%\n",
      "2018-05-20 16:28:03 iteration 400/1000: current training loss = 0.664865, accuracy = 68.75%\n",
      "2018-05-20 16:28:14 iteration 450/1000: current training loss = 1.096301, accuracy = 53.12%\n",
      "2018-05-20 16:28:25 iteration 500/1000: current training loss = 0.598571, accuracy = 71.88%\n",
      "2018-05-20 16:28:36 iteration 550/1000: current training loss = 0.604437, accuracy = 75.00%\n",
      "2018-05-20 16:28:46 iteration 600/1000: current training loss = 0.844587, accuracy = 62.50%\n",
      "2018-05-20 16:28:58 iteration 650/1000: current training loss = 0.942089, accuracy = 68.75%\n",
      "2018-05-20 16:29:09 iteration 700/1000: current training loss = 0.718064, accuracy = 65.62%\n",
      "2018-05-20 16:29:20 iteration 750/1000: current training loss = 0.759293, accuracy = 65.62%\n",
      "2018-05-20 16:29:31 iteration 800/1000: current training loss = 0.658752, accuracy = 65.62%\n",
      "2018-05-20 16:29:42 iteration 850/1000: current training loss = 0.616859, accuracy = 68.75%\n",
      "2018-05-20 16:29:53 iteration 900/1000: current training loss = 0.752473, accuracy = 71.88%\n",
      "2018-05-20 16:30:03 iteration 950/1000: current training loss = 0.698455, accuracy = 71.88%\n",
      "2018-05-20 16:30:14 iteration 1000/1000: current training loss = 0.971064, accuracy = 62.50%\n",
      "2018-05-20 16:30:45 end epoch 40/50: acc_train=68.969% acc_val=65.656% acc_test=64.750%\n",
      "\n",
      "2018-05-20 16:30:45 start epoch 41/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:30:45 iteration 1/1000: current training loss = 0.562074, accuracy = 75.00%\n",
      "2018-05-20 16:30:57 iteration 50/1000: current training loss = 0.671086, accuracy = 71.88%\n",
      "2018-05-20 16:31:08 iteration 100/1000: current training loss = 0.734724, accuracy = 59.38%\n",
      "2018-05-20 16:31:19 iteration 150/1000: current training loss = 0.761697, accuracy = 71.88%\n",
      "2018-05-20 16:31:30 iteration 200/1000: current training loss = 0.739278, accuracy = 75.00%\n",
      "2018-05-20 16:31:42 iteration 250/1000: current training loss = 0.813426, accuracy = 65.62%\n",
      "2018-05-20 16:31:54 iteration 300/1000: current training loss = 0.581033, accuracy = 75.00%\n",
      "2018-05-20 16:32:05 iteration 350/1000: current training loss = 0.803409, accuracy = 59.38%\n",
      "2018-05-20 16:32:17 iteration 400/1000: current training loss = 0.515714, accuracy = 87.50%\n",
      "2018-05-20 16:32:28 iteration 450/1000: current training loss = 0.487003, accuracy = 75.00%\n",
      "2018-05-20 16:32:39 iteration 500/1000: current training loss = 0.510477, accuracy = 81.25%\n",
      "2018-05-20 16:32:50 iteration 550/1000: current training loss = 0.888050, accuracy = 62.50%\n",
      "2018-05-20 16:33:01 iteration 600/1000: current training loss = 0.803709, accuracy = 53.12%\n",
      "2018-05-20 16:33:13 iteration 650/1000: current training loss = 0.545265, accuracy = 81.25%\n",
      "2018-05-20 16:33:24 iteration 700/1000: current training loss = 0.749539, accuracy = 59.38%\n",
      "2018-05-20 16:33:35 iteration 750/1000: current training loss = 0.719386, accuracy = 75.00%\n",
      "2018-05-20 16:33:45 iteration 800/1000: current training loss = 0.969783, accuracy = 65.62%\n",
      "2018-05-20 16:33:56 iteration 850/1000: current training loss = 0.694624, accuracy = 71.88%\n",
      "2018-05-20 16:34:08 iteration 900/1000: current training loss = 1.013833, accuracy = 53.12%\n",
      "2018-05-20 16:34:19 iteration 950/1000: current training loss = 1.001237, accuracy = 56.25%\n",
      "2018-05-20 16:34:30 iteration 1000/1000: current training loss = 0.705439, accuracy = 68.75%\n",
      "2018-05-20 16:34:59 end epoch 41/50: acc_train=71.000% acc_val=63.969% acc_test=65.406%\n",
      "\n",
      "2018-05-20 16:34:59 start epoch 42/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:34:59 iteration 1/1000: current training loss = 0.552393, accuracy = 78.12%\n",
      "2018-05-20 16:35:10 iteration 50/1000: current training loss = 0.600431, accuracy = 68.75%\n",
      "2018-05-20 16:35:21 iteration 100/1000: current training loss = 0.671925, accuracy = 71.88%\n",
      "2018-05-20 16:35:31 iteration 150/1000: current training loss = 0.680436, accuracy = 71.88%\n",
      "2018-05-20 16:35:42 iteration 200/1000: current training loss = 0.519453, accuracy = 78.12%\n",
      "2018-05-20 16:35:53 iteration 250/1000: current training loss = 0.514604, accuracy = 78.12%\n",
      "2018-05-20 16:36:04 iteration 300/1000: current training loss = 0.495630, accuracy = 84.38%\n",
      "2018-05-20 16:36:16 iteration 350/1000: current training loss = 0.595058, accuracy = 75.00%\n",
      "2018-05-20 16:36:27 iteration 400/1000: current training loss = 0.963220, accuracy = 65.62%\n",
      "2018-05-20 16:36:38 iteration 450/1000: current training loss = 0.690558, accuracy = 68.75%\n",
      "2018-05-20 16:36:49 iteration 500/1000: current training loss = 0.540833, accuracy = 81.25%\n",
      "2018-05-20 16:37:00 iteration 550/1000: current training loss = 0.935392, accuracy = 62.50%\n",
      "2018-05-20 16:37:11 iteration 600/1000: current training loss = 0.861630, accuracy = 59.38%\n",
      "2018-05-20 16:37:22 iteration 650/1000: current training loss = 0.664043, accuracy = 68.75%\n",
      "2018-05-20 16:37:35 iteration 700/1000: current training loss = 0.491886, accuracy = 75.00%\n",
      "2018-05-20 16:37:46 iteration 750/1000: current training loss = 0.556286, accuracy = 75.00%\n",
      "2018-05-20 16:37:57 iteration 800/1000: current training loss = 0.727217, accuracy = 62.50%\n",
      "2018-05-20 16:38:09 iteration 850/1000: current training loss = 0.752590, accuracy = 59.38%\n",
      "2018-05-20 16:38:20 iteration 900/1000: current training loss = 0.691849, accuracy = 81.25%\n",
      "2018-05-20 16:38:31 iteration 950/1000: current training loss = 0.904714, accuracy = 62.50%\n",
      "2018-05-20 16:38:43 iteration 1000/1000: current training loss = 0.943603, accuracy = 62.50%\n",
      "2018-05-20 16:39:14 end epoch 42/50: acc_train=68.969% acc_val=64.594% acc_test=64.844%\n",
      "\n",
      "2018-05-20 16:39:14 start epoch 43/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:39:14 iteration 1/1000: current training loss = 0.501644, accuracy = 75.00%\n",
      "2018-05-20 16:39:26 iteration 50/1000: current training loss = 0.750964, accuracy = 59.38%\n",
      "2018-05-20 16:39:37 iteration 100/1000: current training loss = 0.734065, accuracy = 62.50%\n",
      "2018-05-20 16:39:48 iteration 150/1000: current training loss = 0.717702, accuracy = 56.25%\n",
      "2018-05-20 16:39:59 iteration 200/1000: current training loss = 0.727278, accuracy = 68.75%\n",
      "2018-05-20 16:40:10 iteration 250/1000: current training loss = 0.835802, accuracy = 62.50%\n",
      "2018-05-20 16:40:22 iteration 300/1000: current training loss = 0.749143, accuracy = 62.50%\n",
      "2018-05-20 16:40:33 iteration 350/1000: current training loss = 0.721223, accuracy = 71.88%\n",
      "2018-05-20 16:40:46 iteration 400/1000: current training loss = 0.833920, accuracy = 59.38%\n",
      "2018-05-20 16:40:57 iteration 450/1000: current training loss = 0.676975, accuracy = 71.88%\n",
      "2018-05-20 16:41:08 iteration 500/1000: current training loss = 0.640655, accuracy = 71.88%\n",
      "2018-05-20 16:41:19 iteration 550/1000: current training loss = 0.746258, accuracy = 81.25%\n",
      "2018-05-20 16:41:30 iteration 600/1000: current training loss = 0.777446, accuracy = 71.88%\n",
      "2018-05-20 16:41:42 iteration 650/1000: current training loss = 0.573853, accuracy = 75.00%\n",
      "2018-05-20 16:41:53 iteration 700/1000: current training loss = 0.868010, accuracy = 65.62%\n",
      "2018-05-20 16:42:03 iteration 750/1000: current training loss = 0.584603, accuracy = 71.88%\n",
      "2018-05-20 16:42:15 iteration 800/1000: current training loss = 0.748829, accuracy = 59.38%\n",
      "2018-05-20 16:42:26 iteration 850/1000: current training loss = 0.785607, accuracy = 62.50%\n",
      "2018-05-20 16:42:37 iteration 900/1000: current training loss = 0.585452, accuracy = 75.00%\n",
      "2018-05-20 16:42:47 iteration 950/1000: current training loss = 0.556937, accuracy = 75.00%\n",
      "2018-05-20 16:42:58 iteration 1000/1000: current training loss = 0.633842, accuracy = 71.88%\n",
      "2018-05-20 16:43:28 end epoch 43/50: acc_train=69.281% acc_val=64.031% acc_test=65.344%\n",
      "\n",
      "2018-05-20 16:43:28 start epoch 44/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:43:28 iteration 1/1000: current training loss = 0.742609, accuracy = 65.62%\n",
      "2018-05-20 16:43:38 iteration 50/1000: current training loss = 0.826962, accuracy = 65.62%\n",
      "2018-05-20 16:43:50 iteration 100/1000: current training loss = 0.717932, accuracy = 65.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 16:44:00 iteration 150/1000: current training loss = 0.922578, accuracy = 53.12%\n",
      "2018-05-20 16:44:12 iteration 200/1000: current training loss = 0.709253, accuracy = 68.75%\n",
      "2018-05-20 16:44:23 iteration 250/1000: current training loss = 0.762378, accuracy = 78.12%\n",
      "2018-05-20 16:44:34 iteration 300/1000: current training loss = 0.762508, accuracy = 65.62%\n",
      "2018-05-20 16:44:45 iteration 350/1000: current training loss = 0.621405, accuracy = 68.75%\n",
      "2018-05-20 16:44:56 iteration 400/1000: current training loss = 0.709469, accuracy = 68.75%\n",
      "2018-05-20 16:45:07 iteration 450/1000: current training loss = 0.825832, accuracy = 53.12%\n",
      "2018-05-20 16:45:18 iteration 500/1000: current training loss = 0.669043, accuracy = 71.88%\n",
      "2018-05-20 16:45:28 iteration 550/1000: current training loss = 0.575450, accuracy = 78.12%\n",
      "2018-05-20 16:45:39 iteration 600/1000: current training loss = 0.834814, accuracy = 62.50%\n",
      "2018-05-20 16:45:49 iteration 650/1000: current training loss = 0.736026, accuracy = 68.75%\n",
      "2018-05-20 16:46:01 iteration 700/1000: current training loss = 0.877354, accuracy = 62.50%\n",
      "2018-05-20 16:46:11 iteration 750/1000: current training loss = 0.532279, accuracy = 81.25%\n",
      "2018-05-20 16:46:22 iteration 800/1000: current training loss = 0.765515, accuracy = 75.00%\n",
      "2018-05-20 16:46:32 iteration 850/1000: current training loss = 0.790189, accuracy = 62.50%\n",
      "2018-05-20 16:46:43 iteration 900/1000: current training loss = 0.611427, accuracy = 68.75%\n",
      "2018-05-20 16:46:53 iteration 950/1000: current training loss = 0.653569, accuracy = 65.62%\n",
      "2018-05-20 16:47:03 iteration 1000/1000: current training loss = 0.443573, accuracy = 93.75%\n",
      "2018-05-20 16:47:32 end epoch 44/50: acc_train=68.812% acc_val=64.969% acc_test=65.594%\n",
      "\n",
      "2018-05-20 16:47:32 start epoch 45/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:47:33 iteration 1/1000: current training loss = 0.536178, accuracy = 75.00%\n",
      "2018-05-20 16:47:44 iteration 50/1000: current training loss = 0.617390, accuracy = 75.00%\n",
      "2018-05-20 16:47:54 iteration 100/1000: current training loss = 0.839074, accuracy = 62.50%\n",
      "2018-05-20 16:48:05 iteration 150/1000: current training loss = 0.633727, accuracy = 68.75%\n",
      "2018-05-20 16:48:16 iteration 200/1000: current training loss = 0.757856, accuracy = 65.62%\n",
      "2018-05-20 16:48:27 iteration 250/1000: current training loss = 0.628876, accuracy = 71.88%\n",
      "2018-05-20 16:48:39 iteration 300/1000: current training loss = 0.714108, accuracy = 68.75%\n",
      "2018-05-20 16:48:50 iteration 350/1000: current training loss = 0.972492, accuracy = 59.38%\n",
      "2018-05-20 16:49:01 iteration 400/1000: current training loss = 0.512972, accuracy = 78.12%\n",
      "2018-05-20 16:49:12 iteration 450/1000: current training loss = 0.623819, accuracy = 62.50%\n",
      "2018-05-20 16:49:24 iteration 500/1000: current training loss = 0.519766, accuracy = 75.00%\n",
      "2018-05-20 16:49:34 iteration 550/1000: current training loss = 0.794308, accuracy = 65.62%\n",
      "2018-05-20 16:49:45 iteration 600/1000: current training loss = 0.650752, accuracy = 71.88%\n",
      "2018-05-20 16:49:56 iteration 650/1000: current training loss = 0.607122, accuracy = 65.62%\n",
      "2018-05-20 16:50:08 iteration 700/1000: current training loss = 0.752986, accuracy = 68.75%\n",
      "2018-05-20 16:50:19 iteration 750/1000: current training loss = 0.477003, accuracy = 84.38%\n",
      "2018-05-20 16:50:30 iteration 800/1000: current training loss = 0.662191, accuracy = 65.62%\n",
      "2018-05-20 16:50:41 iteration 850/1000: current training loss = 0.726395, accuracy = 68.75%\n",
      "2018-05-20 16:50:51 iteration 900/1000: current training loss = 0.465303, accuracy = 78.12%\n",
      "2018-05-20 16:51:03 iteration 950/1000: current training loss = 0.845144, accuracy = 59.38%\n",
      "2018-05-20 16:51:14 iteration 1000/1000: current training loss = 0.432814, accuracy = 84.38%\n",
      "2018-05-20 16:51:44 end epoch 45/50: acc_train=71.406% acc_val=66.125% acc_test=64.938%\n",
      "\n",
      "2018-05-20 16:51:44 start epoch 46/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:51:45 iteration 1/1000: current training loss = 0.639312, accuracy = 71.88%\n",
      "2018-05-20 16:51:56 iteration 50/1000: current training loss = 0.641648, accuracy = 68.75%\n",
      "2018-05-20 16:52:08 iteration 100/1000: current training loss = 0.740214, accuracy = 65.62%\n",
      "2018-05-20 16:52:19 iteration 150/1000: current training loss = 0.723993, accuracy = 65.62%\n",
      "2018-05-20 16:52:30 iteration 200/1000: current training loss = 0.845418, accuracy = 65.62%\n",
      "2018-05-20 16:52:40 iteration 250/1000: current training loss = 0.594307, accuracy = 75.00%\n",
      "2018-05-20 16:52:51 iteration 300/1000: current training loss = 0.623999, accuracy = 71.88%\n",
      "2018-05-20 16:53:02 iteration 350/1000: current training loss = 0.599758, accuracy = 78.12%\n",
      "2018-05-20 16:53:12 iteration 400/1000: current training loss = 0.587230, accuracy = 68.75%\n",
      "2018-05-20 16:53:23 iteration 450/1000: current training loss = 0.597547, accuracy = 68.75%\n",
      "2018-05-20 16:53:34 iteration 500/1000: current training loss = 0.560180, accuracy = 81.25%\n",
      "2018-05-20 16:53:45 iteration 550/1000: current training loss = 0.655646, accuracy = 71.88%\n",
      "2018-05-20 16:53:57 iteration 600/1000: current training loss = 0.697287, accuracy = 56.25%\n",
      "2018-05-20 16:54:07 iteration 650/1000: current training loss = 0.625734, accuracy = 75.00%\n",
      "2018-05-20 16:54:19 iteration 700/1000: current training loss = 0.684807, accuracy = 68.75%\n",
      "2018-05-20 16:54:30 iteration 750/1000: current training loss = 0.529598, accuracy = 75.00%\n",
      "2018-05-20 16:54:41 iteration 800/1000: current training loss = 0.642148, accuracy = 81.25%\n",
      "2018-05-20 16:54:52 iteration 850/1000: current training loss = 0.741216, accuracy = 62.50%\n",
      "2018-05-20 16:55:03 iteration 900/1000: current training loss = 0.706885, accuracy = 65.62%\n",
      "2018-05-20 16:55:14 iteration 950/1000: current training loss = 0.697561, accuracy = 68.75%\n",
      "2018-05-20 16:55:26 iteration 1000/1000: current training loss = 0.703370, accuracy = 59.38%\n",
      "2018-05-20 16:55:56 end epoch 46/50: acc_train=69.281% acc_val=66.125% acc_test=66.500%\n",
      "\n",
      "2018-05-20 16:55:56 start epoch 47/50, with learning rate = 0.0004000000\n",
      "2018-05-20 16:55:56 iteration 1/1000: current training loss = 0.628449, accuracy = 71.88%\n",
      "2018-05-20 16:56:07 iteration 50/1000: current training loss = 0.838113, accuracy = 59.38%\n",
      "2018-05-20 16:56:19 iteration 100/1000: current training loss = 0.625170, accuracy = 68.75%\n",
      "2018-05-20 16:56:29 iteration 150/1000: current training loss = 0.541725, accuracy = 71.88%\n",
      "2018-05-20 16:56:40 iteration 200/1000: current training loss = 0.726063, accuracy = 75.00%\n",
      "2018-05-20 16:56:52 iteration 250/1000: current training loss = 0.408413, accuracy = 87.50%\n",
      "2018-05-20 16:57:04 iteration 300/1000: current training loss = 0.659098, accuracy = 71.88%\n",
      "2018-05-20 16:57:16 iteration 350/1000: current training loss = 0.557590, accuracy = 68.75%\n",
      "2018-05-20 16:57:27 iteration 400/1000: current training loss = 0.470631, accuracy = 87.50%\n",
      "2018-05-20 16:57:38 iteration 450/1000: current training loss = 0.729198, accuracy = 68.75%\n",
      "2018-05-20 16:57:49 iteration 500/1000: current training loss = 0.703888, accuracy = 68.75%\n",
      "2018-05-20 16:58:00 iteration 550/1000: current training loss = 0.983502, accuracy = 56.25%\n",
      "2018-05-20 16:58:12 iteration 600/1000: current training loss = 0.559617, accuracy = 75.00%\n",
      "2018-05-20 16:58:23 iteration 650/1000: current training loss = 0.569901, accuracy = 78.12%\n",
      "2018-05-20 16:58:35 iteration 700/1000: current training loss = 0.631166, accuracy = 65.62%\n",
      "2018-05-20 16:58:46 iteration 750/1000: current training loss = 0.617288, accuracy = 71.88%\n",
      "2018-05-20 16:58:57 iteration 800/1000: current training loss = 0.769988, accuracy = 62.50%\n",
      "2018-05-20 16:59:08 iteration 850/1000: current training loss = 0.685237, accuracy = 71.88%\n",
      "2018-05-20 16:59:20 iteration 900/1000: current training loss = 0.758846, accuracy = 68.75%\n",
      "2018-05-20 16:59:31 iteration 950/1000: current training loss = 0.753360, accuracy = 62.50%\n",
      "2018-05-20 16:59:42 iteration 1000/1000: current training loss = 0.537172, accuracy = 71.88%\n",
      "2018-05-20 17:00:13 end epoch 47/50: acc_train=70.469% acc_val=65.281% acc_test=64.625%\n",
      "\n",
      "2018-05-20 17:00:13 start epoch 48/50, with learning rate = 0.0004000000\n",
      "2018-05-20 17:00:13 iteration 1/1000: current training loss = 0.818948, accuracy = 59.38%\n",
      "2018-05-20 17:00:24 iteration 50/1000: current training loss = 0.597319, accuracy = 68.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 17:00:37 iteration 100/1000: current training loss = 0.640604, accuracy = 75.00%\n",
      "2018-05-20 17:00:48 iteration 150/1000: current training loss = 0.487228, accuracy = 75.00%\n",
      "2018-05-20 17:00:59 iteration 200/1000: current training loss = 0.638367, accuracy = 71.88%\n",
      "2018-05-20 17:01:11 iteration 250/1000: current training loss = 0.860126, accuracy = 62.50%\n",
      "2018-05-20 17:01:22 iteration 300/1000: current training loss = 0.635076, accuracy = 71.88%\n",
      "2018-05-20 17:01:34 iteration 350/1000: current training loss = 0.552753, accuracy = 81.25%\n",
      "2018-05-20 17:01:45 iteration 400/1000: current training loss = 0.966882, accuracy = 53.12%\n",
      "2018-05-20 17:01:57 iteration 450/1000: current training loss = 0.755670, accuracy = 65.62%\n",
      "2018-05-20 17:02:09 iteration 500/1000: current training loss = 0.539100, accuracy = 81.25%\n",
      "2018-05-20 17:02:19 iteration 550/1000: current training loss = 0.767397, accuracy = 68.75%\n",
      "2018-05-20 17:02:31 iteration 600/1000: current training loss = 0.667083, accuracy = 78.12%\n",
      "2018-05-20 17:02:42 iteration 650/1000: current training loss = 0.595344, accuracy = 81.25%\n",
      "2018-05-20 17:02:52 iteration 700/1000: current training loss = 0.671654, accuracy = 62.50%\n",
      "2018-05-20 17:03:04 iteration 750/1000: current training loss = 0.815227, accuracy = 65.62%\n",
      "2018-05-20 17:03:16 iteration 800/1000: current training loss = 0.709085, accuracy = 59.38%\n",
      "2018-05-20 17:03:27 iteration 850/1000: current training loss = 0.751465, accuracy = 75.00%\n",
      "2018-05-20 17:03:37 iteration 900/1000: current training loss = 0.747527, accuracy = 59.38%\n",
      "2018-05-20 17:03:47 iteration 950/1000: current training loss = 0.540038, accuracy = 75.00%\n",
      "2018-05-20 17:03:58 iteration 1000/1000: current training loss = 0.544103, accuracy = 78.12%\n",
      "2018-05-20 17:04:28 end epoch 48/50: acc_train=69.500% acc_val=64.781% acc_test=64.812%\n",
      "\n",
      "2018-05-20 17:04:28 start epoch 49/50, with learning rate = 0.0004000000\n",
      "2018-05-20 17:04:28 iteration 1/1000: current training loss = 0.551719, accuracy = 78.12%\n",
      "2018-05-20 17:04:40 iteration 50/1000: current training loss = 0.611747, accuracy = 75.00%\n",
      "2018-05-20 17:04:50 iteration 100/1000: current training loss = 0.657038, accuracy = 71.88%\n",
      "2018-05-20 17:05:01 iteration 150/1000: current training loss = 0.723857, accuracy = 68.75%\n",
      "2018-05-20 17:05:12 iteration 200/1000: current training loss = 0.603126, accuracy = 75.00%\n",
      "2018-05-20 17:05:23 iteration 250/1000: current training loss = 0.558408, accuracy = 75.00%\n",
      "2018-05-20 17:05:33 iteration 300/1000: current training loss = 0.638688, accuracy = 68.75%\n",
      "2018-05-20 17:05:45 iteration 350/1000: current training loss = 0.540035, accuracy = 84.38%\n",
      "2018-05-20 17:05:56 iteration 400/1000: current training loss = 0.856736, accuracy = 62.50%\n",
      "2018-05-20 17:06:07 iteration 450/1000: current training loss = 0.683569, accuracy = 68.75%\n",
      "2018-05-20 17:06:18 iteration 500/1000: current training loss = 0.806326, accuracy = 71.88%\n",
      "2018-05-20 17:06:29 iteration 550/1000: current training loss = 0.872908, accuracy = 53.12%\n",
      "2018-05-20 17:06:40 iteration 600/1000: current training loss = 0.527152, accuracy = 78.12%\n",
      "2018-05-20 17:06:50 iteration 650/1000: current training loss = 0.921334, accuracy = 71.88%\n",
      "2018-05-20 17:07:01 iteration 700/1000: current training loss = 0.630399, accuracy = 65.62%\n",
      "2018-05-20 17:07:13 iteration 750/1000: current training loss = 0.657842, accuracy = 68.75%\n",
      "2018-05-20 17:07:24 iteration 800/1000: current training loss = 0.646103, accuracy = 78.12%\n",
      "2018-05-20 17:07:35 iteration 850/1000: current training loss = 0.949797, accuracy = 50.00%\n",
      "2018-05-20 17:07:45 iteration 900/1000: current training loss = 0.561237, accuracy = 81.25%\n",
      "2018-05-20 17:07:57 iteration 950/1000: current training loss = 0.564902, accuracy = 81.25%\n",
      "2018-05-20 17:08:09 iteration 1000/1000: current training loss = 0.593293, accuracy = 75.00%\n",
      "2018-05-20 17:08:39 end epoch 49/50: acc_train=69.000% acc_val=65.188% acc_test=65.281%\n",
      "\n",
      "2018-05-20 17:08:39 start epoch 50/50, with learning rate = 0.0004000000\n",
      "2018-05-20 17:08:39 iteration 1/1000: current training loss = 0.744604, accuracy = 71.88%\n",
      "2018-05-20 17:08:51 iteration 50/1000: current training loss = 0.656473, accuracy = 75.00%\n",
      "2018-05-20 17:09:02 iteration 100/1000: current training loss = 0.568135, accuracy = 84.38%\n",
      "2018-05-20 17:09:14 iteration 150/1000: current training loss = 0.543597, accuracy = 71.88%\n",
      "2018-05-20 17:09:24 iteration 200/1000: current training loss = 0.697352, accuracy = 78.12%\n",
      "2018-05-20 17:09:36 iteration 250/1000: current training loss = 0.689785, accuracy = 62.50%\n",
      "2018-05-20 17:09:47 iteration 300/1000: current training loss = 0.783606, accuracy = 78.12%\n",
      "2018-05-20 17:09:58 iteration 350/1000: current training loss = 0.821922, accuracy = 65.62%\n",
      "2018-05-20 17:10:09 iteration 400/1000: current training loss = 0.578694, accuracy = 78.12%\n",
      "2018-05-20 17:10:19 iteration 450/1000: current training loss = 0.688643, accuracy = 71.88%\n",
      "2018-05-20 17:10:31 iteration 500/1000: current training loss = 0.673377, accuracy = 59.38%\n",
      "2018-05-20 17:10:41 iteration 550/1000: current training loss = 0.456591, accuracy = 90.62%\n",
      "2018-05-20 17:10:53 iteration 600/1000: current training loss = 0.553083, accuracy = 75.00%\n",
      "2018-05-20 17:11:04 iteration 650/1000: current training loss = 0.567256, accuracy = 65.62%\n",
      "2018-05-20 17:11:16 iteration 700/1000: current training loss = 0.732387, accuracy = 71.88%\n",
      "2018-05-20 17:11:26 iteration 750/1000: current training loss = 0.904383, accuracy = 50.00%\n",
      "2018-05-20 17:11:38 iteration 800/1000: current training loss = 0.747380, accuracy = 75.00%\n",
      "2018-05-20 17:11:48 iteration 850/1000: current training loss = 0.517192, accuracy = 75.00%\n",
      "2018-05-20 17:11:58 iteration 900/1000: current training loss = 0.748073, accuracy = 65.62%\n",
      "2018-05-20 17:12:10 iteration 950/1000: current training loss = 0.688144, accuracy = 59.38%\n",
      "2018-05-20 17:12:21 iteration 1000/1000: current training loss = 0.749794, accuracy = 71.88%\n",
      "2018-05-20 17:12:51 end epoch 50/50: acc_train=70.594% acc_val=64.906% acc_test=64.344%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "loss_train_his=[]\n",
    "loss_val_his=[]\n",
    "loss_test_his=[]\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    max_acc=None\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'start epoch %d/%d, with learning rate = %.10f' % (epoch+1,max_epoch,sess.run(learning_rate)))\n",
    "        \n",
    "        train.start_epoch()\n",
    "#         num_iteration=num_train//bs\n",
    "        num_iteration=1000   # equalvalent to sampling a subset of training set to train\n",
    "        for it in range(num_iteration):\n",
    "            output,document_sizes,sentence_sizes,labels=train.next_batch(bs)\n",
    "            feed_dict={X_sentence:output,y:labels,sentence_length:sentence_sizes.reshape(-1,),document_length:document_sizes}\n",
    "            loss_num,acc_num,_=sess.run([loss,accuracy,train_step],feed_dict=feed_dict)\n",
    "            if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                      'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f, accuracy = %.2f%%' % (loss_num,acc_num*100.0))\n",
    "        \n",
    "        loss_train,acc_train=eval(train_eval,100)  # sample some documents to test\n",
    "        loss_val,acc_val=eval(validation,100)\n",
    "        loss_test,acc_test=eval(test,100)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        loss_train_his.append(loss_train)\n",
    "        loss_val_his.append(loss_val)\n",
    "        loss_test_his.append(loss_test)\n",
    "        \n",
    "        if max_acc==None or acc_val>max_acc:\n",
    "            max_acc=acc_val\n",
    "            save_path = saver.save(sess, \"parameters/HAN.ckpt\")\n",
    "            print(\"Currently maximum accuracy on validation set, model saved in path: %s\" % save_path)\n",
    "        \n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.savefig('model-acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),loss_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),loss_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),loss_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss function on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.savefig('model-loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-20 17:12:52,135 : INFO : Restoring parameters from parameters/HAN.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set is 66.891%\n",
      "Accuracy on validation set is 64.659%\n",
      "Accuracy on testing set is 64.734%\n",
      "\n",
      "Total loss on training set is 0.747162\n",
      "Total loss on validation set is 0.796575\n",
      "Total loss on testing set is 0.796836\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"parameters/HAN.ckpt\")\n",
    "    loss_train,acc_train=eval(train_eval,1000)\n",
    "    loss_val,acc_val=eval(validation,1000)\n",
    "    loss_test,acc_test=eval(test,1000)\n",
    "    print('Accuracy on training set is %.3f%%' % (acc_train*100.0))\n",
    "    print('Accuracy on validation set is %.3f%%' % (acc_val*100.0))\n",
    "    print('Accuracy on testing set is %.3f%%' % (acc_test*100.0))\n",
    "    print()\n",
    "    print('Total loss on training set is %f' % loss_train)\n",
    "    print('Total loss on validation set is %f' % loss_val)\n",
    "    print('Total loss on testing set is %f' % loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
